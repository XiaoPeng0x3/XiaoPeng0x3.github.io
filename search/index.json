[{"content":"前言 本系列是hot100补全计划的二叉树篇，主要包含了力扣(leetcode)的二叉树章节部分的习题，除此之外还会有一些比较著名的但不包含在hot100的二叉树题目。\n题目 这道题目是leetcode的第105题，链接在这里(点我)。\n意思就是说\n给二叉树的先序遍历(根左右的遍历循序) 给定二叉树的中序遍历(左根右的遍历循序) 要求还原出二叉树的原始样子 思路 tips: 对于二叉树和链表来说，使用递归是一类通用的解题技巧\n根据先序遍历，显然，第一个结点就是根节点; 根据中序遍历，根节点的左边结点都是左子树，右边节点都是右子树。\n也就是说，我们可以根据这个规则来发现一些东西。首先，先序遍历数组preorder的第一个元素一定是根节点，也就是说，每次我们构建根节点，就可以从preorder出发，每次都确定好对应的root坐标；然后，我们就需要正确的划分，哪些值是属于左子树的，哪些值是属于右子树的。\n也就是说，给定一个root下表，我们需要找到这个root的左子树和右子树，这该怎么寻找呢？实际上，中序遍历就可以很好的解决这个问题。因为我们已经知道，中序遍历可以很好的划分左右子树，那么，对于preorder[root]元素来说，在中序遍历中，其左边的值就是全部属于左子树，其右边的值全部属于右子树，这样我们就可以很好的确定左右子树的范围。\n举个例子 在题目中举例了两个数组，其内容如下\n从这两个数组中，我们可以清楚的看到，preorder[0]即为二叉树的根节点，那么从inorder中，可以得出[9]是其左子树，[15, 20, 7]是其右子树的值。但是，得到了右子树，怎么确定右子树的根节点呢？其实，得到左子树的长度后，即长度为1，那么在preorder[0+1+1]即preorder[2]为其右子树的根节点。\n所以，我们就得到了一般思路，即根节点要从preorder中进行寻找，左右子树的长度要从inorder中进行寻找。\n实现 在寻找某个根节点对应的左右子树的范围一般是这样的，当我们找到一个preorder[root]后，需要在inorder中找到其所在的位置，然后才可以区分左右子树，其过程如下\n1 2 3 4 5 6 7 int ans = 0; for (int i = 0; i \u0026lt; inorder.length; i++) { if (inorder[i] == preorder[root]) { ans = i; break; } } 这样，每次都得进行一次O(N)的寻找，十分耗时，我们可以把inorder存储在一个HashMap中，这样在每次进行寻找时可以以O(1)的时间找到。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 /** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode() {} * TreeNode(int val) { this.val = val; } * TreeNode(int val, TreeNode left, TreeNode right) { * this.val = val; * this.left = left; * this.right = right; * } * } */ class Solution { // 定义一个HashMap来方便快速找到根节点对应的位置 Map\u0026lt;Integer, Integer\u0026gt; indexOfNode = new HashMap\u0026lt;\u0026gt;(); int[] preorder; public TreeNode buildTree(int[] preorder, int[] inorder) { if (preorder == null || inorder == null) { return null; } this.preorder = preorder; // 构建HashMap for (int i = 0; i \u0026lt; inorder.length; i++) { indexOfNode.put(inorder[i], i); } // 开始遍历 return build(0, 0, inorder.length-1); } private TreeNode build(int root, int left, int right){ if (left \u0026gt; right) { return null; } // 在中序遍历中，找到对应根节点的位置 int i = indexOfNode.get(preorder[root]); // 也就是说，对于根节点来说，(left, i-1)是其左子树 // 那么，其右子树范围就是 (i+1, right) // 那么，其左子树对应的根节点就是root+1 // 其右子树对应的根节点就是 root+i-left+1 // 递归的构建左右子树 TreeNode node = new TreeNode(preorder[root]); // 左子树 node.left = build(root+1, left, i-1); // 右子树 node.right = build(root+i-left+1, i+1, right); return node; } } ","date":"2025-07-14T11:26:42+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E4%BB%8E%E5%89%8D%E5%BA%8F%E9%81%8D%E5%8E%86%E5%92%8C%E4%B8%AD%E7%BB%AD%E9%81%8D%E5%8E%86%E4%B8%AD%E6%9E%84%E9%80%A0%E4%BA%8C%E5%8F%89%E6%A0%91/","title":"从前序遍历和中续遍历中构造二叉树"},{"content":"前言 今天来看一看mysql的日志部分，也就是对应的管理篇。并且，我们来配置一下怎么在一台机器上使用docker来实现主从复制。\n日志 错误日志 错误日志记录了mysql启动和停止的时候，以及服务器再运行过程中发生任何严重错误的相关信息，当数据库出现任何故障导致无法启动时，可以先看看错误日志里面的内容。\n可以使用以下命令来查看日志位置\n1 show variables like \u0026#39;%log_error%\u0026#39;; 可以看到该错误日志的位置所在！\nbin log bin log记录了所有的ddl语句和dml语句，但不包括数据查询语句(select show)语句。\n相似的，可以使用\n1 show variables like \u0026#39;%log_bin\u0026#39;; 来查看对应参数和位置，在这些参数中，用的比较多的有两个\nlog_bin_basename：当前数据库服务器的binlog日志的基础名称 log_bin_index：binlog的索引文件，里面记录了当前服务器关联的binlog文件有哪些 格式 使用\n1 show variables like \u0026#39;%binlog_format%\u0026#39;; 来进行查看\n1 2 3 4 5 6 7 mysql\u0026gt; show variables like \u0026#39;%binlog_format%\u0026#39;; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | binlog_format | ROW | +---------------+-------+ 1 row in set, 1 warning (0.00 sec) 查看 这部分很好理解，因为bin log并不是基于普通的文本进行存储的，而是以二进制的形式存储的，所以需要专门的日志查询工具进行查询。\n这个工具就是mysqlbinlog\n删除操作 可以使用以下这几种方式来清理日志\n主从复制 什么是主从复制 主从复制时将数据库的写操作通过二进制日志传到从库服务器中，然后在从库上对这些日志重新执行(也就做重做)，从而实现从库与主库之间的数据保持同步。\nMysql支持一台主库同时向多态从库进行复制，从库也可以作为其它从库的主库，从而实现链型复制。\n主从复制的好处在于\n主库出现服务，可以快速切换到从库进行服务 实现读写分离，降低主库的访问压力 可以在从库中实现备份，从而避免备份期间影响主库服务 原理 Mysql主从复制的核心就是 bin log，具体过程如下\nmaster主库在事务提交的时候，会把数据变更写在bin log中 从库在读取主库的bin log时，写入到从库的中继日志relay log中 slave从库重做中继日志中的事务，从而改变自己的数据。 其原理可由下图概括：\n实际上，主从复制的依赖就是网络和我们之前提到的用户管理，我们在主库中保存各个从库的用户名和密码，当主库发生数据修改，例如修改表结果、存储新的数据等这些操作，就会把这些操作记录在bin log中。对于从库来说，其完成主从复制的核心是两个线程\nIOthread，读取主库的bin log，把数据写到自己的Relay log中 SQLthread，读取Relay log，然后把数据写道自己的库中 环境搭建 一般的，我们可以给两台服务器安装好mysql，并配置用户名和密码，但是我只有一台电脑，理论上一台电脑应该也可以完成这个任务。\n本人使用的是WSL Ubuntu 22的版本\n经过我的研究，我们还是使用docker比较好！在windows下，我们可以无缝来使用docker和wsl来实现！\n镜像拉取 首先是获取对应的镜像\n1 docker pull mysql:8.0.25 拉取下来后，就可以运行容器了\n配置网络 本人之前在master和slave中发现不可以ping通，在docker里面我们可以创建一个网络，并且在启动的时候就使用这个网络启动\n1 docker network create mysql-net 下面就可以使用这个网络选项来启动主库\n1 2 3 4 5 6 7 8 9 # 主库 docker run -p 3307:3306 --name mysql-master \\ --network mysql-net \\ -v /mydata/mysql-master/log:/var/log/mysql \\ -v /mydata/mysql-master/data:/var/lib/mysql \\ -v /mydata/mysql-master/conf:/etc/mysql \\ -v /mydata/mysql-master/mysql-files:/var/lib/mysql-files \\ -e MYSQL_ROOT_PASSWORD=123456 \\ -d mysql:8.0.25 mysql对应的服务是3306，我们就映射到本机的3307端口，这是我们的主库。\n服务配置 然后来到config目录下来完成配置\n1 cd /mydata/mysql-master/conf 创建my.cnf，可能需要root权限\n1 vim my.cnf 填写如下配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 [mysqld] ## 设置server_id，同一局域网中需要唯一 ## 在同一局域网中，可以把这个id置为对应的ip地址的最后一个一段 ## 例如，加入主机的ip是 172.18.0.2，就可以把这个`ip`置为`2` server_id=101 ## 指定不需要同步的数据库名称 binlog-ignore-db=mysql ## 开启二进制日志功能 log-bin=mall-mysql-bin ## 设置二进制日志使用内存大小（事务） binlog_cache_size=1M ## 设置使用的二进制日志格式（mixed,statement,row） binlog_format=mixed ## 二进制日志过期清理时间。默认值为0，表示不自动清理。 expire_logs_days=7 ## 跳过主从复制中遇到的所有错误或指定类型的错误，避免slave端复制中断。 ## 如：1062错误是指一些主键重复，1032错误是因为主从数据库数据不一致 slave_skip_errors=1062 然后重新启动master\n1 docker restart mysql-master 然后进入到master mysql中，创建slave用户\n1 docker exec -it mysql-master /bin/bash # -it是以交互式的方式进入 登录mysql\n1 mysql -uroot -p123456 创建和配置主库 在master中创建和配置slave\n1 CREATE USER \u0026#39;slave\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;123456\u0026#39;; -- 用户名是slave, 密码是123456 给slave用户授权\n1 GRANT REPLICATION SLAVE,REPLICATION CLIENT ON *.* TO \u0026#39;slave\u0026#39;@\u0026#39;%\u0026#39;; 我们来看看主库当前有哪些用户\n1 2 3 4 5 6 mysql\u0026gt; SELECT user, host FROM mysql.user; +------------------+-----------+ | user | host | +------------------+-----------+ | root | % | | slave | % | 可以看到slave用户已经成功创建了。\n这里我们就不去赘述用户创建和管理了，感兴趣的可以去MySQL补全计划(一)中去详细了解。\n修改密码，使之可以使用弱密码\n1 ALTER USER \u0026#39;slave\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED WITH mysql_native_password BY \u0026#39;123456\u0026#39;; 创建和配置从库 启动从库\n1 2 3 4 5 6 7 8 docker run -p 3308:3306 --name mysql-slave \\ --network mysql-net \\ -v /mydata/mysql-slave/log:/var/log/mysql \\ -v /mydata/mysql-slave/data:/var/lib/mysql \\ -v /mydata/mysql-slave/conf:/etc/mysql \\ -v /mydata/mysql-slave/mysql-files:/var/lib/mysql-files \\ -e MYSQL_ROOT_PASSWORD=123456 \\ -d mysql:8.0.25 启动后，进入从库\n1 docker exec -it mysql-slave /bin/bash 进入后，我们可以先ping一下master，看看是否可以连通。\n可能在ping的时候发现没有ping工具，这个现象是正常的，我们可以安装一下\n1 2 apt update apt install -y iputils-ping docker内有对应的dns解析，对于一个ip，我们可以使用对应的名称作为ip地址\n1 ping mysql-master 如果可以ping通，说明到目前为止，网络正常！然后退出！\n注意，主从复制依赖于顺畅的网络环境！\n对应的，我们也需要给slave创建对应的config\n1 cd /mydata/mysql-slave/conf 创建配置文件，可能需要管理员权限\n1 vim my.cnf 写入下面内容\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 [mysqld] ## 设置server_id，同一局域网中需要唯一 server_id=102 ## 指定不需要同步的数据库名称 binlog-ignore-db=mysql ## 开启二进制日志功能，以备Slave作为其它数据库实例的Master时使用 log-bin=mall-mysql-slave1-bin ## 设置二进制日志使用内存大小（事务） binlog_cache_size=1M ## 设置使用的二进制日志格式（mixed,statement,row） binlog_format=mixed ## 二进制日志过期清理时间。默认值为0，表示不自动清理。 expire_logs_days=7 ## 跳过主从复制中遇到的所有错误或指定类型的错误，避免slave端复制中断。 ## 如：1062错误是指一些主键重复，1032错误是因为主从数据库数据不一致 slave_skip_errors=1062 ## relay_log配置中继日志 relay_log=mall-mysql-relay-bin ## log_slave_updates表示slave将复制事件写进自己的二进制日志 log_slave_updates=1 ## slave设置为只读（具有super权限的用户除外） read_only=1 重启服务\n1 docker restart mysql-slave 查看主库的内容 进入主库，登录数据库后，查看当前的一些配置\n1 show master status; 注意，file和position是我们需要记录的。\n查看从库 登录从库后，配置从库信息,根据上面的信息，可以进行如下配置\n1 change master to master_host=\u0026#39;mysql-master\u0026#39;, master_user=\u0026#39;slave\u0026#39;, master_password=\u0026#39;123456\u0026#39;, master_port=3306, master_log_file=\u0026#39;mall-mysql-bin.000004\u0026#39;, master_log_pos=935, master_connect_retry=30; 配置好后，可以打开主从复制\n1 start slave; 查看从库是否开始复制\n可以看到，Slave_IO_Running和Slave_SQL_Running已经正常运行了！\n插入数据测试 我们登录主库，插入一些数据试试。\n1 create database test01; 1 use test01; 然后创建一张表\n1 2 3 4 5 create table user ( id int auto_increment, name varchar(50), age int ); 插入一些数据试试\n1 insert into user(id, name) values (1, \u0026#39;zxp1\u0026#39;), (2, \u0026#39;zxp2\u0026#39;); 查看一下\n1 2 3 4 5 6 7 8 select * from user; +----+------+------+ | id | name | age | +----+------+------+ | 1 | zxp1 | NULL | | 2 | zxp2 | NULL | +----+------+------+ 2 rows in set (0.00 sec) 可以看到，主库已经有这些数据了，如果一切顺利，那么从库也会有当前对应的数据！\n主从复制总结 主从复制的主要依赖就是bin log和relay log，bin log，这个日志记录了所有的增删改操作，一般来说，**主库只开启bin log**即可。\nrelay log是从库的中转站，从库会把读到的bin log写入relay log中，等待Slave_SQL_Thread线程读取恢复，一般来说，如果从库后续不作为主库，那么不用开启bin log。\n总结 本篇总结了日志以及主从复制的过程，后面我们来介绍一下分库分表！\n","date":"2025-06-20T22:46:17+08:00","permalink":"https://XiaoPeng0x3.github.io/p/mysql%E8%A1%A5%E5%85%A8%E8%AE%A1%E5%88%92%E4%B8%83/","title":"Mysql补全计划(七)"},{"content":"前言 好耶！是MySQL补全计划六！\nMVCC MVCC的意义就在于可以使不同的事务并发读数据不会阻塞，我们接着上篇，来说一下MVCC的核心概念。\nreadview readview是快照读sql执行时mvcc读取数据的依据，包括四个核心字段。\n上篇我们说到，除了我们自己添加的数据外，mysql还会自动给我们添加一些隐藏字段。\nDB_TRX_ID：当前是哪个事务修改了这行数据，记录ID，而且，ID还是自增的 DB_ROLL_PTR：当进行回滚时，需要回滚到的版本，这个指针是指向undo log的 所以，我们是否可以读某个事务(ID可以区分)修改的数据，就可以与这个事务的ID进行比较。\n而read view正好帮我们维护了一些还未提交的事务id，read view把这些个未提交的事务叫做活跃事务。\nreadview对版本链的访问规则 事务开始时（开启手动提交）：\n系统会为这个事务分配一个新的事务 ID（trx_id） 如果该事务第一次执行快照读（普通的 SELECT），就会创建一个 ReadView ReadView 的组成：\ncreator_trx_id：表示这个 ReadView 是哪个事务创建的 m_ids：当前系统中活跃的事务 ID（还没提交的） min_trx_id：m_ids 中最小的 ID max_trx_id：创建 ReadView 时，系统中下一个即将分配的事务 ID 其他事务在执行写操作时：\n修改数据不直接覆盖旧数据，而是： 把旧值写入 undo log，形成版本链（由 roll_pointer 连接） 将新值写到页中，标记新的 trx_id 当前事务读取数据时：\n遇到一条记录的当前版本 根据该记录的 trx_id 和本事务的 ReadView 判断它是否可见 如果不可见，顺着 undo log 中的版本链回退 找到第一个符合 ReadView 可见性规则的旧版本 ReadView 决定你“能不能看”某个版本，undo log 提供你“可以看的版本”。\n不同的隔离级别，生成read view的时机是不同的\nread commit，在这个隔离级别中，只可以读到已经提交的事务的数据，不保证可重复读，也就是在每次快照读的时候都会生成read view rr，保证可重复读，仅在第一次执行快照读的时候生成read view，后续复用read view 视图/存储过程/触发器 什么是视图 视图时一种虚拟存在的表。视图中的数据并不在数据库中存在，行和列的数据来自定义视图查询中使用的表，并且是动态生成的。\n创建视图view 我们可以使用create和replace with关键字来创建或修改视图。\n1 create or replace view 视图名称 as select col1, col2, ... from ... where ...; 意思就是说，select的返回结果其实也是一张表，根据返回结果，我们来创建对应的视图结构\n这里有一张user表，其内容如下\n1 2 3 4 5 6 7 8 9 10 11 mysql\u0026gt; select * from user; +----+------+------+ | id | name | age | +----+------+------+ | 1 | zxp1 | 18 | | 2 | zxp2 | 19 | | 3 | zxp3 | 20 | | 4 | zxp4 | 22 | | 5 | zxp5 | 23 | +----+------+------+ 5 rows in set (0.00 sec) 假设我们要创建只有两列关键字的视图\n1 mysql\u0026gt; create or replace view view_id_name as select id, name from user; 当创建好视图后，就可以像正常操作sql数据库中其它的表一样\n1 2 3 4 5 6 7 8 9 10 select * from view_id_name; +----+------+ | id | name | +----+------+ | 1 | zxp1 | | 2 | zxp2 | | 3 | zxp3 | | 4 | zxp4 | | 5 | zxp5 | +----+------+ 可以看到，只有两列字段，剩余的age字段在创建视图的时候我们没有选择这个字段，所以视图中不包含这个字段。\n查询当前数据库的view 查看创建视图的语句\n1 show create view 视图名; 例如\n1 show create view view_id_name; 其中就包含我们创建这个视图时所使用到的sql语句。\n修改视图 修改视图，一种是重新创建\n1 create or replace view 视图名称 as select .....; 就是使用replace语句来替代原始的那个视图。\n或者直接把视图当作表来使用，即使用alter关键字。\n1 alter view 视图名 as select .... 重新进行选择即可。\n删除视图 使用drop关键字\n1 drop view if exists 视图名 MySQL管理 系统数据库 Mysql数据库安装完后，自带了四个数据库，我们可以看看当前所有的数据库\n1 show databases; 数据库备份 1 mysqldump 总结 下面我们来看一看一些更加高级的操作，例如，分库分表、读写分离等这些操作！\n","date":"2025-06-18T14:46:17+08:00","permalink":"https://XiaoPeng0x3.github.io/p/mysql%E8%A1%A5%E5%85%A8%E8%AE%A1%E5%88%92%E5%85%AD/","title":"Mysql补全计划(六)"},{"content":"前言 今天继续来完成补全计划！\nInnoDB存储引擎 逻辑存储结构 表空间\n表空间是InnoDB存储引擎逻辑的最高层，当用户启用了参数innodb_file——per_table，则每张表就会有一个表空间，一个mysql实例可以对应多个表空间，用于存储记录、索引。\n段\n分为数据段、索引段、回滚段，InnoDB是索引组织表，数据段就是B+树的叶子结点。段可以用来管理多个区。\n区\n每个区的大小为1M，默认情况下，InnoDB存储引擎页大小为16k，即一个区里面有64个连续的页。\n页\n页是磁盘管理的最小单元，每个页大小默认为16kb\n行\nInnoDB存储数据是按照行进行存放的。\n在行中，默认有两个隐藏字段：\nTrx_id：每次对某条记录进行改动时，都会把对应的事务id赋值给trx_id隐藏列 Roll_pointer：每次对某条记录进行改动时，都会把旧的版本写入undo日志，然后这个隐藏列就相当于一个指针，可以通过它来找到该记录修改前的信息，也就是可以用这个指针来进行undo日志的操作。 内存结构 主要分为四大块，Buffer Pool、Change buffer、Adaptive Hash Index、Log Buffer\nBuffer Pool 缓存池的思想我们已经很熟悉了，如果每次操作都是进行磁盘IO，那么效率会很慢，，我们可以把经常使用的数据加载到缓存池里面，这样就可以避免每次访问的时候都进行磁盘IO。\n在执行增删改查操作时，先操作缓存池中的数据，然后再以一定的频率刷新到磁盘中，从而减少磁盘IO，加快处理速度。\nChange Buffer 这个其实也是缓存而已，只不过是专门对于二级索引建立的，在修改二级索引有关的数据时，会把修改的遍历提交到这个缓存中，等待某个时机后会与buffer pool合并刷新到磁盘中。\n自适应hash 自适应hash索引，用于优化对Buffer Pool数据的查询。\nhash索引在进行等值匹配时，一般性能是要高于B+树，但是hash索引又不适合做范围查询、模糊匹配等。\n自适应hash可以理解为是InnoDB存储引擎对热点数据、热点页加快访问而自动创建管理的索引。\nLog Buffer 日志缓冲区，用来保存需要写入到磁盘中的日志数据。像redo log等。在进行进行修改或删除语句时，mysql会开启自动事务提交。在提交时，为了保证事务的一致性和完整性，我们通常会使用redo log，即将事务写道log buffer中，等待一定时机把这个事务刷新到磁盘里面，这样，就算mysql崩溃了，也会去重新加载磁盘上的redo log，然后更新事务。\n🙅‍自动事务提交不支持事务回滚(RollBack)\n事务什么时候提交到磁盘有三种选择：\n每次事务提交的时候写入缓存并立即刷新到磁盘中，是默认选项 每秒将日志刷新到磁盘中 日志提交到缓存中后，每秒刷新日志到磁盘中。 磁盘结构 后台线程 主要包括\nMaster Thread IO Thread Purge Thread Page Cleaner Thread Master Thread 核心后台线程，负责调度其它线程，还负责将缓冲池中的数据异步刷新到磁盘中。\nIO Thread 可以通过下面这条指令来查看InnoDB的状态信息\n1 show engine innodb status \\D; Purge(净化) Thread 用于回收事务已经提交的undo log，在事务提交后，undo log可能就不用了\nPage Cleaner Thread 协助Master Thread刷新脏页到磁盘的线程，减轻Master Thread的压力。\n事务 事务四大特性 原子性 要么全部成功提交，要么全部失败，不存在中间状态。\n一致性 事务完成时，所有数据必须保证一致性。\n隔离性 多个事务之间互不影响，而且事务在不受外界影响的隔离环境中运行。\n持久性 事务一旦提交或者回滚，它对数据库中的数据改变就是永久的。\n对于这四大特性，由实现原理分为两部分，其中原子性、一致性、持久性是通过redo log和undo log保障的。\n隔离性是通过数据库的锁和MVCC保证的。\nLog日志 最重要的就是redo log和undo log，下面来详细说一下啊这些日志。\nRedo log和WAL预写日志 Redo log主要分为两类，一种是存在于磁盘上，另一种就是存在于内存中。\n内存中主要是重做缓存的部分，也就是log buffer缓存，当事务提交时，就会把所有修改信息全部保存在缓存中，然后根据一定的刷新时机，把所有信息全部存储在该日志文件中。\n可以重载恢复的一部分是存在于磁盘上，从而做到了持久化恢复，当发生错误时，就可以进行数据恢复。\n记住，redo log是帮助Mysql服务器崩溃重启而建立的，当数据库崩溃时，再次重启就可以从磁盘中的redo log日志中加载未提交恢复的事务。\n在业务操作中，我们操作数据一般是随机读写磁盘的，而不是顺序读写磁盘的。而redo log在往磁盘中写入文件，由于是日志文件，都是顺序写的。顺序写的效率要远大于随机写。这种先写日志的方式，叫做WAL。\n其实上面这段话有些绕，这种方式也算是一种独特的持久化策略。因为每次修改一个数据、删除数据基本上都是随机的，此时，每次都写一个数据实际上就会造成一次随机读取。而对于redo log日志来说，先写入内存中，然后再刷新到磁盘中，对于磁盘中的日志文件来说，就是**(顺序读写的(连续追加**)\nUndo log 事务的一个关键特点就是支持回滚\n当事务进行回滚时，之前对数据的修改“都不算话”！\n其实现思路是反着来，例如，当执行一条insert语句时，就在undo log中保存一条相反语句即可，属于是逻辑日志。\n多版本并发控制MVCC 我觉得MVCC就是为各种事务的隔离级别所设置的。实际上，MVCC把各种读写分为多个版本，从而做到可重复读。\n当前读 当前读读的是最新的版本，读取时还要确保其它并发事务不能修改当前记录，下面这些操作都是一种当前读\n1 select ... lock in share mode; -- 共享锁 1 2 3 update insert delete -- 排他锁 都是一种当前读。\n快照读 简单的select 就是快照读，快照读的意思就是在某个时候对数据进行一个“拍照”，记录下那个时候的数据。\n还记得我们的几种事务隔离级别吗？他们解决什么问题来着？\nRC，读提交，解决的是“一个线程可以读到另一个线程未提交的事务”的问题，对应问题是脏读\nRR，可重复读，解决的是“一个线程前后读到的数据不一致的情况”，对应问题是不可重复读\n序列化，解决的是“一个线程之前可以读到数据，之后发现读不了这个数据”的问题，对应的问题是幻读\n对于Rc隔离级别，一个线程只可以读到另一个线程已经提交过的事务，所以，当另一个线程的事务未提交时，每次select的时候都生成一个快照读。\n对于RR的隔离级别，实际上，为了解决可重复读，一个很容易想到的思路是，在第一次对某个数据进行查询操作的时候，就生成这个数据的快照，这样，每次都去读这个数据的快照，也就解决了“不可重复读”的问题。\n对于”序列化“的隔离级别来说，快照读会变为当前读，总是保证事务提交后再去读，也就是说，序列化读到的数据永远都是最正确的，也就解决了幻读。\nMVCC MVCC的含义就是去维护一份数据的多个版本，从而避免了在并发场景下同一份数据被争抢而导致的频繁lock/unlock操作。\n下面我们来分析一下InnoDB表中的一些个隐藏字段\n隐藏字段 在创建表时，除了我们手动创建的字段类型外，还有一些辅助字段。\nundo log 前面已经介绍了一些undo log的基本思想，实际上，当事务成功提交后，所产生的undo log就会被删除。\n下面我们来看一看上面这些隐藏字段和undo log是怎么做到事务回滚的吧！\n假设有一张表为\n在刚开始，这个表的某行数据还没有被修改过，所以也就没有需要回滚的版本。\n假设有多个事务在并发执行，我们假设为事务1 事务2 事务3 事务4，当事务1进行数据提交时，我们需要修改三个地方\n生成对应的undo log日志，以免日后数据回滚 修改DB_TRX_ID字段，把这个值改为1，意思是”事务id为1的事务修改了这行数据 修改当前数据的DB_ROLL_PTR数据字段，把当前字段指向undo log 同样的，这几个事务在并发执行的时候，也就会轮流修改这些数据，由于事务还未提交，对应的undo log日志也会一直存在，从而变成一个undo log日志链\n总结 下次来讨论一下视图以及MVCC实现的一些具体原理。\n","date":"2025-06-17T14:46:17+08:00","permalink":"https://XiaoPeng0x3.github.io/p/mysql%E8%A1%A5%E5%85%A8%E8%AE%A1%E5%88%92%E4%BA%94/","title":"Mysql补全计划(五)"},{"content":"前言 下面我们来看一看sql优化的一些东西。\nsql优化 插入数据 插入数据的sql就是insert语句，其优化策略一般是将多次插入改为批量插入，这样可以减少IO的次数。\n批量插入数据 例如，如果我们要插入三条数据，可以这样写\n1 2 3 insert into user (name, age) values (\u0026#39;zxp1\u0026#39;, 18); insert into user (name, age) values (\u0026#39;zxp2\u0026#39;, 19); insert into user (name, age) values (\u0026#39;zxp3\u0026#39;, 20); 这种方式每次都要进行磁盘IO的连接和读写，可以使用批量插入来减少磁盘IO的读写\n1 insert into user (name, age) values (\u0026#39;zxp1\u0026#39;, 18), (\u0026#39;zxp2\u0026#39;, 19, \u0026#39;zxp3\u0026#39;, 20); 手动提交事务 在默认情况下事务是自动执行的，基本上是每执行一条更新操作就会去提交事务。而频繁的事务提交也是会影响性能的。因此，我们可以以手动提交事务的方式来减少事务的提交。\n1 2 3 4 5 6 start transaction; insert into user (name, age) values (\u0026#39;zxp1\u0026#39;, 18); insert into user (name, age) values (\u0026#39;zxp2\u0026#39;, 19); insert into user (name, age) values (\u0026#39;zxp3\u0026#39;, 20); commit; -- 多条插入语句仅仅是举例子使用 主键顺序插入 我们假设主键值是自增的id，那么，什么是主键顺序插入呢？\n1 2 乱序: 8, 1, 9, 0, 2, 4, 3 顺序: 1, 2, 3, 5, 6, 9, 20 也就是说，按照顺序插入可以减少数据库底层数据结构的调整。\n使用load而不是insert 当数据量特别大时，例如，一次性插入几百万条的记录，就应该使用load命令\n1 2 3 4 5 6 7 8 9 -- 连接数据库，表示要从本地文件中导入 mysql –-local-infile -u root -p -- 设置全局参数local_infile为1，开启从本地加载文件导入数据的开关 set global local_infile = 1; -- 执行load指令将准备好的数据，加载到表结构中 load data local infile \u0026#39;/root/sql1.log\u0026#39; into table tb_user fields terminated by \u0026#39;,\u0026#39; lines terminated by \u0026#39;\\n\u0026#39; ; 数据之间使用,进行分隔，每一行之间使用换行作为结尾。\n深分页优化 在项目中，对数据进行分页优化是很有用的，假设这里有几十万条数据，那么，我们在做分页查询时，其sql可能如下\n1 select * from user limit 0, 10; 这个意思是说，取出前十条数据，然后返回前10条数据。当数据量很大时，sql查询看起来像是下面的这样\n1 select * from user limit 100000, 10; 也就是说，取出前100010条数据，然后返回100000到100010中间的数据。\n一种优化办法是去使用索引。\n首先查出分页数据对应的id\n1 select id from user order by id limit 100000, 10; 由于id是具有主键索引的，所以查询效率很快。\n然后再根据id来进行查询，也就是子查询\n1 2 3 4 5 SELECT * FROM user WHERE id IN ( SELECT id FROM user ORDER BY id LIMIT 100000, 10 ) ORDER BY id; count优化 update优化 update语句一般与where筛选一起使用，所以，从查询的角度出发，查询时使用索引可以加快查找。\n而且，由于InnoDB支持行级锁，即单条数据，但是行锁是针对索引的锁，行锁未启用就会升级为表锁。\n在使用id进行查询更新时，使用的是行级锁\n1 update user set name = \u0026#39;zxp3\u0026#39; where id = 1; 事务提交后，行锁释放。\n使用非索引字段进行查询时，使用的是表级锁。\n1 update user set name = \u0026#39;zxp4\u0026#39; where name = \u0026#39;zxp5\u0026#39;; InnoDB的行锁是针对索引加的锁，不是针对记录加的锁，并且该索引不可以失效，否则会从行锁升级为表锁\n锁 下面来说记录一下mysql中的锁，这一部分也是非常重要的知识。\n全局锁 全局锁是对整个数据库进行加锁，加锁之后整个实例就只处于只读状态，后续的写操作都会被阻塞。\n其典型的应用场景就是做全库的逻辑备份，对所有的表进行锁定，进而保证数据的一致性和完整性。\n加全局锁 1 flush tables with read lock; 释放锁 1 unlock tables; 表级锁 每次操作都锁住整张表，锁的粒度大，发生锁冲突的概率最高，并发程度最低。对于表级锁，主要分为以下三类\n表锁 元数据锁(meta data lock) 意向锁 表锁 表锁其实就是一种读写锁，读的时候可以多个线程共享的去读，写的时候只能有一个线程在写。而且读写锁之间是互斥的，即读的时候会阻塞写操作，写的时候会阻塞读操作。\n加表锁 可以给这张表加上read lock或者write lock\n1 2 3 4 5 -- 给名字为table_name的表加上读锁 lock tables table_name read; -- 给名字为table_name的表加上写锁 lock tables table_name write; 释放表锁 释放的时候需要使用关键字unlock\n1 unlock tables table_name; 元数据锁 meta data lock，这个锁是mysql底层为了保证数据的完整性而自带的一种结构。它无需我们手动添加。\n其实元数据锁就是读写锁的一种，可以分为\nshared_read_only和shared_no_read_write，即对一张表的数据只可以并发读或排他的写，对应于上述提到的表锁 shared_read，表层级的并发读 shared_write，表层级的并发写 exclusive，表层级的大锁，即这种sql语句可能会改变表的结构。 主要的，对于表而不是行数据，此类锁基本上分为改变表结构的锁和不改变表结构的锁。\n改变表结构的sql语句是\n1 alter table .... 在执行此类语句的时候，其它线程都不可以进行读写操作。\n对应的，像不改变表结构的读\n1 select 对应的，不改变表结构的写\n1 2 3 insert delete update 这些语句可以并发的写\n并发写 在 MDL(元数据锁) 的语境下，不是指两个线程同时修改同一行数据，而是指多个线程可以同时修改同一张表中的不同数据行，只要不涉及结构（DDL）更改，就不会互相阻塞。\n意向锁 意向锁是为了避免行级锁和表级锁产生冲突的一个快速检测方案。\n当事务要对某些行加锁时（比如 SELECT ... FOR UPDATE），InnoDB 会自动在表级别加一个“意向锁”（IS 或 IX）来声明： “我打算锁住这张表的某些行”。\n对于行级锁来说，其锁住的是某一行数据，此时，如果我们需要添加表锁，那么就得一行一行的去进行遍历，知道找到被加锁的行为止。\n当数据规模较小的时候，这样的方案可以接受，但是当数据规模过大，那么这样的操作就是一个耗时操作。\n所以，mysql的解决方案是，在添加行锁的时候就添加一个意向锁，这样就不需要进行耗时检测。\n只要理解了意向锁是行锁引起的，那么就好理解意向共享锁和意向排他锁。\n意向共享锁： 与表锁的读锁兼容，与写锁互斥 意向排他锁：与表锁共享锁和排他锁都互斥 行级锁 行级锁，每次锁住对应的行数据。锁定粒度最小，发生锁冲突的概率最低，并发程度最高。行级锁主要分为下面三个\n行锁：锁定单行记录的锁，防止其它事务对此进行update和delete，在RC RR隔离级别下都支持。\n间隙锁：锁定索引记录间隙，确保索引间隙不变，防止其它事务对这个间隙进行insert，产生幻读。在RR隔离级别下支持\n临键锁\n行锁和间隙锁的组合，锁住数据同时锁住前面的间隙Gap，在RR隔离级别下支持\n行锁 行锁里面可以分为共享锁和排他锁，其实也就还是读写锁。\n需要注意的一点是，无索引行锁升级为表锁。行锁针对是索引，如果字段没有索引的话，行锁就会升级为表锁。\n间隙锁\u0026amp;临健锁 默认情况下，InnoDB在RR事务隔离级别允许，InnoDB使用next-key锁进行搜索和索引扫描，防止幻读。\n总结 下次来看看还有哪些比较难的机制！\n","date":"2025-06-16T14:46:17+08:00","permalink":"https://XiaoPeng0x3.github.io/p/mysql%E8%A1%A5%E5%85%A8%E8%AE%A1%E5%88%92%E5%9B%9B/","title":"Mysql补全计划(四)"},{"content":"前言 今天开始进阶篇的内容。\n存储引擎 上图是mysql数据库的全部架构，按照分层的思路，可以分为\n连接层 服务层 引擎层 存储层 存储引擎介绍 在创建表时，可以指定使用的存储引擎，在默认情况下，mysql使用的是InnoDB存储引擎。\n查看当前支持的存储引擎 可以使用show engines来查看当前支持的引擎\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 mysql\u0026gt; show engines; +--------------------+---------+----------------------------------------------------------------+--------------+------+------------+ | Engine | Support | Comment | Transactions | XA | Savepoints | +--------------------+---------+----------------------------------------------------------------+--------------+------+------------+ | MEMORY | YES | Hash based, stored in memory, useful for temporary tables | NO | NO | NO | | MRG_MYISAM | YES | Collection of identical MyISAM tables | NO | NO | NO | | CSV | YES | CSV storage engine | NO | NO | NO | | FEDERATED | NO | Federated MySQL storage engine | NULL | NULL | NULL | | PERFORMANCE_SCHEMA | YES | Performance Schema | NO | NO | NO | | MyISAM | YES | MyISAM storage engine | NO | NO | NO | | InnoDB | DEFAULT | Supports transactions, row-level locking, and foreign keys | YES | YES | YES | | ndbinfo | NO | MySQL Cluster system information storage engine | NULL | NULL | NULL | | BLACKHOLE | YES | /dev/null storage engine (anything you write to it disappears) | NO | NO | NO | | ARCHIVE | YES | Archive storage engine | NO | NO | NO | | ndbcluster | NO | Clustered, fault-tolerant tables | NULL | NULL | NULL | +--------------------+---------+----------------------------------------------------------------+--------------+------+------------+ 11 rows in set (0.01 sec) 建表时指定引擎 之前的建表语句不变，发生变化的是在最后指定存储引擎的部分\n1 2 3 4 create table xxx ( ... ... ) engine = innodb; 主流存储引擎介绍 这里主要包括InnoDB、MyISAM和Memory这三种引擎。\nInnoDB存储引擎 在之前使用show engines命令中，有一些对于InnoDB存储引擎的介绍。\n1 | InnoDB | DEFAULT | Supports transactions, row-level locking, and foreign keys | YES | YES | YES | 其中一项描述很关键\nSupports transactions, row-level locking, and foreign keys\n支持事务、行级锁和外键。\ninnodb引擎的每张表都会对应一个表空间文件。存储该表的表结构、数据和索引。但是这种文件并不是一定会生成的。\n我们可以查询一下当前是否开启了。\n1 2 3 4 5 6 7 mysql\u0026gt; show variables like \u0026#39;innodb_file_per_table\u0026#39;; +-----------------------+-------+ | Variable_name | Value | +-----------------------+-------+ | innodb_file_per_table | ON | +-----------------------+-------+ 1 row in set, 1 warning (0.01 sec) 可以看到确实是开启了。\n逻辑存储结构 包括表空间、段、区、页、行。\n表空间：表空间是由多个段组成的，是存储引擎逻辑的最高层 段：常见段有数据段、索引段、回滚段等，一个段包含多个区。 区：区是表空间的单元结构，每个区的大小为1M。 页：页是组成区的最小单元，页是InnoDB存储引擎磁盘管理的最小单元，每个页的默认大小是16kb 行：数据存储是按照行来进行存储的。 MyISAM MyISAM的特点是\n不支持事务 不支持外键 不支持行级锁 支持表锁 对于启用了MyISAM存储引擎的表来说，其文件一般是以下面这几种格式结尾的\nxxx.sdi，存储表的结构信息 xxx.myd， 存储数据 xxx.myi，存储索引 Memory 基于内存的，使用哈希索引作为默认索引的数据。\n区别汇总 如何选择存储引擎 一般的，直接默认使用InnoDB即可。而MyISAM一般可以用来读和插入操作比较多的常见。而如果不希望将表数据持久化，这个时候就可以考虑使用Memory.\n索引 索引是一种特殊的数据结构，本质上是空间换时间的一种做法，对于数据库来说，索引一般是在引擎层实现的，不同的存储引擎也有着不同的支持策略。\nB-Tree B树是一种树的结构，其目的就是去避免二叉搜索树的退化和红黑树复杂实现而采用的一种策略。\n在B树中，叶子结点和非叶子结点都会存放数据。\nB+ Tree B+树是B树的改良版，B+树叶子结点才是存储的真正的数据，而非叶子结点只是起到索引的作用。\n而且，为了方便叶子结点之间的查找，叶子之间又组成了一个链表结构。\n这样设计的一个很大好处是，每一个非叶子结点只负责对数据进行索引，不负责存储，这样可以一级一级将整个树设计的非常大。\nMysql B+树索引 Mysql在B+树的基础上还将叶子结点的链表进行了优化设计。即将叶子结点之间使用双向链表进行相连，同时首尾链表也进行相连，这样在查找时更加高效。\nHash索引 Mysql还有一种索引是Hash索引，这种索引基本上是特指Memory引擎，不过InnoDB引擎也有自适应hash。\n关于hash算法，相信大家已经特别熟悉了，就不再都说了。\n索引分类 索引主要分为下面这几类\n主键索引,primary key 唯一索引，unique修饰 常规索引，可以通过create关键字来创建 全文索引，fulltext关键字，不常用，不进行讨论。 InnoDB的索引 聚集索引 聚集索引的意思是，索引和存储数据放在了一块，索引的叶子结点保存了行数据。\n这种索引是必须有的，而且只有一个。\n聚集索引选取规则：\n一般的，当指定主键后，主键索引就是聚集索引。 如果不存在主键，那么unique索引就是聚集索引 如果上述都没有，那么InnoDB会自动生成一个rowid作为隐藏的聚集索引。 二级索引 索引与数据分开存储，索引的叶子结点存储的值是对应数据行的主键。二级索引可以存在多个。\n按索引搜索的顺序 首先要看当前要搜索的字段是什么？假设这里有一个user表\n1 2 3 id 主键 name 二级索引 age 可以看到，如果我们是按照id进行搜索的，那么搜索语句看起来是这样的\n1 select * from user where id = ?; 因为主键是聚集索引，也就是说，这个索引的叶子结点就是对应的行数据，那么在进行搜索的时候，直接返回数据即可。\n如果我们要根据姓名来进行查询，那么sql语句可能如下\n1 select * from user where name = \u0026#39;xxx\u0026#39;; 由于姓名是二级索引，二级索引的索引与数据是分开存放的，所以在进行搜索时，最后查找到叶子结点的值是对应行数据的id。\n由于拿到的是主键值，所以，还需要再次去表中查找，这个过程就叫做回表。\n索引总结 InnoDB根据存储结构的不同，将索引分为聚集索引和二级索引。\n聚集索引每张表只有一个，当指定primary key或unique时主键时会自动创建。注意，此索引的叶子结点就是对应的行数据。所以查询这个索引可以直接找到对应的数据，不用回表再次查询。\n而二级索引获取的是一个主键，需要根据拿到的主键值再次进行回表操作，效率比聚集索引差。\n创建索引 下面来看看怎么创建索引\n给user表的age字段创建一个索引\n1 create index idx_user_age on user(age); 除此之外，还可以创建联合索引\n1 CREATE INDEX idx_user_pro_age_sta ON tb_user(profession,age,status); 查看表的索引 1 show index from user; 删除索引 1 drop index index_name on table_name; SQL性能分析 分析sql的执行频率 一般的，对于sql来说，无非就是一些读写操作。我们可以看看当前数据库哪些sql执行频率比较高\n1 show [session | global] status like \u0026#39;Com______\u0026#39;; -- 下划线的个数是6个 如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 mysql\u0026gt; show session status like \u0026#39;Com_______\u0026#39;; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | Com_binlog | 0 | | Com_commit | 0 | | Com_delete | 0 | | Com_import | 0 | | Com_insert | 0 | | Com_repair | 0 | | Com_revoke | 0 | | Com_select | 2 | | Com_signal | 0 | | Com_update | 0 | | Com_xa_end | 0 | +---------------+-------+ 这个有什么用呢？\n实际上，索引并不是一劳永逸的，他也是需要代价的！任何的效率高的查找本质上都是时间换空间。\n而且，请考虑一种场景，对于频繁的插入、删除、更新数据，也就意味着频繁的调整B+树，此时，索引甚至可能成为累赘。\n通过查询当前sql语句的执行频率，如果是读特别多的数据，那么建立索引就非常的合适。如果是写数据特别多，那么就要只给合适的字段添加索引！\n慢查询日志 慢日志是所有sql语句耗时超过指定时间(long_query_time)的一个记录日志。\n查看是否开启慢查询日志 1 2 3 4 5 6 7 mysql\u0026gt; show variables like \u0026#39;slow_query_log\u0026#39;; +----------------+-------+ | Variable_name | Value | +----------------+-------+ | slow_query_log | OFF | +----------------+-------+ 1 row in set, 1 warning (0.00 sec) 可以看到，并没有开启慢日志。\n查看慢日志存放的位置 1 2 3 4 5 6 7 mysql\u0026gt; show variables like \u0026#39;slow_query%\u0026#39;; +---------------------+---------------------------------------------+ | Variable_name | Value | +---------------------+---------------------------------------------+ | slow_query_log | OFF | | slow_query_log_file | D:\\mysql_file\\data\\localhost.log | +---------------------+---------------------------------------------+ 设置慢日志查询 通过修改全局参数的方式\n可以修改全局参数来开启慢日志查询\n1 set global slow_query_log=\u0026#39;ON\u0026#39;; 设置操作超时时间\n1 set global long_query_time=1; 通过修改配置文件\n一般是etc/my.cnf\n需要添加一下内容\n1 2 3 4 5 # 慢日志开关 slow_query_log=1 # 设置慢日志超时时间 long_query_time=2; 然后重启服务器\n1 systemctl restart mysqld 可以执行一个耗时操作看一下\n1 select sleep(3); 看一眼慢日志\n1 2 3 4 5 6 7 Time Id Command Argument # Time: 2025-06-16T09:45:16.135675Z # User@Host: root[root] @ localhost [::1] Id: 9 # Query_time: 3.000487 Lock_time: 0.000000 Rows_sent: 1 Rows_examined: 1 use test1; SET timestamp=1750067113; select sleep(3); 可以看到，在test1数据库下的select操作耗时越为3s，是一个慢日志。\nprofile详情 show profiles可以帮助我们了解事件都耗费到哪里去了，通过have_profiling参数可以查看当前Mysql是否支持profile优化\n查看是否支持profile优化 1 2 3 4 5 6 mysql\u0026gt; select @@have_profiling; +------------------+ | @@have_profiling | +------------------+ | YES | +------------------+ 查看是否开启\n1 2 3 4 5 6 7 mysql\u0026gt; select @@profiling; +-------------+ | @@profiling | +-------------+ | 0 | +-------------+ 1 row in set, 1 warning (0.00 sec) 开启profile 1 set session profiling = 1; 查看sql的耗时 在开启profile后，可以使用show profiles来查看当前执行过的所有sql的情况\n1 2 3 4 5 6 7 8 9 10 -- 查看每一条`sql`耗时的基本情况 mysql\u0026gt; show profiles; +----------+------------+---------------------------------+ | Query_ID | Duration | Query | +----------+------------+---------------------------------+ | 1 | 0.00029800 | select @@profiling | | 2 | 0.00042275 | select * from user | | 3 | 0.00045525 | select * from user where id = 1 | +----------+------------+---------------------------------+ 3 rows in set, 1 warning (0.00 sec) 查看具体的耗时时间 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 -- 查看指定`id`的各个阶段的耗时情况 mysql\u0026gt; show profile for query 3; +--------------------------------+----------+ | Status | Duration | +--------------------------------+----------+ | starting | 0.000131 | | Executing hook on transaction | 0.000008 | | starting | 0.000009 | | checking permissions | 0.000006 | | Opening tables | 0.000053 | | init | 0.000005 | | System lock | 0.000010 | | optimizing | 0.000013 | | statistics | 0.000074 | | preparing | 0.000016 | | executing | 0.000012 | | end | 0.000003 | | query end | 0.000003 | | waiting for handler commit | 0.000011 | | closing tables | 0.000009 | | freeing items | 0.000080 | | cleaning up | 0.000014 | +--------------------------------+----------+ 查看指定id的sql的cpu的使用情况 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 mysql\u0026gt; show profile cpu for query 1; +----------------------+----------+----------+------------+ | Status | Duration | CPU_user | CPU_system | +----------------------+----------+----------+------------+ | starting | 0.000139 | 0.000000 | 0.000000 | | checking permissions | 0.000007 | 0.000000 | 0.000000 | | Opening tables | 0.000019 | 0.000000 | 0.000000 | | init | 0.000006 | 0.000000 | 0.000000 | | optimizing | 0.000014 | 0.000000 | 0.000000 | | executing | 0.000015 | 0.000000 | 0.000000 | | end | 0.000003 | 0.000000 | 0.000000 | | query end | 0.000009 | 0.000000 | 0.000000 | | closing tables | 0.000003 | 0.000000 | 0.000000 | | freeing items | 0.000072 | 0.000000 | 0.000000 | | cleaning up | 0.000012 | 0.000000 | 0.000000 | +----------------------+----------+----------+------------+ 11 rows in set, 1 warning (0.00 sec) explain explain也是分析sql执行的一个好工具，我们只需要在对应的查询语句前加上explain或者desc即可。\n1 2 3 4 5 6 7 mysql\u0026gt; explain select * from user; +----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+ | 1 | SIMPLE | user | NULL | ALL | NULL | NULL | NULL | NULL | 5 | 100.00 | NULL | +----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+ 1 row in set, 1 warning (0.00 sec) 索引使用 最左前缀原则 在联合索引时，例如user表里面有\nname age status 这三个字段，我们可以给这三个字段创建联合索引\n1 create index idx_user_name_age_status on user (name, age, status); 索引生效的情况 下面这三种情况都是索引生效的\n1 select * from user where name = \u0026#39;xxx\u0026#39; and age = xxx and status = xxx; 这种情况也是生效的\n1 select * from user where name = \u0026#39;xxx\u0026#39; and age = xxx; 这种情况也是生效的\n1 select * from user where name = \u0026#39;xxx\u0026#39; and status = xxx; 但是这种情况是索引部分生效，也就是说，在查找数据时，跳过了age索引\n上面这几种情况都是索引生效的，即，我们将a,b,c三个字段作为联合索引\n1 2 3 4 a, b, c a, c a, b a 这都是索引生效的情况。\n索引失效情况 如果查询条件缺乏最左侧那个字段，就会造成索引失效，实际上，下面这段也是索引生效的\n1 b, c, a 只要存在最左侧那一列，就满足最左前缀原则，与字段先后顺序无关。\n范围查询导致索引失效 联合索引中，出现范围查询(\u0026gt;,\u0026lt;)，会导致范围查询右侧的列索引失效。\n1 select * from user where name = \u0026#39;xxx\u0026#39; and age \u0026gt; 20 and status = 1; age \u0026gt; 20这个范围查询会使status失效。\n而使用\u0026gt;=或\u0026lt;=这类的范围查询不会使得范围查询右侧的字段索引失效，所以应该尽量使用\u0026gt;=或者\u0026lt;=来进行范围查询，避免索引失效。\n索引失效的情况 索引列运算 如果对索引的字段进行运算操作，那么就会导致索引失败，例如，这里有一个substring函数，假设我们的name是一个单索引。\n精确匹配不会导致索引失效\n1 select * from user where name = \u0026#39;zxp1\u0026#39;; 对name字段进行再操作就会导致失效\n1 select * from user where name = substring(name, 10, 2) = \u0026#39;zx\u0026#39;; 字符串不加引号 对于一个字符串类型的字段，在查询的时候可以不加引号，看下面的一个例子。\n1 select * from user where phone_number = 1234567890; 和\n1 select * from user where phone_number = \u0026#39;1234567890\u0026#39;; 实际上，mysql会做隐式转换，但是，转换后索引并不会起作用，所以查询字符串类型时加引号\n模糊查询 在使用like进行模糊查询时，如果左侧没有通配符，那么索引还是可以生效的。\n1 select * from user where name = \u0026#39;zxp%\u0026#39;; 上面这个语句是可以走索引进行查询的。\n下面这两个都是索引失效的\n1 select * from user where name = \u0026#39;%xp\u0026#39; 和\n1 select * from user where name = \u0026#39;%xp%\u0026#39;; or连接条件 如果or左右两侧需要查询的字段有一个没有索引的话，就会造成索引失效。\n1 select * from user where name = \u0026#39;zxp1\u0026#39; or age = 18; 如果name字段有索引而age字段没有的话，那么name和age都不会去走索引，这就要求两边字段都要有索引。\n数据分布影响 索引在大量数据时才会体现出其优势，如果数据量很小，有些时候直接全表扫描也会很快。所以，在mysql执行的时候，其内部就会进行评价。\n如果它认为走索引不如全表扫描，那么索引就会失效。\nSQL提示 SQL语句中可以加入一些人为提示来进行操作优化。\nuse index\n建议mysql使用哪个索引来完成查询，仅仅是建议，不一定使用，内部还会进行评估。\n1 select * from user use index(idx_user_name) where name = \u0026#39;zxp\u0026#39;; ignore index\n查询时忽略某个索引\n1 select * from user ignore index (idx_user_name) where name = \u0026#39;zxp\u0026#39;; force index\n查询是强制使用索引\n1 select * from user force index (idx_user_name) where name = \u0026#39;zxp\u0026#39;; 覆盖索引 覆盖索引指的是需要查询的字段全部都是索引值，不需要回表。\n对于其它字段来说，因为聚集索引只能有一个(多余会造成数据冗余)，所以其它都是二级索引。\n二级索引中，叶子结点全部都存储的是索引字段值+主键id。\n例如，对于，对于name字段来说，其叶子结点就是(name，id)。\n前缀索引 当需要创建的字段值是字符串或者text类型的时候，全部把字段值作为索引来使用会导致占用大量的空间，此时就可以使用前缀索引来进行空间优化。\n前缀索引的意思是，使用一部分前缀来作为索引值\n例如，我们要使用email的前7个字符作为索引\n1 create index idx_email_7 on user(email(7)); 单列索引和联合索引 单列索引即索引只包含了单个列，联合索引是指索引包含了多个列。\n需要注意的是，如果查询的字段都是单列索引，那么只会选择其中的一个进行查询。\n相反，如果两个字段是联合索引的话，就可以全部查询到，从而避免二次回表查询。\n索引设计原则 数据量大，读多写少的操作建立索引 经常作为查询条件的字段应该建立索引 字符串类型的字段，可以考虑建立前缀索引 尽量使用联合索引，可以避免回表查询 索引不是越多越好！ 设计表时，尽量给每个字段添加一定的约束，像unique,not null等 总结 这部分是mysql进阶的第一部分！\n","date":"2025-06-15T12:46:17+08:00","permalink":"https://XiaoPeng0x3.github.io/p/mysql%E8%A1%A5%E5%85%A8%E8%AE%A1%E5%88%92%E4%B8%89/","title":"Mysql补全计划(三)"},{"content":"前言 这次继续来写补全计划。\n表设计原则 很多情况下，在开发业务时就需要制定好表的格式，一般来说，各个表之间也存在着各种各样的联系，基本可以分为\n一对一结构 一对多或者多对一 多对多 一对多结构 一对多结构是很常见的一种关系，例如，一个部门会有很多个员工，而一个员工一般只属于一个部门。\n对于这种关系来说，一般是设计两张表，一张是员工表，另一张是部门表，其中，在“多的一方”(一对多的多)建立外键，指向“一”的主键(外键必须是unique或者primary key)\n多对多的关系 学生与课程的关系 一个学生可以选修多门课程，一个课程也可被多个学生选择 建立第三张中间表，中间表至少包含两个外键，关联两方的主键。 例如，将学生表和课程表的学生id字段和课程表的id字段抽取出来，组合为一张新的表。当学生需要休学时，这个学期的课表就被设置为空；同理，当选课学生过少时，也可以把这门课给取消\n一对一的关系 一对一关系一般多见于像用户基本信息的表种，一般的，用户个人信息有很多，像年龄，身高、体重；像学历水平、毕业院校、毕业时间等。如果把这些用户信息全部存在一起，会导致使用时不够灵活。可以使用以下的方法。\n将用户个人信息进行拆分，个人信息可以分为：用户基本信息、用户教育信息、用户家庭信息等。我们只需要在用户教育信息和用户家人信息中新增用户基本信息的一个主键即可。\n多表查询 多表查询可以分为连接查询和子查询两类。\n连接查询 内连接 内连接是查询两张表的公共部分，也就是交集部分。这里有两种语法，我们还以员工和部门表来作为案例演示一下。\n例如，我们希望查询员工的个人信息和员工的部门，员工表只有部门表的一个外键，所以只查询员工表是无法查询出员工所属的部门，必须对两张表进行联合查询才可以。\n隐式内连接\n1 select 字段 from 表1， 表2 where ....; 显式内连接\n1 select 字段 from 表1 [inner] join 表2 on 连接条件 例如，使用内连接查询员工的姓名和部门\n1 select e.name, d.name from emp as e join dept as d on e.dept_id = d.id; 外连接 外连接包括左外连接和右外连接。左外连接查询左表所有的数据，右外连接查询右表所有的数据。\n之所以分左右，是因为在有些情境下，我们需要查询在左表中出现了但是有表中没有出现的内容。左右连接之间可以相互转换。\n子查询 子查询的含义是将查询结果进行进一步筛选后可以将查询的结果作为数据源进行筛选。子查询的使用比较灵活，但是效率不高，这里不在详细展开讲解。\n事务 事务(transaction)是一组操作的集合，对于事务来说，也就是这一组操作，要么同时成功执行，要么就是失败执行，不存在事务中的事务有些执行成功了，有些执行失败了的情况。\n查看事务/设置事务的提交方式 这里有一个全局设置@@autocommit，对于事务来说，默认是自动提交的\n1 2 3 4 5 6 7 mysql\u0026gt; select @@autocommit; +--------------+ | @@autocommit | +--------------+ | 1 | +--------------+ 1 row in set (0.00 sec) 我们把这个变量设置为0，这样事务就会变为手动提交\n1 set @@autocommit = 0; 此时，如果要进行更新删除操作，必须要手动提交事务才会更新到数据库。\n提交事务 1 commit; 回滚事务 1 rollback; 开启事务 开启事务的意思是，执行开启事务代码后，此后一系列的更新操作必须要提交事务后才可以更新。\n1 2 start transaction; begin; 上面两行代码都可以开启事务\n事务四大特性 ACID\nA：Atomicity，原子性，事务是不可分割的最小执行单元，不存在中间态 C: Consistency，一致性，事务完成后，所有的数据都是一致状态 I: Isolation: 隔离性，事务不受外界并发操作的影响 D: Durability: 持久性，事务提交或回滚后，它对数据库中的数据改变就是永久的 并发事务问题 这是指事务不同的隔离等级所导致的，这个时候就会引起各种各样奇怪的读问题\n脏读/read uncommit 脏读的意思是读到了未提交事务的数据。例如，在并发操作下，这里有一个变量money，假设此时余额是100，对于A来说，他消费了50元，此时数据还未提交。这个时候，B也来进行消费了，他想要消费100元，由于此时的事务隔离级别是read uncommit，此时B发现只剩50元，所以操作失败。\n如果A操作成功的话，那么B这样做实际上是没有问题的，但是假设银行服务器出现了波动，那么A事务操作失败，就要发生回滚，也就是说，此时A消费失败，B可以进行消费，但是B读到了未提交的数据，造成消费失败。\n不可重复读/ read commit 不可重复读的意思是，在一个事务中，前后两次读到的同一个变量的值不相同。这是很有可能发生的，也就是说，事务A可以读到事务B对同一变量的修改。实际上这样做是有很大问题的。\n幻读 / repetable read 幻读的意思是，第一次读的时候发现某某数据不存在，然后插入数据时又发现数据存在了，像是幻觉一样。\n思考一种常见，假设A要插入一条员工信息，插入时先查看一下是否存在该用户，发现不存在，这个时候B插入了这条信息，此时A再插入，发现已经存在了。\n其实，调整隔离级别就可以避免这些问题。\n隔离级别 对于这些隔离级别来说，从上往下，数据库的并发能力越差，性能也就越差，但是解决的问题也越多。\n查看隔离级别 可以查看当前数据库的隔离级别\n1 select @@transaction_isolation; 可以看到，默认情况下，mysql使用的数据库隔离级别是\n1 2 3 4 5 6 7 mysql\u0026gt; select @@transaction_isolation; +-------------------------+ | @@transaction_isolation | +-------------------------+ | REPEATABLE-READ | +-------------------------+ 1 row in set (0.00 sec) 可重复读，也就是说，可以解决脏读和不可重复读问题。\n修改隔离级别 1 2 mysql\u0026gt; set session transaction isolation level read uncommitted; Query OK, 0 rows affected (0.00 sec) 此时再查看一下当前数据库的隔离级别\n1 2 3 4 5 6 7 mysql\u0026gt; select @@transaction_isolation; +-------------------------+ | @@transaction_isolation | +-------------------------+ | READ-UNCOMMITTED | +-------------------------+ 1 row in set (0.00 sec) 注意，这只是当前session的所有数据库隔离级别，如果要以后永久修改的话，可以使用global\n1 2 mysql\u0026gt; set global transaction isolation level read uncommitted; Query OK, 0 rows affected (0.00 sec) 总结 基础2就是这样了，下面是进阶内容！\n","date":"2025-06-14T22:46:17+08:00","permalink":"https://XiaoPeng0x3.github.io/p/mysql%E8%A1%A5%E5%85%A8%E8%AE%A1%E5%88%92%E4%BA%8C/","title":"Mysql补全计划(二)"},{"content":"前言 MySQL作为一个开源流行的数据库，是我们必不可少的学习工具，此系列是基础到进阶再到一些高级的操作的教程。\n启动与连接 在正确安装Mysql后，首先要确保mysql服务在正常运行，在cmd命令行中，可以快捷的启动mysql的服务\n1 2 3 4 # 启动服务 net start mysql # 停止服务 net stop mysql 启动后，我们还需要进行连接，这里的连接实际上都是根据网络来进行连接的，在cmd控制台中，可以根据以下格式去进行连接\n1 mysql -h[ip] -p[port] -u root -p 如果是本地(localhost)，那么在启动的时候不用带上ip和port这两个参数 -u的意思是，你作为哪个用户进行登录 -p的意思是，登录的时候需要指定密码(password) 例如，我们要登录本地的mysql,就可以这样写\n1 mysql -uroot -p3306 也可以这样登录\n1 mysql -h127.0.0.1 -p3306 -uroot -p SQL DDL 数据定义语言。\n创建数据库 查询所有数据库\n1 show databases; 查询当前所使用的数据库\n1 select database(); 创建数据库\n一般的，最简单的创建数据库的语句是\n1 create database [database_name]; 但是实际上，这里还可以存在着一些额外的判断条件，例如，如果想要创建的数据库与已经存在的一个数据库名称相同，那么就会发生错误，我们可以使用if not exist来进行判断\n例如，创建一个名字叫做test1的数据库\n1 create database if not exists test1; 此外，我们还可以指定创建数据库时所使用的字符集。\n1 create database if not exists test1 default charset utf8mb4; utf8mb4是最常见的字符集，这个字符集支持的字符特别多，甚至还包含了emoji。\n而另外一个常见的参数就是比较规则，实际上，常用的排序规则有两个\nutf8mb4_general_ci：更快，但对某些字符排序不够精细。\nutf8mb4_unicode_ci：更准确，但略慢。\n所以，一个更加完整的写法是\n1 2 3 create database if not exists test1 default charset utf8mb4 default collate utf8mb4_general_ci; 在开发环境中，一个更加清楚的SQL语句是一个好的习惯。\n删除数据库 删除数据库的关键字是drop，一般的，删除一个数据库可以这样写\n1 drop database test1; 同理，如果这个数据库不存在，那么删除也会出现错误，可以使用If exists进行判断\n1 drop database if exists test1; 查看表结构 查看表的结构是一个常用操作，这里有两种方式\ndesc\n1 desc [table_name]; 例如，查看表user的字段结构，可以使用desc user。\nshow columns from [table_name]\n1 show columns from user; 推荐desc，简单好记！\n查询指定的建表语句 可以使用show create table [table_name]这个关键字来查看创建表的时候的sql语句\n1 show create table user; 数值类型 常见的数据类型就是那几大类，整形、浮点型、字符串型还有时间类型，更加细分的来说，可以分为下面的几种。可以参考c语言的各种数据类型\n整形 tinyint,有点像char类型，1 byte，范围是[-128, 127]， smallint,类比short类型，2 bytes mediumint，3 bytes int，类比int, 4 bytes bigint,类比long,8 bytes 浮点数 float，4 bytes\ndouble, 8 bytes\ndecimal, 其大小依赖于精度，注意，所有与钱有关的数据都应该使用decimal来使用。\n字符串类型 常用的数据类型有char varchar text\n日期时间相关 其中，比较有趣的是时间戳类型，当我们向表里面插入字段时，使用时间戳可以帮助我们记录下来这个数据的插入时间\n1 2 3 4 5 6 CREATE TABLE user ( id INT PRIMARY KEY AUTO_INCREMENT, name VARCHAR(50), create_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP, -- 使用当前时间 update_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP ); 修改表字段 修改表内字段的排列顺序、数据类型和新建字段这些操作都是与表修改有关的操作，这些修改的字段都有一个公共前缀\n1 alter table [table_name] .... 新添字段 使用add来进行添加操作\n例如，要向user表中添加一个新的字段，例如height身高字段。\n1 alter table user add height double 修改数据 修改数据所使用的关键字是update，例如\n1 update user set name = xxx, age = xxx, .... where xxx = xxx;(后面是过滤条件) 建表语句 创建表的关键字是create，例如，要创建一个user表\n1 2 3 4 create table user( name varchar(50), age int ); 建表时，一个很关键的内容就是去添加约束，下面来根据一个具体的例子，来看看怎么添加约束。\n外键约束 为了防止误导，这里在建表时就添加上约束。\n首先考虑一种情形，即用户和用户所在的部门的这种情形\n员工表应当包含员工的各种信息，像姓名、年龄、所在的职业、入职时间和所在的部门等 部门应该包含部门的名称 也就是说，部门表和员工表这两个看起来没什么关系，但是，如果我们不加以外键约束，那么设想一种情况，当解散某个部门时，首先从部门表里面删除这个部门信息，如果不添加外键约束，那么删除部门表的操作并不会影响到员工表，但是员工表中还包含有部门信息。\n首先来创建这两张表\n员工表\n1 2 3 4 5 6 7 CREATE TABLE User( id INT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(50) NOT NULL, age INT, job varchar(20) comment \u0026#39;职位\u0026#39;, dept_id int comment \u0026#39;部门id\u0026#39; ); 部门表\n1 2 3 4 CREATE TABLE dept( id INT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(50) NOT NULL ); 例如，我们要执行一个删除操作\n1 delete from dept where dept_id = 1; 此时，由于外键存在约束，不允许我们删除。\n外键约束可以在建表时添加，也可以在建表后补充添加。\n1 2 3 4 5 6 creat table User( ...... constraint fk_user_dept_id foreign key (dept_id) renferences dept (id); ); 建表后添加\n1 alter table user add constraint fk_user_dept_id foreign key (dept_id) references dept (id); 可以这样理解\n外键是一种约束，constraint user表的dept_id是外键 这个外键是对dept表的id字段的引用。 同理，可以使用下面的命令来删除外键\n1 alter table user drop foreign key fk_user_dept_id; 删除更新操作 如果真的要删除数据呢？\n这个时候，还可以再添加一些约束。\n如果我们想要达成，删除部门时，也可以删除其它员工的操作，就可以在设置外键的时候设置为Cascade。\n1 alter table user add constraint fk_user_dept_id foreign key (dept_id) references dept (id) on update cascade on delete cascade; 管理用户 在之前登入数据库时，我们都是使用root用户进行登录的，众所周知，root权限具有所有权限，有时候我们不希望有一些删库跑路的操作，这时，就可以对外提供一些用户。\n查询用户 1 select * from mysql.user; 查询当前所有的用户，不过这个结构展示的东西太多了，我们可以先查看两个字段，一个是Host和User。\n1 2 3 4 5 6 7 8 9 mysql\u0026gt; select host, user from mysql.user; +-----------+------------------+ | host | user | +-----------+------------------+ | localhost | mysql.infoschema | | localhost | mysql.session | | localhost | mysql.sys | | localhost | root | +-----------+------------------+ 可以看到，当前用户就一个，就是root用户。其它都是一些权限。\n1 2 3 4 5 +--------------------------+-----------------------------------+------+-----+-----------------------+-------+ | Host | char(255) | NO | PRI | | | | User | char(32) | NO | PRI | | | | Select_priv | enum(\u0026#39;N\u0026#39;,\u0026#39;Y\u0026#39;) | NO | | N ........ 创建用户 创建用户\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 create user \u0026#39;用户名\u0026#39;@\u0026#39;主机名\u0026#39; identified by \u0026#39;密码\u0026#39;; mysql\u0026gt; create user \u0026#39;zxp\u0026#39;@\u0026#39;localhost\u0026#39; identified by \u0026#39;123456\u0026#39;; Query OK, 0 rows affected (0.03 sec) mysql\u0026gt; select host, user from mysql.user; +-----------+------------------+ | host | user | +-----------+------------------+ | localhost | mysql.infoschema | | localhost | mysql.session | | localhost | mysql.sys | | localhost | root | | localhost | zxp | +-----------+------------------+ 查看当前登录的用户\n1 2 3 4 5 6 7 mysql\u0026gt; select user(); +---------------+ | user() | +---------------+ | zxp@localhost | +---------------+ 1 row in set (0.00 sec) 修改用户的密码\n注意是root用户\n1 alter user \u0026#39;zxp\u0026#39;@\u0026#39;localhost\u0026#39; identified with mysql_native_password by \u0026#39;新密码\u0026#39;; 注意注意，这只是创建用户，用户的权限都还没有被分配。\nTips\n在很多时候，我们需要远程连接主机，此时，就需要匹配远程主机\n1 2 3 4 5 6 7 8 9 10 mysql\u0026gt; select host, user from mysql.user; +-----------+------------------+ | host | user | +-----------+------------------+ | localhost | mysql.infoschema | | localhost | mysql.session | | localhost | mysql.sys | | localhost | root | | localhost | zxp | +-----------+------------------+ host的配置意味着可以连接的ip地址，这里默认都是localhost，你可以配置为自己常用的ip地址，这样就可以远程登录主机\n此外，还可以使用%来进行统配匹配，%意味着任意ip地址的主机都可以连接。\n用户授权 权限控制\n查询权限\n可以查询root用户所有的权限。\n1 show grants for \u0026#39;用户名\u0026#39;@\u0026#39;主机名\u0026#39;; 授权\n可以授予各种权利。\n1 grant 权限 on 数据库名.表名 to \u0026#39;用户名\u0026#39;@\u0026#39;主机名\u0026#39;; 例如，可以向zxp授予test1.user的增删改查能力。\n1 2 3 4 5 6 7 8 mysql\u0026gt; use test1 Database changed mysql\u0026gt; create table user( -\u0026gt; id int auto_increment primary key, -\u0026gt; name varchar(50), -\u0026gt; age int -\u0026gt; ); Query OK, 0 rows affected (0.03 sec) 使用zxp进行登录\n1 mysql -p3307 -uzxp -p 登录进来后，发现并没有database\n1 2 3 4 5 6 7 8 mysql\u0026gt; show databases; +--------------------+ | Database | +--------------------+ | information_schema | | performance_schema | +--------------------+ 2 rows in set (0.00 sec) 这是因为root没有给用户授权。我们可以授予zxp各种权限\n1 2 mysql\u0026gt; grant select on test1.* to \u0026#39;zxp\u0026#39;@\u0026#39;localhost\u0026#39;; Query OK, 0 rows affected (0.01 sec) 重新登录，发现显示了test1\n1 2 3 4 5 6 7 8 9 mysql\u0026gt; show databases; +--------------------+ | Database | +--------------------+ | information_schema | | performance_schema | | test1 | +--------------------+ 3 rows in set (0.00 sec) 我们可以查询一下试试。\n1 2 3 4 5 6 7 8 9 10 11 12 mysql\u0026gt; use test1; Database changed mysql\u0026gt; show tables; +-----------------+ | Tables_in_test1 | +-----------------+ | user | +-----------------+ 1 row in set (0.00 sec) mysql\u0026gt; select * from user; Empty set (0.00 sec) 可以看到查询不出数据(这是因为还没有插入数据)。\n使用root添加一些数据\n1 2 3 4 5 6 7 8 9 10 11 12 13 mysql\u0026gt; insert into user(name, age) values (\u0026#39;zxp1\u0026#39;, 18), (\u0026#39;zxp2\u0026#39;, 19), (\u0026#39;zxp3\u0026#39;, 20); Query OK, 3 rows affected (0.01 sec) Records: 3 Duplicates: 0 Warnings: 0 mysql\u0026gt; select * from user; +----+------+------+ | id | name | age | +----+------+------+ | 1 | zxp1 | 18 | | 2 | zxp2 | 19 | | 3 | zxp3 | 20 | +----+------+------+ 3 rows in set (0.00 sec) 此时再在zxp中查询一下\n1 2 3 4 5 6 7 8 9 mysql\u0026gt; select * from user; +----+------+------+ | id | name | age | +----+------+------+ | 1 | zxp1 | 18 | | 2 | zxp2 | 19 | | 3 | zxp3 | 20 | +----+------+------+ 3 rows in set (0.00 sec) 我们可以尝试一下使用zxp用户进行插入数据\n1 2 mysql\u0026gt; insert into user(name, age) values (\u0026#39;zxp4\u0026#39;, 22), (\u0026#39;zxp5\u0026#39;, 23); ERROR 1142 (42000): INSERT command denied to user \u0026#39;zxp\u0026#39;@\u0026#39;localhost\u0026#39; for table \u0026#39;user\u0026#39; 可以看到操作直接被拒绝了，一般的，可以给用户查询、新增、更新的权限，而不给与删除权限。\n1 2 3 4 5 6 7 8 mysql\u0026gt; show grants for \u0026#39;zxp\u0026#39;@\u0026#39;localhost\u0026#39;; +-----------------------------------------------------------------------+ | Grants for zxp@localhost | +-----------------------------------------------------------------------+ | GRANT USAGE ON *.* TO `zxp`@`localhost` | | GRANT SELECT, INSERT, UPDATE, ALTER ON `test1`.* TO `zxp`@`localhost` | +-----------------------------------------------------------------------+ 2 rows in set (0.00 sec) 此时，我们再使用zxp去更新数据试一下\n1 2 3 mysql\u0026gt; insert into user(name, age) values (\u0026#39;zxp4\u0026#39;, 22), (\u0026#39;zxp5\u0026#39;, 23); Query OK, 2 rows affected (0.01 sec) Records: 2 Duplicates: 0 Warnings: 0 芜湖！更新成功了！\n来试一下删除时会怎么样！\n1 2 mysql\u0026gt; delete from user where id = 1; ERROR 1142 (42000): DELETE command denied to user \u0026#39;zxp\u0026#39;@\u0026#39;localhost\u0026#39; for table \u0026#39;user\u0026#39; 可以看到不允许删除\n试一下删除整张表试一试\n1 2 mysql\u0026gt; drop table user; ERROR 1142 (42000): DROP command denied to user \u0026#39;zxp\u0026#39;@\u0026#39;localhost\u0026#39; for table \u0026#39;user\u0026#39; 可以看到，也不允许删除。\n取消授权 授权后还可以取消授权。\n例如，取消插入数据的权限\n1 2 mysql\u0026gt; revoke insert on test1.* from \u0026#39;zxp\u0026#39;@\u0026#39;localhost\u0026#39;; Query OK, 0 rows affected (0.01 sec) 此时，我们可以再尝试一下插入数据试一试\n1 2 mysql\u0026gt; insert into user(name, age) values (\u0026#39;zxp4\u0026#39;, 22), (\u0026#39;zxp5\u0026#39;, 23); ERROR 1142 (42000): INSERT command denied to user \u0026#39;zxp\u0026#39;@\u0026#39;localhost\u0026#39; for table \u0026#39;user\u0026#39; 可以看到权限被禁止了。\n总结 补全计划(一)到此为止，欢迎交流！\n","date":"2025-06-13T22:46:17+08:00","permalink":"https://XiaoPeng0x3.github.io/p/mysql%E8%A1%A5%E5%85%A8%E8%AE%A1%E5%88%92%E4%B8%80/","title":"Mysql补全计划(一)"},{"content":"前言 在完成一个接口功能的开发后，对其测试是必不可少的，一般的，测试连通性或者对一些数据进行验证时，可以使用postman这些功能，更常见的，如果我们想要测试一个接口的性能，就可以使用jmeter来进行测试。\nGet请求测试 在安装后jmeter后，就可以使用命令行来启动jmeter。在cmd中，直接输入jmeter\n1 jmeter 如果正确安装后，就可以启动图形页面。\n进来后的界面如下，注意到左上角有个测试计划\n右键这个测试计划，就可以添加我们需要的测试。\n新建一个线程组后，就可以配置线程组的一些信息。\n下面来分析一下这些参数的意义：\n线程数\n线程数越高，意味着测试的并发度也就越高。\nramp-up\n这个是启动时间。例如，当ramp-up设置为10秒，线程数设置为100时，也就意味着10秒内会启动100个线程（不是1秒中启动十个）\n循环次数\n这个就比较好理解了，假设线程数设置为100，那么这100个线程执行完可以视为一次循环。\n测试Get请求\n在localhost:8081地址运行这一个简单的web应用\n1 2 3 4 @GetMapping(\u0026#34;/hello\u0026#34;) public Result\u0026lt;String\u0026gt; hello() { return Result.success(\u0026#34;Hello\u0026#34;); } 以此，来演示怎么进行Get请求测试。\n在建立好线程组后，可以添加一个http请求\n其页面是这样的：\n需要我们填写的地方有：\n协议 服务器或IP 端口号 例如，当前的sayHello程序运行在localhost:8081，在服务器IP栏，我们可以填入localhost，端口号就是8081，注意到下面还有一个路径，也就是我们映射的地址“/hello\u0026quot;。\n查看结果\n为了更好的显示结果，可以添加查看结果树和汇总报告。\n查看结果树：可以方便的看到在测试过程中请求和响应数据\n汇总报告：可以提供一种数据化的方式来评估测试，例如QPS、平均响应等\n这些必要东西已经设置好了，我们可以点击测试按钮来进行测试。\n这里设置总线程数为100，启动时间为5s，循环2次。\n下面是结果\n首先是汇总报告\n然后是结果树\n测试Post请求 与Get请求不同的是，Post请求一般是带数据的，这里，数据的来源可以有两部分。\n这里有一个简单的计算BMI的程序\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 @PostMapping(\u0026#34;user/bmi\u0026#34;) public Result\u0026lt;String\u0026gt; calcBMIController(@RequestBody User user) { // 体重单位是千克 // 身高单位是厘米 double userBMI = user.getWeight() / Math.pow(user.getHeight() / 100, 2); user.setBmi(userBMI); // 插入到数据库 return Result.success(user.toString()); } public class User { private int id; private String name; private int age; private double height; private double weight; private double bmi; public int getId() { return id; } public void setId(int id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } public double getHeight() { return height; } public void setHeight(double height) { this.height = height; } public double getWeight() { return weight; } public void setWeight(double weight) { this.weight = weight; } public double getBmi() { return bmi; } public void setBmi(double bmi) { this.bmi = bmi; } public User(int id, String name, int age, double height, double weight, double bmi) { this.id = id; this.name = name; this.age = age; this.height = height; this.weight = weight; this.bmi = bmi; } public User() { } } 首先是在jmeter中直接数据写入数据。\n然后，奇怪的事情发生了，数据格式不支持！\n{\u0026ldquo;timestamp\u0026rdquo;:\u0026ldquo;2025-06-12T16:06:41.924+00:00\u0026rdquo;,\u0026ldquo;status\u0026rdquo;:415,\u0026ldquo;error\u0026rdquo;:\u0026ldquo;Unsupported Media Type\u0026rdquo;,\u0026ldquo;path\u0026rdquo;:\u0026quot;/user/bmi\u0026quot;}\n其实这是因为后端不知道我们发送的是否是json类型数据的格式，我们必须在请求头里面声明\n1 application/json; charset=UTF-8 可以看到，请求头确实不是我们预期的那样\n我们可以设置请求头：\n再次发送一下请求看一看。\n可以看到，请求全部通过了。\n总结 这次分享就到这里了，后续看一看怎么导入外部数据来进行测试。\n","date":"2025-06-12T22:57:33+08:00","permalink":"https://XiaoPeng0x3.github.io/p/jmeter%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B1%E5%9F%BA%E6%9C%AC%E6%B5%8B%E8%AF%95/","title":"Jmeter使用案例(1):基本测试。"},{"content":"注册register 在开发注册功能是有一下几个功能需要实现：\n前端传递User数据需要更新到user表里面 参数检查 难点在于怎么实现UserService里面的内容，我们可以把需求来拆分一下。\n发送邮件验证 在注册时要填入自己的邮箱，这里我们需要使用Springboot的email邮件服务。\n这里我使用qq邮箱的smtp服务进行开发，开通邮箱smtp服务的教程网上有很多，这里就不去写了。\n配置信息 1 2 3 4 5 6 7 spring.mail.host=smtp.qq.com spring.mail.port=465 spring.mail.username=xxxxx@qq.com spring.mail.from=xxxxxx@qq.com spring.mail.password=xxxxxx # 这里是生成的邮箱校验码，不是邮箱的登录密码 spring.mail.properties.mail.smtp.auth=true spring.mail.properties.mail.smtp.ssl.enable=true 在配置好后，我们还要使用mail服务，下面是mail服务的maven坐标\n1 2 3 4 5 \u0026lt;!-- 邮箱服务 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-mail\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 可以将这个服务封装为util类\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 @Component public class EmailSendService { private static final Logger log = LoggerFactory.getLogger(EmailSendService.class); // 发送的客户端 @Autowired private JavaMailSender mailSender; // 发送的邮件信息 @Value(\u0026#34;${spring.mail.from}\u0026#34;) private String from; /** * * @param to * @param subject * @param content */ public void sendSimpleMail(String to, String subject, String content) { // 创建发送信息 MimeMessage mimeMessage = mailSender.createMimeMessage(); MimeMessageHelper mimeMessageHelper = new MimeMessageHelper(mimeMessage); try { mimeMessageHelper.setFrom(from); log.info(\u0026#34;from: \u0026#34; + from); mimeMessageHelper.setTo(to); mimeMessageHelper.setSubject(subject); mimeMessageHelper.setText(content, true); // true表示支持html的内容 mailSender.send(mimeMessage); log.info(\u0026#34;发送成功\u0026#34;); } catch (Exception e) { log.error(\u0026#34;发送失败{}\u0026#34;, e.getMessage()); } } } 其实这些都是固定写法，不必要去纠结，能看懂即可。\nAOP字段封装 在discuss_post和user等其它实体类里面经常有create_time这个字段，如果我们每次都去在创建这些对象的时候其实不太好管理，因此我们可以使用aop来实现这些公共方法\n自定义注解 根据AOP的知识，我们以可以自定义注解+Aspect类的方式来实现\n1 2 3 4 5 6 7 8 9 10 11 package com.zxp.nowcodercommunity.annotation; import java.lang.annotation.ElementType; import java.lang.annotation.Retention; import java.lang.annotation.RetentionPolicy; import java.lang.annotation.Target; @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.METHOD) public @interface AutoCreateTime { } 自定义Aspect类\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 package com.zxp.nowcodercommunity.aspect; import org.aspectj.lang.ProceedingJoinPoint; import org.aspectj.lang.annotation.Around; import org.aspectj.lang.annotation.Aspect; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.stereotype.Component; import java.lang.reflect.Field; import java.time.LocalDateTime; @Aspect @Component public class CreateTimeAspect { private static final Logger log = LoggerFactory.getLogger(CreateTimeAspect.class); @Around(\u0026#34;@annotation(com.zxp.nowcodercommunity.annotation.AutoCreateTime)\u0026#34;) public Object createTime(ProceedingJoinPoint joinPoint) throws Throwable { log.info(\u0026#34;开始createTime字段填充\u0026#34;); // 通过反射来获取运行时对象 Object[] args = joinPoint.getArgs(); if (args.length \u0026gt; 0) { // 获取第一个参数 Object entity = args[0]; Field[] fields = entity.getClass().getDeclaredFields(); for (Field field : fields) { if (\u0026#34;createTime\u0026#34;.equals(field.getName())) { // 只处理 createTime 字段 field.setAccessible(true); if (field.get(entity) == null) { // 只有 createTime 为空时才赋值 field.set(entity, LocalDateTime.now()); } break; } } } return joinPoint.proceed(); } } 然后在需要填充这些公共字段的方法上加上我们的注解，例如，可以在用户注册成功后加上该注解\n1 2 @AutoCreateTime void insertUser(User user); 总结 开发注册功能的难点就是以上几点！\n","date":"2025-03-26T21:53:53+08:00","permalink":"https://XiaoPeng0x3.github.io/p/03-%E6%B3%A8%E5%86%8C%E5%8A%9F%E8%83%BD%E5%BC%80%E5%8F%91/","title":"03 注册功能开发"},{"content":"Tips commit 未提交完所有的文件 当文件足够多时，有时候我们会忘记将所有文件提交到一个git记录里面而执行git commit。这个时候有两种做法：\n重新提交一次commit记录\n1 git add xxx xxx 然后再提交\n1 git commit 这样会导致多出现一次commit，比较难以接受\n合并到一次commit\ncommit有一个参数就是去修改上次提交的信息，包括提交的message和文件\n先提交未提交的文件到暂存区\n1 git add xxx xxx 然后commit\n1 git commit --amend --no-edit # 表示不修改`commit`信息 这样就可以将之前未提交的文件添加进来\n","date":"2025-03-26T21:46:17+08:00","permalink":"https://XiaoPeng0x3.github.io/p/git%E5%B0%8F%E6%8A%80%E5%B7%A7/","title":"Git小技巧"},{"content":"首页开发 对于首页开发来说，要做到下面两点：\n确定好每页的页数，最好是有一个默认值\n前端可以选择各种成熟的element-ui即可\n确定好前端返回的数据类型\n页面参数 这是前端api的请求\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 /** * 获取帖子 * @param {int} current 当前页码 * @param {int} limit 每页条数 * @param {int} orderMode 排序模式，0 最新 1 热门 * @returns */ static async getDiscussPosts(current = 1, limit = 5, orderMode = 0) { return get(\u0026#39;/index\u0026#39;, { current, limit, orderMode }) } 前端会使用get方法来向后端传递三个参数，分别是：\n当前的页数:current\n每页最大帖子数：limit\n展示帖子的优先级：orderMode\n这个参数的意思是，在默认情况(orderMode = 0)下展示的帖子都是根据创建时间来进行展示的，当orderMode = 1时意思是首页展示的页面要根据热度去进行排序。\n功能开发 在开发时我们要遵守mvc三层架构的形式去进行开发\ncontroller\n前端的请求参数\nservice\n业务逻辑方法\nmapper\n数据库调用\nHomecontroller 根据前端来确定对应的方法\n1 2 @GetMapping public Result\u0026lt;List\u0026lt;Map\u0026lt;String, Object\u0026gt;\u0026gt;\u0026gt; getDiscussPosts(@RequestParam(defaultValue = \u0026#34;1\u0026#34;) int current, @RequestParam(defaultValue = \u0026#34;5\u0026#34;) int limit, @RequestParam(defaultValue = \u0026#34;0\u0026#34;) int orderMode) 这是函数签名，因为前端在传递这些参数的时候是有默认值的，所以我们可以使用RequestParam注解来设定默认值。\n因为数据返回的是一个List\u0026lt;Map\u0026lt;\u0026gt;\u0026gt;的形式，所以我们还需要一个可以展示post的一个实体类，这样就可以把前端查询到的帖子数据由实体类进行返回。\nHomeService 在HomeService里面需要实现查询封装页数的行为，在数据库中的分页查询就是limit语句，像下面这样\n1 2 3 select * from Post limit offset, pageSize 所以对于page和pageSize这两个参数来说，其中\npage = (current - 1) * pageSize pageSize = limit 在service层可以拼接好这两个字段，然后传递到mapper层里面\nHomeMapper 在首页展示出所有的帖子其实执行的就是一次select语句，这个方法与根据特定用户id进行查找的方式很像，所以我们可以封装一个userId参数，这样在实现根据用户id进行查找的时候就可以传进用户id来进行查找，而不用额外的去写一个方法。\n因为userId是可有可无的，所以对于此类比较复杂的动态sql，我们可以将其书写在xml文件里面。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 \u0026lt;sql id=\u0026#34;commonField\u0026#34;\u0026gt; id, user_id, title, content, type, status, create_time, comment_count, score \u0026lt;/sql\u0026gt; \u0026lt;select id=\u0026#34;getDiscussPosts\u0026#34; resultType=\u0026#34;com.zxp.nowcodercommunity.pojo.DiscussPost\u0026#34;\u0026gt; select \u0026lt;include refid=\u0026#34;commonField\u0026#34;/\u0026gt; from discuss_post where status != 2 \u0026lt;if test=\u0026#34;userId != 0\u0026#34;\u0026gt; and user_id = #{userId} \u0026lt;/if\u0026gt; order by type desc, \u0026lt;if test=\u0026#34;orderMode == 1\u0026#34;\u0026gt; score desc, \u0026lt;/if\u0026gt; create_time desc limit #{offset}, #{limit} \u0026lt;/select\u0026gt; 构造返回数据 在controller层就可以使用service层里面传递回来的数据进行构造，来自后端的数据大概是这样\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \u0026#34;code\u0026#34;: 200, \u0026#34;msg\u0026#34;: null, \u0026#34;data\u0026#34;: [ { \u0026#34;post\u0026#34;: { \u0026#34;id\u0026#34;: 275, \u0026#34;userId\u0026#34;: 11, \u0026#34;title\u0026#34;: \u0026#34;我是管理员\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;我是管理员，你们都老实点！\u0026#34;, \u0026#34;type\u0026#34;: 1, \u0026#34;status\u0026#34;: 1, \u0026#34;createTime\u0026#34;: \u0026#34;2019-05-16T10:58:44.000+00:00\u0026#34;, \u0026#34;commentCount\u0026#34;: 12, \u0026#34;score\u0026#34;: 1751.2900346113624 }, \u0026#34;user\u0026#34;: { \u0026#34;id\u0026#34;: 11, \u0026#34;username\u0026#34;: \u0026#34;nowcoder11\u0026#34;, \u0026#34;headerUrl\u0026#34;: \u0026#34;http://images.nowcoder.com/head/11t.png\u0026#34;, \u0026#34;type\u0026#34;: null, \u0026#34;createTime\u0026#34;: null } } } 也就是说需要我们显示的去构造一个List类型的数据，最后返回。\n获取来自mapper的数据 获取来自mapper数据后\n1 List\u0026lt;DiscussPost\u0026gt; list = homeService.getDiscussPosts(0, page.getCurrent(), page.getLimit(), 0); 显然，list里面存储的就是我们的post数据，我们还要根据post里面的userId来构造user数据，这个过程可以使用stream流来进行操作。\n1 2 3 4 5 6 7 8 9 10 11 12 13 List\u0026lt;Map\u0026lt;String, Object\u0026gt;\u0026gt; data = list.stream().map(post -\u0026gt; { Map\u0026lt;String, Object\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); map.put(\u0026#34;post\u0026#34;, post); // 获取用户信息并拷贝属性 User user = userService.getUserById(post.getUserId()); UserVo userVo = new UserVo(); // 拷贝到UserVo里面 BeanUtils.copyProperties(user, userVo); map.put(\u0026#34;user\u0026#34;, userVo); return map; }).collect(Collectors.toList()); 测试 前端工程是vue3项目，所以在准备好前端工程前使用postman来进行测试\n测试/index接口 在向前端发送数据后，可以看到确实接收到了数据\n前后端联调 启动vue项目后，可以看到首页已经成功的将帖子展示了出来\n但是当前页面还有一个缺点就是时间格式显示的不太正常，这是因为我们并未给定时间格式的形式。一种简单的方式是在参数上指定好日期格式。\n1 2 3 4 5 6 7 8 9 10 11 12 13 public class DiscussPost { private int id; private int userId; private String title; private String content; private int type; private int status; @JsonFormat(pattern = \u0026#34;yyyy-MM-dd hh:mm:ss\u0026#34;) private Date createTime; private int commentCount; private double score; } 这样时间就可以按照我们需要的格式去显示\n同时，我们还需要开发一个查询帖子总数的服务，这个比较简单，思路和分页查询差不多，就不写了。\n总结 我觉得对于一个初学者，下面几点都很重要\n分层架构 统一的数据响应格式 良好的数据封装 ","date":"2025-03-25T21:53:57+08:00","permalink":"https://XiaoPeng0x3.github.io/p/02-%E9%A6%96%E9%A1%B5%E5%8A%9F%E8%83%BD%E5%BC%80%E5%8F%91/","title":"02 首页功能开发"},{"content":"AOP方法 AOP提取所有的重复方法，可以把共性的逻辑全部抽取在AOP方法里面，从而减少重复的代码。下面就统计代码逻辑执行的时间来实现一个AOP类。\n创建一个AOP类 为了创建一个AOP类，需要做到下面几点\n加入Maven坐标\n@Componment注解\n注解此类交给IOC容器管理\n加上@Aspect注解\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 @Component @Slf4j @Aspect public class TimeAspect { /** * 统计一个方法运行所需要的时间 * 1.指定ProceedingJoinPoint对象 * 2.抛出异常 * 3.返回被记录方法的返回值 * 4.编写aop影响的表达式 * @param joinPoint * @return * @throws Throwable */ // 第一个*表示所有的返回值 // 中间的表示 com.itheima.service包下的所有类和接口下的所有方法 // 最后的(..)表示方法的参数可以是任意值 public Object time(ProceedingJoinPoint joinPoint) throws Throwable { long start = System.currentTimeMillis(); Object result = joinPoint.proceed(); long end = System.currentTimeMillis(); long time = end - start; log.info(joinPoint.getSignature() + \u0026#34;方法占用的时间{}ms\u0026#34;, time); return result; } } 抽取公共逻辑 要想让方法被Aspect类中的方法逻辑掌控，就需要声明注解以及execution注解来确定这个方法的控制范围。\n在创建一个AOP类后，我们可以在这个类里面编写需要执行的一些公共逻辑\n指定注解\nSpring中AOP的通知类型：\n@Around：环绕通知，此注解标注的通知方法在目标方法前、后都被执行 @Before：前置通知，此注解标注的通知方法在目标方法前被执行 @After ：后置通知，此注解标注的通知方法在目标方法后被执行，无论是否有异常都会执行 @AfterReturning ： 返回后通知，此注解标注的通知方法在目标方法后被执行，有异常不会执行 @AfterThrowing ： 异常后通知，此注解标注的通知方法发生异常后执行 指定表达式\n在表达式指定的范围内的指定方法都会执行注解注释的方法。\n对于上面的计算方法耗时的方式就是遵守这几个原则的：\n创建好AOP类\n@Aspect注释 @Component 将需要公共执行的逻辑封装到方法内\n1 2 3 4 5 6 7 8 public Object time(ProceedingJoinPoint joinPoint) throws Throwable { long start = System.currentTimeMillis(); Object result = joinPoint.proceed(); long end = System.currentTimeMillis(); long time = end - start; log.info(joinPoint.getSignature() + \u0026#34;方法占用的时间{}ms\u0026#34;, time); return result; } 声明该方法的作用时间和影响的方法\n1 @Around(\u0026#34;execution(* com.itheima.service.*.*(..))\u0026#34;); 这里的表达式有下面这两个格式:\nexecution主要根据方法的返回值、包名、类名、方法名、方法参数等信息来匹配，语法为：\n1 execution(访问修饰符? 返回值 包名.类名.?方法名(方法参数) throws 异常?) 其中带?的表示可以省略的部分\n访问修饰符：可省略（比如: public、protected）\n包名.类名： 可省略\nthrows 异常：可省略（注意是方法上声明抛出的异常，不是实际抛出的异常）\n示例：\n1 @Before(\u0026#34;execution(void com.itheima.service.impl.DeptServiceImpl.delete(java.lang.Integer))\u0026#34;) 可以使用通配符描述切入点\n* ：单个独立的任意符号，可以通配任意返回值、包名、类名、方法名、任意类型的一个参数，也可以通配包、类、方法名的一部分\n.. ：多个连续的任意符号，可以通配任意层级的包，或任意类型、任意个数的参数\n注意，如果只是使用一个'.'，那么不会匹配到该包下的子包，如果需要匹配子包的话要使用..\n抽取公共注解 我们可以将这个类完善一下，如下面的代码所示：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 package com.itheima.aop; import lombok.extern.slf4j.Slf4j; import org.aspectj.lang.JoinPoint; import org.aspectj.lang.ProceedingJoinPoint; import org.aspectj.lang.annotation.*; import org.springframework.stereotype.Component; @Component @Slf4j @Aspect public class TimeAspect { /** * 统计一个方法运行所需要的时间 * 1.指定ProceedingJoinPoint对象 * 2.抛出异常 * 3.返回被记录方法的返回值 * 4.编写aop影响的表达式 * @param joinPoint * @return * @throws Throwable */ // 第一个*表示所有的返回值 // 中间的表示 com.itheima.service包下的所有类和接口下的所有方法 // 最后的(..)表示方法的参数可以是任意值 @Around(\u0026#34;execution(* com.itheima.service.*.*(..))\u0026#34;) public Object time(ProceedingJoinPoint joinPoint) throws Throwable { long start = System.currentTimeMillis(); Object result = joinPoint.proceed(); long end = System.currentTimeMillis(); long time = end - start; log.info(joinPoint.getSignature() + \u0026#34;方法占用的时间{}ms\u0026#34;, time); return result; } @Before(\u0026#34;execution(* com.itheima.service.*.*(..))\u0026#34;) public void before(JoinPoint joinPoint) { log.info(\u0026#34;Before Aspect\u0026#34;); } @After(\u0026#34;execution(* com.itheima.service.*.*(..))\u0026#34;) public void after(JoinPoint joinPoint) { log.info(\u0026#34;After Aspect\u0026#34;); } @AfterReturning(\u0026#34;execution(* com.itheima.service.*.*(..))\u0026#34;) public void afterReturning(JoinPoint joinPoint) { log.info(\u0026#34;AfterReturning Aspect\u0026#34;); } @AfterThrowing(\u0026#34;execution(* com.itheima.service.*.*(..))\u0026#34;) public void afterThrowing(JoinPoint joinPoint) { log.info(\u0026#34;AfterThrowing Aspect\u0026#34;); } } 这里包含了所有的注解，在正常情况下，如果不发生异常，那么@AfterThrowing是不会执行的。\n可以看到，这里面有大量的重复的execution表达式，如果后续改变了切入点表达式的话，就需要我们一个一个的去进行修改。\n这里也提供了一个可以抽取公共表达式的方法\n@PointCut 使用这个注解可以将一个表达式封装为一个新的方法。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 package com.itheima.aop; import lombok.extern.slf4j.Slf4j; import org.aspectj.lang.JoinPoint; import org.aspectj.lang.ProceedingJoinPoint; import org.aspectj.lang.annotation.*; import org.springframework.stereotype.Component; @Component @Slf4j @Aspect public class TimeAspect { @Pointcut(\u0026#34;execution(* com.itheima.service.*.*(..))\u0026#34;) public void pointCut() {} /** * 统计一个方法运行所需要的时间 * 1.指定ProceedingJoinPoint对象 * 2.抛出异常 * 3.返回被记录方法的返回值 * 4.编写aop影响的表达式 * @param joinPoint * @return * @throws Throwable */ // 第一个*表示所有的返回值 // 中间的表示 com.itheima.service包下的所有类和接口下的所有方法 // 最后的(..)表示方法的参数可以是任意值 @Around(\u0026#34;pointCut()\u0026#34;) public Object time(ProceedingJoinPoint joinPoint) throws Throwable { long start = System.currentTimeMillis(); Object result = joinPoint.proceed(); long end = System.currentTimeMillis(); long time = end - start; log.info(joinPoint.getSignature() + \u0026#34;方法占用的时间{}ms\u0026#34;, time); return result; } @Before(\u0026#34;pointCut()\u0026#34;) public void before(JoinPoint joinPoint) { log.info(\u0026#34;Before Aspect\u0026#34;); } @After(\u0026#34;pointCut()\u0026#34;) public void after(JoinPoint joinPoint) { log.info(\u0026#34;After Aspect\u0026#34;); } @AfterReturning(\u0026#34;pointCut()\u0026#34;) public void afterReturning(JoinPoint joinPoint) { log.info(\u0026#34;AfterReturning Aspect\u0026#34;); } @AfterThrowing(\u0026#34;pointCut()\u0026#34;) public void afterThrowing(JoinPoint joinPoint) { log.info(\u0026#34;AfterThrowing Aspect\u0026#34;); } } 这样，我们只需要修改一次execution表达式就可以。\nannotation注解 使用execution注解可以方便的将一群行为相同的方法抽取出来，但是当进行几个特别的方法且方法之间没有什么共同联系的时候抽取就显得比较麻烦。此时我们就可以使用自定义注解的方式去修饰我们需要抽取的方法。\n自定义注释\n1 2 3 4 5 6 7 8 9 10 11 package com.itheima.annotation; import java.lang.annotation.ElementType; import java.lang.annotation.Retention; import java.lang.annotation.RetentionPolicy; import java.lang.annotation.Target; @Target(ElementType.METHOD) @Retention(RetentionPolicy.RUNTIME) public @interface MyLog { } 将需要抽取的类加上注释\n例如，我们需要将list和delete方法的逻辑抽取在一起。\n1 2 3 4 5 6 7 8 9 @MyLog public List\u0026lt;Emp\u0026gt; list(String name, Short gender, LocalDate begin, LocalDate end); /** * 批量删除 * @param ids */ @MyLog void delete(List\u0026lt;Integer\u0026gt; ids); 加上自定义注释。\n切面类消息中使用@annotation注释\n1 2 3 4 5 6 7 8 9 @Before(\u0026#34;@annotation(com.itheima.annotation.MyLog)\u0026#34;) public void before(JoinPoint joinPoint) { log.info(\u0026#34;Before Aspect\u0026#34;); } @After(\u0026#34;@annotation(com.itheima.annotation.MyLog)\u0026#34;) public void after(JoinPoint joinPoint) { log.info(\u0026#34;After Aspect\u0026#34;); } @annotation()里面可以传递我们自定义的注解，从而影响到被注解注解的方法。\n连接点 在Spring中用JoinPoint抽象了连接点，用它可以获得方法执行时的相关信息，如目标类名、方法名、方法参数等。\n对于@Around通知，获取连接点信息只能使用ProceedingJoinPoint类型\n对于其他四种通知，获取连接点信息只能使用JoinPoint，它是ProceedingJoinPoint的父类型\n通过连接点的方式可以获得方法的一些字段属性。\n总结 这就是AOP的所有涉及到的知识~\n","date":"2025-03-19T22:04:16+08:00","permalink":"https://XiaoPeng0x3.github.io/p/aop%E5%9F%BA%E7%A1%80/","title":"Aop基础"},{"content":"开启事务 在发生异常时又可能会导致数据库的数据不完整。下面我们来模拟一个案例。\n删除与解散部门 在解散部门的时候，我们不仅要删除该部门，而且还要删除隶属于该部门的所有员工。不过，这两个操作并不是原子性的。\n如果正常运行的话，就会是下面这样\n1 2 3 4 5 6 7 8 9 10 // Service层 public void deleteDeptById (Integer id) { // 删除部门 deptMapper.deleteDeptById(id); // anything // 删除员工 empMapper.deleteEmpByDeptId(id); } 如果这两条语句之间不发生异常，那么就可以正确的把dept和emp删除，从而可以确保数据的完整性；如果中间发生异常，那么就会导致发生异常之前的代码可以被成功执行，而异常之后的代码未被执行。\n为了解决这个问题，我们就需要使用事务这个注解来解决\n@Transactional 这个注解可以帮助我们开启事务。\n@Transactional注解书写位置：\n方法 当前方法交给spring进行事务管理 类 当前类中所有的方法都交由spring进行事务管理 接口 接口下所有的实现类当中所有的方法都交给spring 进行事务管理 一般的，我们可以只在Service的实现类里面去加上这个注解。当方法发生异常时，就会引发事务回滚。我们来模拟一下发生异常的情况。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 public class DeptServiceImpl implements DeptService { @Autowired private DeptMapper deptMapper; @Autowired private EmpMapper empMapper; //根据部门id，删除部门信息及部门下的所有员工 @Override public void delete(Integer id){ //根据部门id删除部门信息 deptMapper.deleteById(id); //模拟：异常发生 int i = 1/0; //删除部门下的所有员工信息 empMapper.deleteByDeptId(id); } } 当异常发生时，删除员工信息的操作不会成功。\n加上注解后，可以发现，数据库成功回滚\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @Slf4j @Service public class DeptServiceImpl implements DeptService { @Autowired private DeptMapper deptMapper; @Autowired private EmpMapper empMapper; @Override @Transactional //当前方法添加了事务管理 public void delete(Integer id){ //根据部门id删除部门信息 deptMapper.deleteById(id); //模拟：异常发生 int i = 1/0; //删除部门下的所有员工信息 empMapper.deleteByDeptId(id); } } rollbackFor 如果这里我们手动抛出一个异常，即throws new Exception，那么事务回滚还是会发生失败，这是因为抛出异常相当于该方法直接return，那么return后的语句是不会执行的，所以这个时候就需要我们在@Transactional中指定异常处理的类别。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @Slf4j @Service public class DeptServiceImpl implements DeptService { @Autowired private DeptMapper deptMapper; @Autowired private EmpMapper empMapper; @Override @Transactional(rollbackFor=Exception.class) // 不管出现什么异常，都要回滚事务 public void delete(Integer id){ //根据部门id删除部门信息 deptMapper.deleteById(id); //模拟：异常发生 int num = id/0; //删除部门下的所有员工信息 empMapper.deleteByDeptId(id); } } 一般的，如果不加rollbackFor，那么只支持RunTimeException。\npropagation 事务也是具有自己的传播性的。\n假设这里有一个方法a，在方法a里面我们有=又去调用方法b，假设a和b都开启了事务，那么有没有可能会发生事务失败的情况呢，我们来模拟一下一种情形。\n删除解散部门 在删除部门时，不管删除成功与否，我们都希望把删除的操作写进日志里面。看起来像是下面这样\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 @Slf4j @Service //@Transactional //当前业务实现类中的所有的方法，都添加了spring事务管理机制 public class DeptServiceImpl implements DeptService { @Autowired private DeptMapper deptMapper; @Autowired private EmpMapper empMapper; @Autowired private DeptLogService deptLogService; //根据部门id，删除部门信息及部门下的所有员工 @Override @Log @Transactional(rollbackFor = Exception.class) public void delete(Integer id) throws Exception { try { //根据部门id删除部门信息 deptMapper.deleteById(id); //模拟：异常 if(true){ throw new Exception(\u0026#34;出现异常了~~~\u0026#34;); } //删除部门下的所有员工信息 empMapper.deleteByDeptId(id); }finally { //不论是否有异常，最终都要执行的代码：记录日志 DeptLog deptLog = new DeptLog(); deptLog.setCreateTime(LocalDateTime.now()); deptLog.setDescription(\u0026#34;执行了解散部门的操作，此时解散的是\u0026#34;+id+\u0026#34;号部门\u0026#34;); //调用其他业务类中的方法 deptLogService.insert(deptLog); } } //省略其他代码... } 首先，这两个操作都是开启事务的，这就意味着一旦发生了异常操作，那么事务是必须要进行回退的。但是插入日志的操作是在删除部门这个逻辑里面的，经过实验我们得出了一个结论\n在默认情况下，如果a方法发生异常并回滚，那么a调用的支持事务的方法也要跟着a进行回滚a中的方法与a存在于同一个事物环境。\n在这个方法里面，如果发生了异常，那么执行插入日志操作会发生回滚。\n使用@Transactional(propagation = Propagation.REQUIRES_NEW)参数来进行指定\n通过这行注释，就说明被注释的方法要求一个新的事物环境，与它被调用的那个方法不在同一个事物环境内，这样当调用者发生事务回滚时，被调用者不会发生事务回滚。\n到此事务传播行为已演示完成，事务的传播行为我们只需要掌握两个：REQUIRED、REQUIRES_NEW。\nREQUIRED ：大部分情况下都是用该传播行为即可。\nREQUIRES_NEW ：当我们不希望事务之间相互影响时，可以使用该传播行为。比如：下订单前需要记录日志，不论订单保存成功与否，都需要保证日志记录能够记录成功。\n总结 使用@Transactional开启注解 rollback进行回滚，默认情况下只在发生运行时异常时才进行回滚，可以加入Exception.class参数来支持所有异常回滚。 事务传播。可以使用REQUIRES_NEW参数来指定该方法说一个独立的事务 ","date":"2025-03-19T11:20:30+08:00","permalink":"https://XiaoPeng0x3.github.io/p/spring%E4%BA%8B%E5%8A%A1/","title":"Spring事务"},{"content":"Exception 案例中构造了一个有关sql的一个异常代码，因为Dept表中的dept_name是一个unique字段，那么当我们向这样表中尝试加入一个已经存在的部门时就会引发异常，而我们要做的就是去设置一个全局异常处理器去捕获这个异常，并将结果响应发送给前端。\n全局处理器 当数据库连接发生异常时，会将这个异常信息传递到service层，而service会将这个异常信息传递给controller层，最后controller层会将这个异常信息传递到全局异常处理器，在全局异常处理器中把错误信息响应给前端。\n这里主要用到了两个注解\nRestControllerAdvice\n这个注解表明了当前这个类是全局异常处理类\nExceptionHandler\n可以在这个注解里面标明这个方法接受哪些异常处理\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package com.itheima.exception; import com.itheima.pojo.Result; import lombok.extern.slf4j.Slf4j; import org.springframework.web.bind.annotation.ExceptionHandler; import org.springframework.web.bind.annotation.RestControllerAdvice; @Slf4j @RestControllerAdvice // 全局异常信息注解 public class GlobalException { @ExceptionHandler(value = Exception.class) // 所有发生异常的类 public Result exception(Exception e) { log.error(e.getMessage()); return Result.error(e.getMessage()); } } ","date":"2025-03-19T10:00:26+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E5%85%A8%E5%B1%80%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/","title":"全局异常处理"},{"content":"登录功能 对于没有登录管理系统的用户，这些用户是不可以直接对其它的资源进行访问的。但是目前我们并没有做到要对用户进行拦截的操作。总共有这几种方法\njwt和token验证+Filter 过滤 jwt和Interceptor拦截器拦截 JWT令牌 JWT令牌就是一个经过长编码后的一个字符串，其内容主要可以分为\npayload 签名算法 过期时间 我们可以使用JWTS工具来生成一个jwt令牌\n1 2 3 4 5 6 String jwt = Jwts.builder() .setClaims(claims) .signWith(SignatureAlgorithm.HS256, signKey) .setExpiration(new Date(System.currentTimeMillis() + expireTime)) .compact(); return jwt; 其中，Claims是指定要进行填充的字段，其类型是Map\u0026lt;String, Object\u0026gt;数据类型，第二个参数就是去指定签名算法和签名的key。第三个参数就是去设置这个令牌的有效时间，最后把他们打包为一个字符串即可。\n同理，我们也可以解析一个JWT令牌\n1 2 3 4 5 6 7 public static Claims parseJwt(String jwt) { Claims body = Jwts.parser() .setSigningKey(signKey) .parseClaimsJws(jwt) .getBody(); return body; } 只要指定好这个生成JWT时对应的签名，那么就可以得到这个JWT令牌，并且，如果这个JWT已经过期或者签名被篡改，那么在解析的过程中就会引发异常，因此可以在校验JWT是否合法中起到作用，对应的，如果在校验过程中引发异常，那么就可能是下面这几种情况\nJWT令牌过期 JWT令牌错误 可以用来进行对用户的校验。\nFilter过滤器 可以使用过滤器来过滤请求。\n实现Filter接口 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @WebFilter(urlPatterns = \u0026#34;/login\u0026#34;) // 拦截器需要拦截的路径 public class DemoFilter implements Filter { // 初始化方法，只调用一次 @Override public void init(FilterConfig filterConfig) throws ServletException { System.out.println(\u0026#34;filter开始调用了\u0026#34;); } // 拦截时调用该方法 @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { System.out.println(\u0026#34;放行前的逻辑\u0026#34;); // 放行逻辑 // 如果不写下面这行放行逻辑的代码，那么数据其实就一直处于被拦截的状态 filterChain.doFilter(servletRequest, servletResponse); System.out.println(\u0026#34;放行后的逻辑\u0026#34;); } // 销毁方法，只执行一次 @Override public void destroy() { Filter.super.destroy(); } } 这里总共是需要重写三个方法，其中的doFilter对应我们的拦截逻辑。除此之外，我们还需要加上一个@WebFilter接口，里面可以指定要拦截的url。\n实现登录验证 使用Filter来进行拦截，一种可行的思路是：无论是哪种方法，我们都进行拦截，只不过在拦截时进行区分。\n如果当前方法是login方法，那么我们直接放行，让用户登录即可。\n如果是非login方法，我们需要对用户进行JWT令牌校验\n首先JWT令牌在登录后服务器会返回给前端浏览器 浏览器每次请求都会在header里面携带这个JWT令牌 如果在验证过程中发生异常，那么说明JWT令牌存在问题，我们不能让该请求通过\n只有JWT验证成功时才可以登录。\n将拦截器的操作全部封装在doFilter里面\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { HttpServletRequest request = (HttpServletRequest) servletRequest; HttpServletResponse response = (HttpServletResponse) servletResponse; // 获取请求的url String urlRequest = request.getRequestURL().toString(); if (urlRequest.contains(\u0026#34;login\u0026#34;)) { // 直接放行 filterChain.doFilter(request, response); return; } // 此时拦截到的就是非login请求 // 获取token String token = request.getHeader(\u0026#34;token\u0026#34;); // 使用jwt对token进行校验 log.info(\u0026#34;token:{}\u0026#34;, token); if (!StringUtils.hasLength(token)) { log.info(\u0026#34;token不存在\u0026#34;); // 将这个信息返回回去 Result notLogin = Result.error(\u0026#34;NOT_LOGIN\u0026#34;); String json = JSONObject.toJSONString(notLogin); response.setContentType(\u0026#34;application/json;charset=utf-8\u0026#34;); //响应 response.getWriter().write(json); return; } // 令牌存在 // 将令牌进行解析 try { JwtUtil.parseJwt(token); } catch (Exception e) { log.info(\u0026#34;token解析失败\u0026#34;); Result notLogin = Result.error(\u0026#34;NOT_LOGIN\u0026#34;); String json = JSONObject.toJSONString(notLogin); response.setContentType(\u0026#34;application/json;charset=utf-8\u0026#34;); //响应 response.getWriter().write(json); } // 如果不发生错误，那么就在这里放行 filterChain.doFilter(request, response); } 所以其拦截思路是：\n拦截所有的请求，login操作除外 拦截到请求后，检验JWT，JWT检验无误后放行 Interceptor拦截 需要实现HandlerInterceptor接口，同时重写这个函数里面的方法。\n1 2 3 4 5 preHandle方法：目标资源方法执行前执行。 返回true：放行 返回false：不放行 ​\tpostHandle方法：目标资源方法执行后执行 ​\tafterCompletion方法：视图渲染完毕后执行，最后执行 这里我们只需要重写preHandle方法即可，注意方法返回true代表是放行操作，返回false就是不放行。\ndemo 逻辑几乎相同，不过在不放行时选择返回false\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { // 返回值为True表示放行 // 返回值为False表示不放行 // 实现登录拦截 // 获取请求的url log.info(\u0026#34;Interceptor....\u0026#34;); String urlRequest = request.getRequestURL().toString(); if (urlRequest.contains(\u0026#34;login\u0026#34;)) { // 直接放行 return true; } // 其它操作 String token = request.getHeader(\u0026#34;token\u0026#34;); if (!StringUtils.hasLength(token)) { log.info(\u0026#34;token不存在\u0026#34;); // 将这个信息返回回去 Result notLogin = Result.error(\u0026#34;NOT_LOGIN\u0026#34;); String json = JSONObject.toJSONString(notLogin); response.setContentType(\u0026#34;application/json;charset=utf-8\u0026#34;); //响应 response.getWriter().write(json);// token不存在 return false; } // 检验token try { JwtUtil.parseJwt(token); }catch (Exception e){ log.info(\u0026#34;token不存在\u0026#34;); // 将这个信息返回回去 Result notLogin = Result.error(\u0026#34;NOT_LOGIN\u0026#34;); String json = JSONObject.toJSONString(notLogin); response.setContentType(\u0026#34;application/json;charset=utf-8\u0026#34;); //响应 response.getWriter().write(json); return false; } return true; } 注册WebConfig 1 2 3 4 5 @Autowired private LoginCheckInterceptor loginCheckInterceptor; public void addInterceptors(InterceptorRegistry registry) { registry.addInterceptor(loginCheckInterceptor).addPathPatterns(\u0026#34;/**\u0026#34;).excludePathPatterns(\u0026#34;/login\u0026#34;); } 添加路径 在拦截器中除了可以设置/**拦截所有资源外，还有一些常见拦截路径设置：\n拦截路径 含义 举例 /* 一级路径 能匹配/depts，/emps，/login，不能匹配 /depts/1 /** 任意级路径 能匹配/depts，/depts/1，/depts/1/2 /depts/* /depts下的一级路径 能匹配/depts/1，不能匹配/depts/1/2，/depts /depts/** /depts下的任意级路径 能匹配/depts，/depts/1，/depts/1/2，不能匹配/emps/1 以上就是拦截器的执行流程。通过执行流程分析，大家应该已经清楚了过滤器和拦截器之间的区别，其实它们之间的区别主要是两点：\n接口规范不同：过滤器需要实现Filter接口，而拦截器需要实现HandlerInterceptor接口。 拦截范围不同：过滤器Filter会拦截所有的资源，而Interceptor只会拦截Spring环境中的资源。 总结 我们使用两种方式实现了登录校验功能，一般来说，对于api之类的请求，我们可以直接使用Interceptor来实现~\n","date":"2025-03-18T20:58:57+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E7%99%BB%E5%BD%95%E4%B8%8E%E7%99%BB%E5%BD%95%E9%AA%8C%E8%AF%81/","title":"登录与登录验证"},{"content":"基础命令2 这次来看一些其它的基础命令。\nincrbyfloat 浮点数增长命令，需要给定一个需要增长的浮点数。浮点数没有自增自减操作\n1 incrbyfloat num 1.1 setnx 设定新的键值对数据，前提是原始的key不存在。\n1 setnx age 18 返回值为\n1 2 127.0.0.1:6379\u0026gt; setnx age 19 (integer) 1 假如再使用这个命令的话\n1 2 127.0.0.1:6379\u0026gt; setnx age 19 (integer) 0 这个命令并不会执行，只有当age不存在的时候，才可以成功设置age\nsetex 添加一个string类型的键值对，并指定有效期。\n1 setex name 10 \u0026#34;bob\u0026#34; 设定了一个{name: \u0026ldquo;bob\u0026rdquo;}的键值对，并且有效期为10s\nKey形式的层级命令 为了使键能够有所区别，这里可以使用\n1 项目名：实体类：id的形式来进行存储 1 set zxp:user:1 \u0026#39;{\u0026#34;id\u0026#34; : 1, \u0026#34;name\u0026#34;: Alice, \u0026#34;age\u0026#34;: 18}\u0026#39; 如果又新增了一个user，那么就可以更改后面的数字来表示新增\n1 set zxp:user:2 \u0026#39;{\u0026#34;id\u0026#34; : 2, \u0026#34;name\u0026#34;: Bob, \u0026#34;age\u0026#34;: 20}\u0026#39; 同样的，这些命令都可以使用get命令来进行查询\n1 2 get zxp:user:2 \u0026#34;{\\\u0026#34;id\\\u0026#34; : 2, \\\u0026#34;name\\\u0026#34;: Bob, \\\u0026#34;age\\\u0026#34;: 20}\u0026#34; Hash类型的数据 Hash类型的数据的所有命令与一开始关于string类型的很像，只是所有的命令前面需要加上一个H来进行区分。\nHset 格式是\n1 hset key field value 例如，我们要添加一个hash类型的数据\n1 hset zxp:user:3 name \u0026#34;Dube\u0026#34; 这样做比string类型更加直白和方便\nhmset 可以一次性添加多个field数据\n1 hmset zxp:user:4 name \u0026#34;Jack\u0026#34; age 20 phone 19988899999 Hget和Hmget 这两个命令可以根据key值来进行查询。\n1 2 3 hmget zxp:user:4 name age 1) \u0026#34;Jack\u0026#34; 2) \u0026#34;20\u0026#34; 需要注意的是，需要指定多个field名称才可以查询。\nHgetall 这个命令可以查询哈希表中的所有数据(键值对类型)\n1 2 3 127.0.0.1:6379\u0026gt; hgetall zxp:user:3 1) \u0026#34;name\u0026#34; 2) \u0026#34;Dube\u0026#34; 只需要指定key的名称即可。\nhkeys 获取所有的key值，再zxp:user:3这个表中只有一个键，那就是姓名这个键，我们可以查询一下这个表里面所有键的名称。\n1 hkeys zxp:user:3 这样就可以查询到所有键的名称\nHvals 查询所有value的值\n1 hvals zxp:user:3 输出结果是：\n1 2 127.0.0.1:6379\u0026gt; hvals zxp:user:3 1) \u0026#34;Dube\u0026#34; hincrby 让key值的一个字段增加指定步长。\n先创建一个hset格式的数据\n1 2 127.0.0.1:6379\u0026gt; hmset zxp:user:5 name \u0026#34;Bob\u0026#34; age 18 OK 获取所有的值\n1 2 3 4 5 127.0.0.1:6379\u0026gt; hgetall zxp:user:5 1) \u0026#34;name\u0026#34; 2) \u0026#34;Bob\u0026#34; 3) \u0026#34;age\u0026#34; 4) \u0026#34;18\u0026#34; 我们让这个age字段增加20\n1 2 127.0.0.1:6379\u0026gt; hincrby zxp:user:5 age 20 (integer) 38 结果成功返回。\nHsetnx 给key添加一个不存在表格中的字段，如果这个字段存在，那么不会添加成功。\n我们可以给zxp:user:5添加一个name字段\n1 2 127.0.0.1:6379\u0026gt; hsetnx zxp:user:5 name \u0026#34;Mom\u0026#34; (integer) 0 可以发现添加失败了\n总结 这是有关哈希数据的一些基本命令，与一开始的string类型的数据相比,hash数据添加更加方便，不过他们的形式还是一样的。\n","date":"2025-03-13T14:20:01+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A42/","title":"基础命令2"},{"content":"基础命令 首先需要启动redis服务，可以通过sudo systemctl start redis，一般来说可以通过配置来使得redis进行开启自启。\n在使用时，可以使用:\n1 redis-cil 来登入redis\n这个时候需要如果我们要执行一些查询或者删除操作的话，redis会提示我们没有权限，这是因为我们在安装redis的时候就已经制定好了登录密码，此时可以使用auth命令来登录\n1 2 redis-cli # 登录 auth name password # 验证密码登录 如果没有name，那么就可以直接输入密码登录。\n创建一个字段 set 由于redis的数据结构就是基础key-value的形式，所以创建对象时要指定对象的名称和值。\n创建姓名键值对\n1 set name \u0026#34;Bob\u0026#34; 这样就创建好了一个{name:Bob}的数据结构\n创建数值型\n1 set age 18 如果需要添加新的成员，可以使用set来创建。需要注意的是，创建出来的key-value的value的类型是字符串类型。但是对于一些数据(长得像整数)，那么redis还可以对这个数据进行自增自减操作。\nincr和decr(整形自增自减) 自增\n1 incr age 这样在此进行查询的时候，age就变为19，但它的类型依旧是字符串类型。\n自减\n1 decr age 每次自增自减的值都是1\nincrby和decrby 可以指定key增长的数目\n增加2\n1 incrby age 2 减小2\n1 decrby age 2 get get可以获取对应key的value\n获取name的名称\n1 get name 获取age的值\n1 get age 答案会返回18\ntype 使用type可以查看一个key的数据类型\n查看age的类型\n1 type age 需要注意的是，这类数字在底层都是按照字节数组来存储的，对于数字来说，直接把数字转换为二进制数组来存储\n查看name的类型\n1 type name mset 可以一次设置多个键值对类型\n1 mset name \u0026#34;bob\u0026#34; age 18 school \u0026#34;xiaoxue\u0026#34; 这样就设置好了键值对信息。需要注意的是，这是一个原子操作，如果前面有设置失败的字段，那么后面的字段也不会成功的设置。\nmget 同样的，这个命令可以一下次获取多个key的值\n1 mget name age school 这样就会得到多个返回的值。\nexpire 和 ttl 由于redis是基于内存的nosql数据库，所以其数据存储的越多，那么占用的内存也就越多(一个典型的例子是短信验证码),我们可以给这个数据设置一个有效期(默认单位是秒)\n给age设置20秒的有效期\n1 expire age 20 设置完后可以使用ttl(time to live)来获取有效剩余时间\n1 ttl age 总结 这是一些基础命令！\n","date":"2025-03-12T22:00:14+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A41/","title":"基础命令1"},{"content":"员工管理系统 这是一个综合了前后端、开发流程比较完整的一个简单管理系统。项目中已经提供好了前端页面，但是还需要我们自己去编写后端返回给前端的请求。\n其中需要编写的请求就是增删改查这四个基础的请求。在这个项目中，总共有两个需要创建的表，即员工表和部门表。所有的增删查改任务都是基于这两张表来开展的。\nRestAPI风格 与传统的GET和POST请求不同，RestAPI把每项不同的操作划分给不同的动作\nGET\n可以只负责查询数据\nPOST\n可以负责新增数据\nPUT\n可以负责修改数据\nDelete\n可以负责删除数据\n单单从url里面来说，这四种操作的url几乎相同，只是行为不同，这样就使得更加规范\n项目工程结构 工程的结构总共分为几大类\nController\n专门负责处理来自前端的数据\nMapper\n专门负责处理来自数据库的部分\nService\n负责业务逻辑，如调用来自Mapper的接口，拿到数据库里面的数据后进行各种处理。\nPojo\n负责存放一些实体类，如表里面的Emp，User对象等。\n查询数据 项目中有一个功能就是去实现目前所有部门，用sql语句来表示可能就是\n1 select name, create_time, update_time from dep; 让我们先来熟悉一下思路\nMapper\n在Mapper层需要定义查询数据库的接口，因为查询sql很简短，所以我们可以直接写在java代码里面。\n1 2 3 4 5 @Mapper public interface DeptMapper { //查询所有部门数据 @Select(\u0026#34;select id, name, create_time, update_time from dept\u0026#34;) List\u0026lt;Dept\u0026gt; list(); Mapper是开启容器化注入的一个注解，其实很简单，需要实现对部门的任何操作都可以写在这个DeptMapper当中。\nService\n然后我们需要在Service层获取在Mapper层得到的来自数据库的信息。在service层中，我们定义了Dept接口和DeptImpl实现对象，在这里可以编写对应的方法来获取得到的数据库对象\n1 2 3 4 5 6 @Autowired private DeptMapper deptMapper; @Override public List\u0026lt;Dept\u0026gt; list() { return deptMapper.list(); } Controller\n在controller中调用Service层处理好的数据，然后经过统一的响应格式(Result)返回\n1 2 3 4 5 6 7 8 9 @Autowired private DeptService deptService; //@RequestMapping(value = \u0026#34;/depts\u0026#34; , method = RequestMethod.GET) @GetMapping public Result list(){ List\u0026lt;Dept\u0026gt; deptList = deptService.list(); return Result.success(deptList); } 需要注意的是，在填写url时，我们可以直接指定访问该url的动作，即使用GET方法即可\n1 2 3 4 5 @GetMapping public Result list(){ List\u0026lt;Dept\u0026gt; deptList = deptService.list(); return Result.success(deptList); } 这就是查询数据的操作。\n增加数据 增加一条数据的sql原语句就是\n1 insert into dep(字段名) values(值) 在部门字段中，如果我们需要新添加一个部门，那么在前端页面上只需要输入其姓名即可，然而这个部门包含以下几个属性\n1 2 3 4 5 6 7 8 9 10 mysql\u0026gt; desc dept; +-------------+--------------+------+-----+---------+----------------+ | Field | Type | Null | Key | Default | Extra | +-------------+--------------+------+-----+---------+----------------+ | id | int unsigned | NO | PRI | NULL | auto_increment | | name | varchar(10) | NO | UNI | NULL | | | create_time | datetime | NO | | NULL | | | update_time | datetime | NO | | NULL | | +-------------+--------------+------+-----+---------+----------------+ 4 rows in set (0.00 sec) 其中，后面两个字段需要在我们成功创建对象的时候就添加为创建时的时间now()，而如何传递name字段呢，其实在前端向后端传递数据时，其传递的数据就是json的键值对，我们可以利用RequestBody这个属性来填写该字段，并把该字段的create_time和update_time更新为创建时的时间即可。\n1 2 3 4 // 新增部门 // 需要注意的是后面的参数是来自我们自己封装的Dept对象，所以后面要用驼峰命名法，即和dept里面的数据名一致 @Insert(\u0026#34;insert into dept(name, create_time, update_time) values (#{name}, #{createTime}, #{updateTime})\u0026#34;) void insert(Dept dept); 需要注意的是，在pojo.Dept中，后面两个字段都是按照驼峰命名法来命名的，而数据库中的变量是以下划线来进行分割的，这里有一个变量名转换依赖，在使用这个依赖后就可以将我们的字段成功转换。\n为了防止我以后看到这段代码疑惑，数值在代码中传递的方向是\nController\n获取name，并添加时间字段\nService\n将这个数据传递到Service层中\nMapper\n再传递到Mapper中，然后插入数据。\n显然，在数据流动过程中，如果需要执行查询等这类不需要修改数据库的内容，那么数据就是从数据库出发，返回到controller后完成响应。\n如果需要执行删除、修改等这类的操作，那么数据就是从controller层出发，传递到数据库后，完成响应。\n删除数据 同样的，在Mapper层中定义好删除接口，在Service层中实现逻辑，然后在Controller层中完成响应\n1 2 3 // 根据id来删除 @Delete(\u0026#34;delete from dept where id = #{id}\u0026#34;) void delete(int id); 修改数据 可以根据传进来的id来实现修改部门\n1 2 3 4 5 6 @PutMapping(\u0026#34;{id}\u0026#34;) public Result update(@RequestBody Dept dept){ dept.setUpdateTime(LocalDateTime.now()); deptService.update(dept); return Result.success(); } 在Service层\n1 2 3 4 @Override public void update(Dept dept) { deptMapper.update(dept); } 在Mapper层\n1 2 3 // 修改数据 @Update(\u0026#34;update dept set name = #{name}, update_time = #{updateTime} where id = #{id}\u0026#34;) void update(Dept dept); 总结 这里实现了基本的增删查改功能。现在我们来梳理以下这样一个前后端项目需要哪些条件。\n环境搭建 准备好数据库连接\n分包编写代码。在Mapper层中编写操作数据库有关的代码；在Service层实现业务逻辑；在Controller层实现响应。\nMapper层中都是接口对象，下面是一个包目录的文件结构\n","date":"2025-03-09T20:41:19+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E5%91%98%E5%B7%A5%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F%E6%A1%88%E4%BE%8B/","title":"员工管理系统案例"},{"content":"增删查改 增删查改算是十分重要且基础的部分，这里来跟着文档来实现一下如何进行增删改查。\nMybatis连接数据库 需要我们在application.properties下配置好对应的内容\n1 2 3 4 5 6 7 8 9 10 11 #驱动类名称 spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver #数据库连接的url spring.datasource.url=jdbc:mysql://localhost:3306/mybatisEmp # 数据库用户名称 spring.datasource.username=root # 登录密码 spring.datasource.password=123456 删除 在Mapper层实现删除的接口，对于删除语句来说，其选择的注释就是@Delete，其中编写对应的删除语句即可。我们需要实现一个根据ID来删除的一个操作。\n参数传递 在Mapper层中我们可以编写EmpMapper接口，其中，为了更加灵活，我们可以根据传进来的ID来实现删除逻辑\n1 2 3 4 5 6 @Mapper public interface EmpMapper { // 删除 @Delete(\u0026#34;DELETE from emp where id = #{id}\u0026#34;) public int deleteEmpById(Integer id); } 使用Mapper注释可以将后续实例化的对象通过容器进行管理，后续我们只需要使用AutoWired注释即可。可以在test文件下进行测试。在测试时，每个测试都要写成一个函数，并且要加上@Test注释。\n1 2 3 4 5 @Test public void deleteByPrimaryKey() { int i = empMapper.deleteEmpById(17); System.out.println(\u0026#34;Return Val Of Delete is \u0026#34; + i); } sql注入问题 当使用拼接sql语句去进行某些验证业务时，精心构造的参数会跳过验证，从而起到sql注入攻击的效果。假设我们使用的是sql拼接的方式进行验证，例如，我们要做登录验证。\n一般，数据库会存储用户名和加密后的密码(应该不是明文密码)，当我们进行登录是时，假设是在进行查询操作。\n1 select count(*) where userName = \u0026#39;xxx\u0026#39; and password = \u0026#39;xxx\u0026#39;; 当使用拼接sql时，我们可以构造出来count(*)永远大于0的操作，即password在传递时为' or '1' = '1,这样，拼接后的sql语句就是\n1 select count(*) from emp where userName = \u0026#39;xxx\u0026#39; and password = \u0026#39;\u0026#39; or \u0026#39;1\u0026#39; = \u0026#39;1\u0026#39;; 而1 = 1永远成立，所以这条sql相当于\n1 select count(*) from emp; sql预编译 解决sql注入可以通过预编译的方式去避免。在预编译时，传递的关键参数都是利用占位符去占用，例如上面这条sql语句就会变为\n1 select count(*) from emp where userName = ? and password = ?; 当有参数传递时，就会将?替换为对应的参数，从而避免了sql拼接的问题。在上面传递参数的sql语句中，一共有两种传递参数的办法\n1 2 #{id} ${id} 这两种方式都可以起到参数传递的作用，不过#{id}的方式是预编译sql，可以解决sql注入的问题。\n插入数据 在mysql中，插入数据的sql语句一般是\n1 insert into xxx(字段名) values (xxxx); 使用的注解也是@Insert，当插入的值过多，就可以把这些值封装到一个对象里面，把需要插入的参数使用#{xx}的方式插入即可，这样可以防止sql注入，同时预编译还可以提高查询效率。\n当需要进行模糊搜索时，一般会用到通配符搜索，在使用通配符的时候，有些通配符符号是无法通过#{xx}的形式去拼接的，例如，我想要搜索所有包含张的姓名\n1 select name from user where name like \u0026#39;%张%\u0026#39;; 那么在传递参数的时候，需要把%也传递进去，就像下面一样\n1 2 3 4 5 6 7 8 9 @Mapper public interface EmpMapper { @Select(\u0026#34;select * from emp \u0026#34; + \u0026#34;where name like \u0026#39;%${name}%\u0026#39; \u0026#34; + \u0026#34;and gender = #{gender} \u0026#34; + \u0026#34;and entrydate between #{begin} and #{end} \u0026#34; + \u0026#34;order by update_time desc\u0026#34;) public List\u0026lt;Emp\u0026gt; list(String name, Short gender, LocalDate begin, LocalDate end); } 如果要使用占位参数来编写sql，那么这个句子就变成了\n1 select name from user where name like \u0026#39;%?%\u0026#39; 这样就会出现编译错误，而使用直接拼接的${}话，又会引发sql注入问题，所以可以使用sql的库函数即concat函数来拼接。\n1 2 3 4 5 6 7 8 9 10 11 @Mapper public interface EmpMapper { @Select(\u0026#34;select * from emp \u0026#34; + \u0026#34;where name like concat(\u0026#39;%\u0026#39;,#{name},\u0026#39;%\u0026#39;) \u0026#34; + \u0026#34;and gender = #{gender} \u0026#34; + \u0026#34;and entrydate between #{begin} and #{end} \u0026#34; + \u0026#34;order by update_time desc\u0026#34;) public List\u0026lt;Emp\u0026gt; list(String name, Short gender, LocalDate begin, LocalDate end); } 这样也可以实现预编译，把中间的#{name}替换掉,在预编译后就变成了\n1 select name from user where name like concat(\u0026#39;%\u0026#39;, ?, \u0026#39;%\u0026#39;); XML编写 可以直接把用到的sql语句写到XML文件里面，这样当sql语句开始变得复杂的时候，就可以方便的管理一些sql语句，从而避免将sql语句直接写在java代码里面。\n书写规范 其书写格式一般是\n1 2 3 \u0026lt;mapper namespace=\u0026#34;xxx.xxx.xxx\u0026#34;\u0026gt; \u0026lt;/mapper\u0026gt; 不同的sql语句之间标签不一致，查询语句需要用\u0026lt;select\u0026gt;标签包裹，同理的，例如\u0026lt;insert\u0026gt;、\u0026lt;delete\u0026gt;、\u0026lt;update\u0026gt;\n同时，还需要在这些标签中注明方法名和方法的返回值,假设我们要查询\n1 2 3 4 5 6 7 8 9 10 11 \u0026lt;mapper namespace=\u0026#34;com.itheima.mapper.EmpMapper\u0026#34;\u0026gt; \u0026lt;!--查询操作--\u0026gt; \u0026lt;select id=\u0026#34;list\u0026#34; resultType=\u0026#34;com.itheima.pojo.Emp\u0026#34;\u0026gt; select * from emp where name like concat(\u0026#39;%\u0026#39;,#{name},\u0026#39;%\u0026#39;) and gender = #{gender} and entrydate between #{begin} and #{end} order by update_time desc \u0026lt;/select\u0026gt; \u0026lt;/mapper\u0026gt; 其中id=\u0026quot;list\u0026quot;说明这个查询函数的函数名为list，resultType=xxx说明其返回值类型是xxx，然后在标签中间编写sql语句即可。\n需要注意，可以直接把简单的sql以注解的形式写道java代码里面，也可以写到xml文件里面。越短越简单的sql语句可以写到java代码里面。\n更新数据 同样的，我们可以使用insert来向表里面插入某些数据，也可以使用update来更新一些数据，如果我们不指定所有的字段，那么未指定的字段将会被设置为null,但是这并不是我们希望看到的，有时候只需要修改一两条信息，其它的信息不用做出修改，这个时候我们就可以使用动态sql来避免传递空子段。\n动态SQL 动态SQL是指前端传递过来的参数个数是不确定的，这就要求我们不可以将sql语句写死，而是根据传递结果来确定sql语句的书写，这里就用到了if标签。\n语法 1 2 3 \u0026lt;if test=\u0026#34;判断语句\u0026#34;\u0026gt; \u0026lt;/if\u0026gt; 例如，之前的一条查询语句为\n1 2 3 4 5 6 7 \u0026lt;select id=\u0026#34;list\u0026#34; resultType=\u0026#34;com.itheima.pojo.Emp\u0026#34;\u0026gt; select * from emp where name like concat(\u0026#39;%\u0026#39;,#{name},\u0026#39;%\u0026#39;) and gender = #{gender} and entrydate between #{begin} and #{end} order by update_time desc \u0026lt;/select\u0026gt; 我们就可以根据name gender entrydate这些字段是否为空来进行查询判断\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026lt;select id=\u0026#34;list\u0026#34; resultType=\u0026#34;com.itheima.pojo.Emp\u0026#34;\u0026gt; select * from emp where \u0026lt;if test=\u0026#34;name != null\u0026#34;\u0026gt; name like concat(\u0026#39;%\u0026#39;,#{name},\u0026#39;%\u0026#39;) \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;gender != null\u0026#34;\u0026gt; and gender = #{gender} \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;begin!= null and end != null\u0026#34;\u0026gt; and entrydate between #{begin} and #{end} \u0026lt;/if\u0026gt; order by update_time desc \u0026lt;/select\u0026gt; 当某些字段为空时，即未通过test测试，那么就不会查询该字段。\n问题 但是这样就又会引发一些问题，当name字段为空时，这段SQL语句会变成\n1 2 3 4 5 6 7 \u0026lt;select id=\u0026#34;list\u0026#34; resultType=\u0026#34;com.itheima.pojo.Emp\u0026#34;\u0026gt; select * from emp where and gender = #{gender} and entrydate between #{begin} and #{end} order by update_time desc \u0026lt;/select\u0026gt; 可以发现,，where后面多跟了一个and导致我们的语法发生了错误，当然也有解决办法，就是去使用\u0026lt;where\u0026gt;标签\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \u0026lt;select id=\u0026#34;list\u0026#34; resultType=\u0026#34;com.itheima.pojo.Emp\u0026#34;\u0026gt; select * from emp \u0026lt;where\u0026gt; \u0026lt;!-- if做为where标签的子元素 --\u0026gt; \u0026lt;if test=\u0026#34;name != null\u0026#34;\u0026gt; and name like concat(\u0026#39;%\u0026#39;,#{name},\u0026#39;%\u0026#39;) \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;gender != null\u0026#34;\u0026gt; and gender = #{gender} \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;begin != null and end != null\u0026#34;\u0026gt; and entrydate between #{begin} and #{end} \u0026lt;/if\u0026gt; \u0026lt;/where\u0026gt; order by update_time desc \u0026lt;/select\u0026gt; 更新某些数据 同样的，只有在前端传递管来的值不为null时才对里面的值进行修改\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; ?\u0026gt; \u0026lt;!DOCTYPE mapper PUBLIC \u0026#34;-//mybatis.org//DTD Mapper 3.0//EN\u0026#34; \u0026#34;https://mybatis.org/dtd/mybatis-3-mapper.dtd\u0026#34;\u0026gt; \u0026lt;mapper namespace=\u0026#34;com.itheima.mapper.EmpMapper\u0026#34;\u0026gt; \u0026lt;!--更新操作--\u0026gt; \u0026lt;update id=\u0026#34;update\u0026#34;\u0026gt; update emp set \u0026lt;if test=\u0026#34;username != null\u0026#34;\u0026gt; username=#{username}, \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;name != null\u0026#34;\u0026gt; name=#{name}, \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;gender != null\u0026#34;\u0026gt; gender=#{gender}, \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;image != null\u0026#34;\u0026gt; image=#{image}, \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;job != null\u0026#34;\u0026gt; job=#{job}, \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;entrydate != null\u0026#34;\u0026gt; entrydate=#{entrydate}, \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;deptId != null\u0026#34;\u0026gt; dept_id=#{deptId}, \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;updateTime != null\u0026#34;\u0026gt; update_time=#{updateTime} \u0026lt;/if\u0026gt; where id=#{id} \u0026lt;/update\u0026gt; \u0026lt;/mapper\u0026gt; 可是这样做又会引起sql语句的语法错误，例如当只存在第一个字段而不存在后面那些字段时，下面的语句就是\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; ?\u0026gt; \u0026lt;!DOCTYPE mapper PUBLIC \u0026#34;-//mybatis.org//DTD Mapper 3.0//EN\u0026#34; \u0026#34;https://mybatis.org/dtd/mybatis-3-mapper.dtd\u0026#34;\u0026gt; \u0026lt;mapper namespace=\u0026#34;com.itheima.mapper.EmpMapper\u0026#34;\u0026gt; \u0026lt;!--更新操作--\u0026gt; \u0026lt;update id=\u0026#34;update\u0026#34;\u0026gt; update emp set username=#{username}, where id=#{id} \u0026lt;/update\u0026gt; \u0026lt;/mapper\u0026gt; 也就是后面多加了一个逗号，同样的，我们也可以使用\u0026lt;set\u0026gt;标签来修改\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; ?\u0026gt; \u0026lt;!DOCTYPE mapper PUBLIC \u0026#34;-//mybatis.org//DTD Mapper 3.0//EN\u0026#34; \u0026#34;https://mybatis.org/dtd/mybatis-3-mapper.dtd\u0026#34;\u0026gt; \u0026lt;mapper namespace=\u0026#34;com.itheima.mapper.EmpMapper\u0026#34;\u0026gt; \u0026lt;!--更新操作--\u0026gt; \u0026lt;update id=\u0026#34;update\u0026#34;\u0026gt; update emp \u0026lt;!-- 使用set标签，代替update语句中的set关键字 --\u0026gt; \u0026lt;set\u0026gt; \u0026lt;if test=\u0026#34;username != null\u0026#34;\u0026gt; username=#{username}, \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;name != null\u0026#34;\u0026gt; name=#{name}, \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;gender != null\u0026#34;\u0026gt; gender=#{gender}, \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;image != null\u0026#34;\u0026gt; image=#{image}, \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;job != null\u0026#34;\u0026gt; job=#{job}, \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;entrydate != null\u0026#34;\u0026gt; entrydate=#{entrydate}, \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;deptId != null\u0026#34;\u0026gt; dept_id=#{deptId}, \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;updateTime != null\u0026#34;\u0026gt; update_time=#{updateTime} \u0026lt;/if\u0026gt; \u0026lt;/set\u0026gt; where id=#{id} \u0026lt;/update\u0026gt; \u0026lt;/mapper\u0026gt; sql引用 对于一些重复的sql片段，我们可以使用片段包裹的方式来进行引入\n1 2 3 4 5 \u0026lt;sql id=\u0026#34;name\u0026#34;\u0026gt; select name, age, gender from user \u0026lt;/sql\u0026gt; 这个时候，如果想要引用上面这个sql语句，就可以使用include标签来进行引用\n1 2 3 \u0026lt;include refid=\u0026#34;name\u0026#34;\u0026gt; where id = 1 \u0026lt;/include\u0026gt; 这样就可以把重复的语句选出来。\n总结 这就是Mybatis连接数据库的所有操作，加油~\n","date":"2025-03-08T15:38:35+08:00","permalink":"https://XiaoPeng0x3.github.io/p/mybatis%E5%A2%9E%E5%88%A0%E6%9F%A5%E6%94%B9/","title":"Mybatis增删查改"},{"content":"springboot 解耦是一种很好的设计规范。Dao层负责解析数据，那么在service层里面，我们就需要new来自Dao层的实例对象来进行方法的调用。同理在Controller层我们需要new来自service层的对象，这样做如果后续Dao和Controller层的实现发生改变时，那么调用这两个的实例对象也需要修改。在spring里面也提供了解耦的方法，就是上交管理权限和容器注入。\n把new实例对象的操作交给spring来使用，这样spring就会使用容器来管理存储这些对象，并在使用时通过调用容器的方式来传递参数。\n解耦 将需要解耦的类使用Component，需要管理的对象使用AutoWired来进行注解。并且，为了更好的标注Component的作用，还衍生出了这些类的不同注解，例如：\n@Service：注解服务层 @Controller：注解控制层 @Repository：数据访问类 Mysql 在Mysql中，现在来总结一下事务和索引这两个不太熟悉的概念。\n事务 只有InnodeDB支持事务，其它类型的数据库均不支持。\n事务可以理解为一把锁。除此之外，当事物执行失败的时候，它还支持回归事物，即如果某条sql语句执行失败，那么可以通过事务回滚来回到执行前的状态。\n一些八股 数据库为了解决并发场景提出了事务的概念，而事务包括下面几个特点\n原子性(A)\n事务要么提交成功，要么提交失败后回滚，不存在事务中某一部分的值成功修改，而一部分值没有发生修改。其中失败回滚是通过日志系统来完成的，即在修改数据前先把旧的数据写入undo_log里面，当需要失败回滚时，就可以把数据还原。\n一致性(C)\n一致性是指事务确保了数据的合理性和正确性。合理性是指值的变量在一个合理的范围内\n隔离性(I)\n不同事务之间存在隔离性，并且隔离性也分等级，隔离等级越高，并发效果越好，但是效率越差。\n持久性(D)\n数据库就是专门为持久化数据而准备的\n在此，我们用两个事物A,B来分析一下可能出现的情况。\n脏读 顾名思义，脏读就是读取了一个错误的数据。\n假设一种情况，事务A修改了一些数据，但是还未来得及提交，此时事务B再读这些数据就是发生了脏读。\n幻读 幻读也是发生在查询数据之间的，例如在事务A里面我们要进行查询数据，在两条查询语句之间，事务B插入了新的数据；\n这会导致A查询出来的数据前后结果不一致，注意，幻读一般指查询的集合前后不一致，也就是查询出来的行数前后不同。\n不可重复读 与幻读相似，不过一般指的是某个值而并非某一行数据不可重复读。\n索引 索引是为了加快查询速度而抽象出来的一种数据结构。其中索引使用的数据结构是B+树。\nB+ 树 B+树是一种特殊的数据结构，其数据全部存储在叶子结点，而非叶子结点存储的是下一级的物理地址，与B树不同的的地方有\nB+树的数据只在叶子结点保存 其叶子结点是双向链表连接 查询效率高 但是本质上还是空间换时间的机制来加快查找速度。\n在mysql中，primary key以及unique值会自动创建索引，也就是默认索引，根据索引来查询数据的效率是最高的。\n下面是创建索引的sql语句\n1 create index idx_user_name on user(name) 一般创建的索引命名如下\nidx是代之索引index，user是表名，name是根据哪个字段创建索引，意思就是给user表的name字段创建一个索引。\n","date":"2025-03-06T13:33:42+08:00","permalink":"https://XiaoPeng0x3.github.io/p/springboot%E5%92%8Cmysql/","title":"Springboot和Mysql"},{"content":"Springboot 在IDEA里面配置好springboot的开发环境后，我们就可以进行一个简单的Springboot程序开发了。\n创建controller对象 创建controller包，这个包专门处理来自前端的请求，并把对应的请求返回到前端。下面是我们的一个简单入门程序\n1 2 3 4 5 6 7 8 9 10 11 12 package com.zxp.springbootstart.controller; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; @RestController public class HelloController { @RequestMapping(\u0026#34;/hello\u0026#34;) public String hello() { return \u0026#34;Hello World\u0026#34;; } } 其中，RequestMapping指定了映射的url，只要我们访问对应的这个hello，那么我们就会收到来自后端返回的Hello World。\n参数初始化 如果参数名和前端传递过来的参数名一致，例如下面这个GET请求\n1 localhost:8080/simpletest?name=Bob\u0026amp;age=40 这个URL传递了两个参数，即name和age，只要我们的函数形参与传递过来的参数一致，那么就可以自动初始化好。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package com.zxp.springbootstart.controller; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RequestParam; import org.springframework.web.bind.annotation.RestController; @RestController public class RequestController { @RequestMapping(\u0026#34;/simpletest\u0026#34;) public String simpletest(String name, Integer age) { System.out.println(\u0026#34;Name: \u0026#34; + name); System.out.println(\u0026#34;Age: \u0026#34; + age); return \u0026#34;success\u0026#34;; } } 如果我们不希望一定把参数名称给写死，那么就可以使用一个注解，@RequestParam，这个注解会自动把前端传递过来的参数给初始化好\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package com.zxp.springbootstart.controller; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RequestParam; import org.springframework.web.bind.annotation.RestController; @RestController public class RequestController { @RequestMapping(\u0026#34;/simpletest\u0026#34;) public String simpletest(@RequestParam(\u0026#34;name\u0026#34;) String userename, Integer age) { System.out.println(\u0026#34;Name: \u0026#34; + userename); System.out.println(\u0026#34;Age: \u0026#34; + age); return \u0026#34;success\u0026#34;; } } 多参数传递 假设传递了很多参数，例如name age address 等等，最好的做法是把这些参数全部都封装为一个类。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 package com.zxp.springbootstart.pojo; public class User { private String name; private Integer age; public User(String name, int age) { this.name = name; this.age = age; } public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } @Override public String toString() { return \u0026#34;User [name=\u0026#34; + name + \u0026#34;, age=\u0026#34; + age + \u0026#34;]\u0026#34;; } } 这样我们在传递参数的时候只需要传递User即可。\n数组集合参数 当有许多参数的名称一致时，例如前端传递了一个多选项的参数，我们可以使用数组或者集合来存储这些参数。\n使用数组来进行保存\n1 2 3 4 5 @RequestMapping(\u0026#34;/arrayParam\u0026#34;) public String arrayParam(String[] hobby) { System.out.println(Arrays.toString(hobby)); return \u0026#34;success\u0026#34;; } 也可以使用集合来进行保存\n1 2 3 4 5 @RequestMapping(\u0026#34;/listParam\u0026#34;) public String listParam(@RequestParam(\u0026#34;hobby\u0026#34;) List\u0026lt;String\u0026gt; hobby) { System.out.println(hobby); return \u0026#34;success\u0026#34;; } 时间参数 后端需要使用后端controller方法中，需要使用Date类型或LocalDateTime类型，来封装传递的参数。时间参数需要按照一定的参数进行传递\n1 2 3 4 5 @RequestMapping(\u0026#34;/dateParam\u0026#34;) public String dateParam(@DateTimeFormat(pattern = \u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;) LocalDateTime updateTime){ System.out.println(updateTime); return \u0026#34;OK\u0026#34;; } 需要指定pattern，在前端传递时间参数的时候也需要按照一定的方式去传递时间参数。\nJson Json是键值对类型的数据，其键是字符串类型，下面一个就是一个json数据的示例\n1 2 3 4 { \u0026#34;name\u0026#34;: \u0026#34;Bob\u0026#34;, \u0026#34;age\u0026#34;: 20 } 在传递json参数的时候需要使用RequestBody来进行注解。\n路径参数 在传递参数的时候，还可以传递一些路径参数，需要使用@PathVariable来接受参数\n1 2 3 4 5 @RequestMapping(\u0026#34;/path/{id}\u0026#34;) public String path(@PathVariable Integer id) { System.out.println(id); return \u0026#34;success\u0026#34;; } 在传递参数的时候就可以使用这样的URL\n1 localhost:8080/path/10 那么在后端得到的数据就是10\n统一响应结果 在返回响应时，如果每次都是用不同的类型返回，那么前端每次都得处理不同的数据格式，我们可以把返回结果封装为一个类来进行使用。\n这里又可以分为三种情况\n只需要响应状态 需要返回数据 响应错误 其中需要封装好三个变量\n1 2 3 private int code; private String msg; private Object data; 响应案例 首先将网页等静态资源全部存储在static目录下，在这个案例中，我们需要掌握的技巧有：\n如何动态的获取文件路径 stream流式修改数据 响应数据 动态获取地址\n1 String file = this.getClass().getClassLoader().getResource(\u0026#34;emp.xml\u0026#34;).getFile(); 中间处理\n将前端传过来的数据进行替换\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 empList.stream().forEach(emp -\u0026gt; { String gender = emp.getGender(); if (\u0026#34;1\u0026#34;.equals(gender)) { emp.setGender(\u0026#34;man\u0026#34;); } else if (\u0026#34;2\u0026#34;.equals(gender)) { emp.setGender(\u0026#34;woman\u0026#34;); } String job = emp.getJob(); if (\u0026#34;1\u0026#34;.equals(job)) { emp.setJob(\u0026#34;讲师\u0026#34;); } else if (\u0026#34;2\u0026#34;.equals(job)) { emp.setJob(\u0026#34;班主任\u0026#34;); } else if (\u0026#34;3\u0026#34;.equals(job)) { emp.setJob(\u0026#34;就业指导\u0026#34;); } } ); 在前端传递过来的数据中，其中gender和job都是id来标志的，我们需要将这些数据进行人为替换后再将数据写回前端。\n写回前端时可以使用我们封装好的Result作为统一结果返回。\n1 return Result.success(empList); 开发规范 在开发时要注意分层，每个层只完成自己的那一部分任务即可。如果把所有的逻辑全部写在controller层不太规范。\n一般的，我们可以将这个逻辑划分为三个部分，即后端处理来自前端的响应controller层；响应后完成server服务；中间需要进行数据的可以再封装一个dao层来专门管理数据。\n我们来改造一下这个代码\n首先在dao层处理所有的获取资源操作，实现一个dao接口\n1 2 3 4 public interface EmpDao{ // 获取资源并将资源封装到集合里面 public List\u0026lt;Emp\u0026gt; listEmp(){} } 实现这个接口\n1 2 3 4 5 6 7 8 @Override public List\u0026lt;Emp\u0026gt; listEmp() { // 返回listEmp //1. 解析前端页面 String file = this.getClass().getClassLoader().getResource(\u0026#34;emp.xml\u0026#34;).getFile(); List\u0026lt;Emp\u0026gt; empList = XmlParserUtils.parse(file, Emp.class); return empList; } 实现service代码，这里负责处理数据，也就是业务逻辑.\n1 2 3 public interface EmpService { public List\u0026lt;Emp\u0026gt; listEmp(); } 在controller层对数据进行响应。\n1 2 3 4 5 6 7 private EmpService empService = new EmpServiceA(); @RequestMapping(\u0026#34;/listEmp\u0026#34;) public Result listEmp() { // 得到处理的listemp List\u0026lt;Emp\u0026gt; list = empService.listEmp(); return Result.success(list); } 这样做其实还不够规范，可以看到我们每次都是去new一个私有对象然后再去获取里面的方法，如果后面修改名称的话，这样就会导致代码报错。下次来记录怎么解决这个问题。\n","date":"2025-03-05T16:54:54+08:00","permalink":"https://XiaoPeng0x3.github.io/p/springboot%E5%85%A5%E9%97%A8%E4%BD%93%E9%AA%8C/","title":"Springboot入门体验"},{"content":"启动服务 在win下可以使用net启动mysql服务。\n首先打开具有管理员权限的cmd窗口，否则将会被禁止访问\n在窗口中输入\n1 net start mysql 然后就可以等待服务开启\n如果想要关闭服务，只需要输入\n1 net stop mysql 登录服务 在mysql中可以指定用户的登录，其命令格式如下‘\n1 mysql -u root -p password -u参数后面接的是登录时的用户名\n如果想要登出mysql，\n1 quit; 密码相关 修改密码 假设已经知道原来的密码，比如原来的密码是123，想要把密码修改为123456。\n登录mysql\n1 mysql -u root -p 123 修改表\n1 SET PASSWORD FOR \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39; = \u0026#39;123456\u0026#39;; 刷新权限\n1 2 FLUSH PRIVILEGES; EXIT; 登出\n可以进行验证\n忘记密码 关闭所有正在运行的mysql服务\n1 net stop mysql 以跳过授权的方式登录\n1 mysqld --skip-grant-tables --skip-networking 直接登录\n1 mysql -u root 然后进行修改\n1 2 3 ALTER USER \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;123456\u0026#39;; FLUSH PRIVILEGES; EXIT; 重启mysql\n1 net start mysql 进行验证\n","date":"2025-03-04T10:22:06+08:00","permalink":"https://XiaoPeng0x3.github.io/p/mysql%E7%9A%84%E7%99%BB%E9%99%86/","title":"Mysql的登陆"},{"content":"Js的知识 引入方式 在html文件中引入js只需要输入\n1 2 3 \u0026lt;script\u0026gt; \u0026lt;/script\u0026gt; 即可，这个script标签在任意格子内即可，可以在head标签内，可以在body标签内，也可以在html标签的外面\n输出 一共有三种输出方式\n弹窗\n1 2 3 \u0026lt;script\u0026gt; alert(\u0026#34;Hello\u0026#34;) \u0026lt;/script\u0026gt; 输出到屏幕中\n1 2 3 \u0026lt;script\u0026gt; document.write(\u0026#34;Hello\u0026#34;) \u0026lt;/script\u0026gt; 控制台输出\n1 2 3 \u0026lt;script\u0026gt; console.write(\u0026#34;Hello\u0026#34;) \u0026lt;/script\u0026gt; 变量 js是弱语言类型，其不在乎变量的类型。可以使用var和let关键字声明一个变量，唯一的区别就是，var声明的关键字是全局关键变量，而let是局部变量。\n全局变量可以多次赋值，而局部变量不能多次赋值。\n1 2 var a = 10; // 局部变量 let b = 10; // 全局变量 除此之外还可以使用const声明常量，常量不可进行修改\n数据类型 可以使用typeof来查看一个变量或者值的类型，例如\n1 2 typeof 10 // number typeof \u0026#34;10\u0026#34; // string 运算符 js的运算符与其它语言不同的有===和!===这两个。由于js是静态语言，所以当将不同的类型相比较时，如果使用==，例如\n1 1 == \u0026#34;1\u0026#34; 输出结果为true\n如果想要比较这两个类型时，可以使用强相等\n1 1 === \u0026#34;1\u0026#34; 输出结果为false\n函数 直接声明一个函数，function关键字\n1 2 3 function add(a, b) { return a + b; } 使用var声明关键字\n1 2 3 var add = function(a, b) { return a + b; } json json是一种键值对形式的输入，其k值是string类型\n1 2 3 4 5 6 7 //定义json var jsonstr = \u0026#39;{\u0026#34;name\u0026#34;:\u0026#34;Tom\u0026#34;, \u0026#34;age\u0026#34;:18, \u0026#34;addr\u0026#34;:[\u0026#34;北京\u0026#34;,\u0026#34;上海\u0026#34;,\u0026#34;西安\u0026#34;]}\u0026#39;; alert(jsonstr.name); //json字符串--js对象 var obj = JSON.parse(jsonstr); alert(obj.name); Bom对象 统称为浏览器对象\n弹窗类的消息都是windows下的消息\n1 2 window.alert(\u0026#34;Hello BOM\u0026#34;); alert(\u0026#34;Hello BOM Window\u0026#34;); 1 2 3 4 5 6 7 8 9 alert(\u0026#39;提示信息\u0026#39;) confirm(\u0026#34;确认信息\u0026#34;) prompt(\u0026#34;弹出输入框\u0026#34;) open(\u0026#34;url地址\u0026#34;，“_black或_self”，“新窗口的大小”） close() 关闭当前的网页 1 2 3 4 5 6 7 setTimeout(函数，时间) 只执行一次 clearTimeout(定时器名称) 清除定时器，用于停止执行setTimeout()方法的函数代码。 setInterval(函数，时间) 无限执行 clearInterval() 方法用于停止 setInterval() 方法执行的函数代码。 location 对象\n对象用于获得当前页面的地址 (URL)，并把浏览器重定向到新的页面。\nwindow.location对象在编写时可不使用 window 这个前缀。 一些例子：\nlocation.herf = \u0026lsquo;url地址\u0026rsquo;\nlocation.hostname 返回 web 主机的域名\nDom操作 通过document来进行一系列操作，可以通过类，标签，id，name这些操作来获取html的元素，这也体现了为什么js可以对前端进行一些交互的原因，下面是一些例子\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 //1. 点亮灯泡 : src 属性值 var img = document.getElementById(\u0026#39;h1\u0026#39;); img.src = \u0026#34;img/on.gif\u0026#34;; //2. 将所有div标签的内容后面加上: very good (红色字体) -- \u0026lt;font color=\u0026#39;red\u0026#39;\u0026gt;\u0026lt;/font\u0026gt; var divs = document.getElementsByTagName(\u0026#39;div\u0026#39;); for (let i = 0; i \u0026lt; divs.length; i++) { const div = divs[i]; div.innerHTML += \u0026#34;\u0026lt;font color=\u0026#39;red\u0026#39;\u0026gt;very good\u0026lt;/font\u0026gt;\u0026#34;; } //3. 使所有的复选框呈现选中状态 var ins = document.getElementsByName(\u0026#39;hobby\u0026#39;); for (let i = 0; i \u0026lt; ins.length; i++) { const check = ins[i]; check.checked = true;//选中 } 事件绑定 大多数交互的事件由鼠标产生，例如点击、移动、缩放等这些操作，我们可以写两个按钮，点击这两个按钮的时候会弹出两个通知\n1 2 3 4 \u0026lt;body\u0026gt; \u0026lt;input type=\u0026#34;button\u0026#34; id=\u0026#34;btn1\u0026#34; value=\u0026#34;事件绑定1\u0026#34; onclick=\u0026#34;on()\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;button\u0026#34; id=\u0026#34;btn2\u0026#34; value=\u0026#34;事件绑定2\u0026#34; onclick=\u0026#34;off()\u0026#34;\u0026gt; \u0026lt;/body\u0026gt; 这里添加的监听时间是onclick，我们来实现一下这两个js函数\n1 2 3 function on() { alert(\u0026#34;开启\u0026#34;) } off函数\n1 2 3 function off() { alert(\u0026#34;关闭\u0026#34;) } 值得注意的是，在html样式里面，需要以函数调用的形式来指定，如果只写上函数名是不会调用的。\n","date":"2025-02-28T19:01:25+08:00","permalink":"https://XiaoPeng0x3.github.io/p/js%E7%9F%A5%E8%AF%86/","title":"Js知识"},{"content":"html 下面来看一些高级技巧\n视频video video的标签是\n1 \u0026lt;video src=\u0026#34;xxx\u0026#34; controls width=\u0026#34;\u0026#34;\u0026gt;\u0026lt;/video\u0026gt; 同样的指明路径就可以添加进来，初次之外还需要添加播放组件和进度条这些功能\n音频 audio 音频的标签\n1 \u0026lt;audio src= \u0026#34;XXX\u0026#34;\u0026gt;\u0026lt;/audio\u0026gt; 正文 正文的标签是\u0026lt;p\u0026gt;,使用正文可以包裹一些文字\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;!-- 超链接部分 --\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;焦点访谈：中国底气 新思想夯实大国粮仓\u0026lt;/title\u0026gt; \u0026lt;!-- \u0026lt;style\u0026gt; h1 { color: blue; } \u0026lt;/style\u0026gt; --\u0026gt; \u0026lt;link rel=\u0026#34;style\u0026#34; href=\u0026#34;../css/news.css\u0026#34;/\u0026gt; \u0026lt;style\u0026gt; span { color: red; } .time { color: black; } #f { color: black; } a { color: red; text-decoration: none } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;img src=\u0026#34;../img/news_logo.png\u0026#34; /\u0026gt; 新浪政务 \u0026gt; 正文 \u0026lt;!-- \u0026lt;h1 style=\u0026#34;color: red;\u0026#34;\u0026gt; 焦点访谈：中国底气 新思想夯实大国粮仓 \u0026lt;/h1\u0026gt; --\u0026gt; \u0026lt;h1\u0026gt; 焦点访谈：中国底气 新思想夯实大国粮仓 \u0026lt;/h1\u0026gt; \u0026lt;hr/\u0026gt; \u0026lt;span class=\u0026#34;time\u0026#34;\u0026gt; 2023年03月02日 21:50 \u0026lt;/span\u0026gt; \u0026lt;span id=\u0026#34;f\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;baidu.com\u0026#34;\u0026gt; 央视网 \u0026lt;/a\u0026gt; \u0026lt;/span\u0026gt; \u0026lt;hr/\u0026gt; \u0026lt;!-- 添加视频 --\u0026gt; \u0026lt;video src=\u0026#34;../video/1.mp4\u0026#34; controls width=\u0026#34;900\u0026#34;\u0026gt;\u0026lt;/video\u0026gt; \u0026lt;!-- 添加音频 --\u0026gt; \u0026lt;audio src=\u0026#34;../audio/1.mp3\u0026#34;\u0026gt;\u0026lt;/audio\u0026gt; \u0026lt;p\u0026gt; \u0026lt;strong\u0026gt;央视网消息\u0026lt;/strong\u0026gt; （焦点访谈）：党的十八大以来，以习近平同志为核心的党中央始终把解决粮食安全问题作为治国理政的头等大事，重农抓粮一系列政策举措有力有效，我国粮食产量站稳1.3万亿斤台阶，实现谷物基本自给、口粮绝对安全。我们把饭碗牢牢端在自己手中，为保障经济社会发展提供了坚实支撑，为应对各种风险挑战赢得了主动。连续八年1.3万亿斤，这个沉甸甸的数据是如何取得的呢？ \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; 人勤春来早，春耕农事忙。立春之后，由南到北，我国春耕春管工作陆续展开，春天的田野处处生机盎然。 \u0026lt;/p\u0026gt; \u0026lt;img src=\u0026#34;../img/1.jpg\u0026#34;\u0026gt; \u0026lt;p\u0026gt; 今年，我国启动了新一轮千亿斤粮食产能提升行动，这是一个新的起点。2015年以来，我国粮食产量连续8年稳定在1.3万亿斤以上，人均粮食占有量始终稳稳高于国际公认的400公斤粮食安全线。从十年前的约12200亿斤到2022年的约13700亿斤，粮食产量提高了1500亿斤。 \u0026lt;/p\u0026gt; \u0026lt;img src=\u0026#34;../img/2.jpg\u0026#34;\u0026gt; \u0026lt;p\u0026gt; 中国式现代化一个重要的中国特色是人口规模巨大的现代化。我们粮食生产的发展，意味着我们要立足国内，解决14亿多人吃饭的问题。仓廪实，天下安。保障粮食安全是一个永恒的课题，任何时候都不能放松。在以习近平同志为核心的党中央坚强领导下，亿万中国人民辛勤耕耘、不懈奋斗，我们就一定能够牢牢守住粮食安全这一“国之大者”，把中国人的饭碗牢牢端在自己手中，夯实中国式现代化基础。 \u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 其中，加黑加粗的字体可以使用strong标签和b标签来进行表示。\n表格 表格的标签是table开头，在中间，每一行的标签是tr，行内数据是td,如果是第一行，那么可以使用th来进行加粗显示\n例如，以下表格\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;表格\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;table border=\u0026#34;1px\u0026#34; cellspacing=\u0026#34;0\u0026#34; width=\u0026#34;600px\u0026#34;\u0026gt; \u0026lt;!-- 表格的第一个头可以使用th --\u0026gt; \u0026lt;tr\u0026gt; 单元行 \u0026lt;th\u0026gt; 行内内容 序号 \u0026lt;/th\u0026gt; \u0026lt;th\u0026gt; 品牌logo \u0026lt;/th\u0026gt; \u0026lt;th\u0026gt; 品牌名称 \u0026lt;/th\u0026gt; \u0026lt;th\u0026gt; 企业名称 \u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 其样式结果如下\n可以把这个表格更加完善一下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;表格\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; td { text-align: center; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;table border=\u0026#34;1px\u0026#34; cellspacing=\u0026#34;0\u0026#34; width=\u0026#34;600px\u0026#34;\u0026gt; \u0026lt;!-- 表格的第一个头可以使用th --\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt; 序号 \u0026lt;/th\u0026gt; \u0026lt;th\u0026gt; 品牌logo \u0026lt;/th\u0026gt; \u0026lt;th\u0026gt; 品牌名称 \u0026lt;/th\u0026gt; \u0026lt;th\u0026gt; 企业名称 \u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; 1 \u0026lt;/td\u0026gt; \u0026lt;!-- logo --\u0026gt; \u0026lt;td\u0026gt; \u0026lt;img src=\u0026#34;../img/huawei.jpg\u0026#34; width=\u0026#34;100\u0026#34;\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; 华为 \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; 华为技术有限公司 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; 2 \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;img src=\u0026#34;../img/alibaba.jpg\u0026#34; width=\u0026#34;100\u0026#34;\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; 阿里 \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; 阿里巴巴集团有限公司 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 为了更加美观，我们可以给这个单元格加入居中效果\n1 2 3 4 5 \u0026lt;style\u0026gt; td { text-align: center; } \u0026lt;/style\u0026gt; 表单 表单是以form的形式开头，form里面有两个参数，一个是action，一个是method这两个参数，action指向的是提交的url，如果不指定，那么默认就是本机;method一个是get，另一个就是post方法，get会把提交的参数全部放到url里面，而post是直接以原始数据的形式进行提交。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=form, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;form action=\u0026#34;\u0026#34;, method=\u0026#34;get\u0026#34;\u0026gt; 用户名：\u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;userName\u0026#34;\u0026gt; 年龄：\u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;age\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;submit\u0026#34; value=\u0026#34;提交\u0026#34;\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 输入条和提交条都是以input标签的形式存在，需要显示使用type来指定他们的提交类型。\n总结 视频可以使用标签video来进行插入\n音频可以使用audio来进行插入\n正文是使用p标签来进行插入，在p包裹的标签内。同时可以使用text-indent来指定每一段的缩进尺寸\n表格是一种特殊的样式\n以table开始 每一行单元格可以使用tr来进行包裹 行中间，如果是第一行非数据元素，可以使用th(table head)来起到加大加粗剧中的样式 其它则可以使用td来进行包裹 表单是以form来进行书写的，action可以指向需要提交的url，method可以选择需要提交的方法。\n","date":"2025-02-28T15:20:43+08:00","permalink":"https://XiaoPeng0x3.github.io/p/html2/","title":"Html2"},{"content":"使用clash来加快访问 在国内想要访问github时比较麻烦，需要走代理，在使用git来push代码的时候可以使用clash的7890端口来走代理，在git中输入以下配置即可\nsocket5代理\n1 2 3 git config --global http.proxy \u0026#39;socks5://127.0.0.1:socks5端口号\u0026#39; git config --global https.proxy \u0026#39;socks5://127.0.0.1:socks5端口号\u0026#39; 端口号一般是7890\nhttp/https代理\n1 2 3 git config --global http.proxy \u0026#39;http://127.0.0.1:http端口号\u0026#39; git config --global https.proxy \u0026#39;https://127.0.0.1:https端口号\u0026#39; 在设置完后，可以ping一下github.com来尝试连通状况\n1 ping github.com ","date":"2025-02-28T14:53:50+08:00","permalink":"https://XiaoPeng0x3.github.io/p/push%E5%92%8Cpoll%E9%97%AE%E9%A2%98/","title":"Push和poll问题"},{"content":"html 通过复制新浪新闻页面来学习html的各种格式。\n快速入门 html的格式很简单，我们可以编辑一个简单的html文件来进行展示，在vscode里可以使用快捷键!来进行模板的生成，生成的代码如下\n1 2 3 4 5 6 7 8 9 10 11 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 其中需要关注的是head和body这两个模块，我们可以先在body里面添加一些内容，例如Hello，World\n1 2 3 4 5 6 7 8 9 10 11 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; Hello, World \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 其结果如下\nhead里面的title标签就是网页的标签，所以我们可以把这标签修改为我们想要的内容，例如\n1 2 3 4 5 6 7 8 9 10 11 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;焦点访谈：中国底气 新思想夯实大国粮仓\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; Hello, World \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 可以看到我们已经修改成功了\n现在我们来模拟一下新浪的新闻标题，新浪的标题结果如下\n我们来思考一下需要完成哪些任务\n新浪的图片标志 新浪\u0026gt;正文是一个超链接 中间的加粗加黑的大字标题 两条分割线 文字的样式 图片 我们需要插入一张图片，这张图片的路径可以是本地的路径，也可以是远程服务器上的路径，不过需要给出对应的url，图片的插入语法如下\n1 \u0026lt;img src:\u0026#34;url\u0026#34; /\u0026gt; 其中需要提供一个url来确定图片，除此之外，还可以调整图片的样式，比如图片的缩放程度。\n1 \u0026lt;img src:\u0026#34;\u0026#34; width=\u0026#34;xxx\u0026#34; height=\u0026#34;xxx\u0026#34; /\u0026gt; 假设我们把图片固定为300x300，那么需要指定好长宽比例\n1 \u0026lt;img src= \u0026#34;xxx\u0026#34; width=\u0026#34;300\u0026#34; height=\u0026#34;300\u0026#34; /\u0026gt; 为了还原，我们把页面设置为原始比例。后面我们先不设置超链接，所以只需要在图片的后面添加几行字即可，所以现在的代码变为\n1 2 3 4 5 6 7 8 9 10 11 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;焦点访谈：中国底气 新思想夯实大国粮仓\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;img src=\u0026#34;../img/news_logo.png\u0026#34; /\u0026gt; 新浪政务\u0026gt;正文 \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 大字标题 这种大字标题其实是\u0026lt;h\u0026gt; \u0026lt;/h\u0026gt;标签的作用，在html一共有6级标题可以设置，所以我们可以使用h1标题来设置。\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;焦点访谈：中国底气 新思想夯实大国粮仓\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;img src=\u0026#34;../img/news_logo.png\u0026#34; /\u0026gt; 新浪政务\u0026gt;正文 \u0026lt;h1\u0026gt; 焦点访谈：中国底气 新思想夯实大国粮仓 \u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 此时下方还有一个分割线，而分割线也是有专门对应的标签，即\u0026lt;hr/\u0026gt;(horizontal rule的缩写).现在我们的标题就变得和示例的标题一摸一样了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;焦点访谈：中国底气 新思想夯实大国粮仓\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;img src=\u0026#34;../img/news_logo.png\u0026#34; /\u0026gt; 新浪政务 \u0026gt; 正文 \u0026lt;h1\u0026gt; 焦点访谈：中国底气 新思想夯实大国粮仓 \u0026lt;/h1\u0026gt; \u0026lt;hr/\u0026gt; 2023年03月02日 21:50 央视网 \u0026lt;hr/\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 样式 我们可以修改字体的颜色，例如，我们把标题的字体颜色修改为red，这里总共有三种方式：\n直接在行内标明颜色\n1 \u0026lt;h1 style=\u0026#34;color: red;\u0026#34;\u0026gt; 焦点访谈：中国底气 新思想夯实大国粮仓 \u0026lt;/h1\u0026gt; 这样大标题就换了颜色\n内嵌样式\n直接在\u0026lt;head\u0026gt;里面添加一个style样式，然后指定标签的名称，把对应的样式填进去即可\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;焦点访谈：中国底气 新思想夯实大国粮仓\u0026lt;/title\u0026gt; 把整体样式填写在这里 \u0026lt;style\u0026gt; h1 { color: blue; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;img src=\u0026#34;../img/news_logo.png\u0026#34; /\u0026gt; 新浪政务 \u0026gt; 正文 \u0026lt;!-- \u0026lt;h1 style=\u0026#34;color: red;\u0026#34;\u0026gt; 焦点访谈：中国底气 新思想夯实大国粮仓 \u0026lt;/h1\u0026gt; --\u0026gt; \u0026lt;h1\u0026gt; 焦点访谈：中国底气 新思想夯实大国粮仓 \u0026lt;/h1\u0026gt; \u0026lt;hr/\u0026gt; 2023年03月02日 21:50 央视网 \u0026lt;hr/\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 注意，这样做会装饰所有\u0026lt;h1\u0026gt;包裹的字体，一个优点是，如果需要统一某些共同的样式，那么这样可以快速的统一一些样式。\n外联样式\n把这些样式专门封装为.css，然后使用link把该样式链接进来。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;焦点访谈：中国底气 新思想夯实大国粮仓\u0026lt;/title\u0026gt; \u0026lt;!-- \u0026lt;style\u0026gt; h1 { color: blue; } \u0026lt;/style\u0026gt; --\u0026gt; 外联样式 \u0026lt;link rel=\u0026#34;style\u0026#34; href=\u0026#34;../css/news.css\u0026#34;/\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;img src=\u0026#34;../img/news_logo.png\u0026#34; /\u0026gt; 新浪政务 \u0026gt; 正文 \u0026lt;!-- \u0026lt;h1 style=\u0026#34;color: red;\u0026#34;\u0026gt; 焦点访谈：中国底气 新思想夯实大国粮仓 \u0026lt;/h1\u0026gt; --\u0026gt; \u0026lt;h1\u0026gt; 焦点访谈：中国底气 新思想夯实大国粮仓 \u0026lt;/h1\u0026gt; \u0026lt;h1\u0026gt; 变成了蓝色\u0026lt;/h1\u0026gt; \u0026lt;hr/\u0026gt; 2023年03月02日 21:50 央视网 \u0026lt;hr/\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; rel：是 \u0026ldquo;relationship\u0026rdquo; 的缩写，表示“关系”。href：是 \u0026ldquo;hypertext reference\u0026rdquo; 的缩写，表示“超文本引用”，即资源的地址或链接。\n总结，当需要重复引用一个样式时，外联样式可能是最好的选择。\n分块样式 当多个内容处于一行的时候，我们可以使用分块的标签来对其进行修饰。\n\u0026lt;span\u0026gt;选择器是一个空白标签，被这个标签包裹的内容在默认情况下不会作出任何修改，但是还是可以凭借这个标签来进行样式的修改，同样的，可以作用在\u0026lt;span\u0026gt;上的修饰也有三种\n直接修饰\n元素选择器\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;焦点访谈：中国底气 新思想夯实大国粮仓\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; 专门进行修饰 span { color: red; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;img src=\u0026#34;../img/news_logo.png\u0026#34; /\u0026gt; 新浪政务 \u0026gt; 正文 \u0026lt;h1\u0026gt; 焦点访谈：中国底气 新思想夯实大国粮仓 \u0026lt;/h1\u0026gt; \u0026lt;hr/\u0026gt; \u0026lt;span\u0026gt;2023年03月02日\u0026lt;/span\u0026gt; 21:50 央视网 \u0026lt;hr/\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 类选择器\n可以在style中指定类选择器,需要我们给这个类起一个名字，例如，我们想要修饰时间，把所有时间字体全部修改为蓝色，那么我们就可以这样写\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;焦点访谈：中国底气 新思想夯实大国粮仓\u0026lt;/title\u0026gt; \u0026lt;!-- \u0026lt;style\u0026gt; h1 { color: blue; } \u0026lt;/style\u0026gt; --\u0026gt; \u0026lt;link rel=\u0026#34;style\u0026#34; href=\u0026#34;../css/news.css\u0026#34;/\u0026gt; \u0026lt;style\u0026gt; span { color: red; } .time { color: blue; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;img src=\u0026#34;../img/news_logo.png\u0026#34; /\u0026gt; 新浪政务 \u0026gt; 正文 \u0026lt;!-- \u0026lt;h1 style=\u0026#34;color: red;\u0026#34;\u0026gt; 焦点访谈：中国底气 新思想夯实大国粮仓 \u0026lt;/h1\u0026gt; --\u0026gt; \u0026lt;h1\u0026gt; 焦点访谈：中国底气 新思想夯实大国粮仓 \u0026lt;/h1\u0026gt; \u0026lt;hr/\u0026gt; \u0026lt;span class=\u0026#34;time\u0026#34;\u0026gt;2023年03月02日\u0026lt;/span\u0026gt; \u0026lt;span\u0026gt; 21:50 央视网 \u0026lt;/span\u0026gt; \u0026lt;hr/\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 在标签里面定义好class，然后在head的style里面使用.classname来进行修饰。值得一提的是，在span和class同时都被修饰的情况下，class的优先级更好\n使用id来进行修饰\n同样的，要想使用id，可以使用#开头来进行修饰，需要注意的是,id是唯一的，不可以重复使用，如果要重复使用，那么可以使用类装饰器\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;焦点访谈：中国底气 新思想夯实大国粮仓\u0026lt;/title\u0026gt; \u0026lt;!-- \u0026lt;style\u0026gt; h1 { color: blue; } \u0026lt;/style\u0026gt; --\u0026gt; \u0026lt;link rel=\u0026#34;style\u0026#34; href=\u0026#34;../css/news.css\u0026#34;/\u0026gt; \u0026lt;style\u0026gt; span { color: red; } .time { color: blue; } #f { color: #00ff00; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;img src=\u0026#34;../img/news_logo.png\u0026#34; /\u0026gt; 新浪政务 \u0026gt; 正文 \u0026lt;!-- \u0026lt;h1 style=\u0026#34;color: red;\u0026#34;\u0026gt; 焦点访谈：中国底气 新思想夯实大国粮仓 \u0026lt;/h1\u0026gt; --\u0026gt; \u0026lt;h1\u0026gt; 焦点访谈：中国底气 新思想夯实大国粮仓 \u0026lt;/h1\u0026gt; \u0026lt;hr/\u0026gt; \u0026lt;span class=\u0026#34;time\u0026#34;\u0026gt; 2023年03月02日 21:50 \u0026lt;/span\u0026gt; \u0026lt;span id=\u0026#34;f\u0026#34;\u0026gt; 央视网 \u0026lt;/span\u0026gt; \u0026lt;hr/\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 超链接 超链接的标签是a，在使用时，我们只需要把超链接标签包裹住对应的文字即可\n1 2 3 4 5 \u0026lt;span id=\u0026#34;f\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;baidu.com\u0026#34;\u0026gt; 央视网 \u0026lt;/a\u0026gt; \u0026lt;/span\u0026gt; 这样就可以把央视网指向对应的链接处，不过这样默认是链接的形式，所以可以设置css样式来取消下划线。\n在style里面为所有的a标签来指定样式\n1 2 3 4 a { color: black; text-decoration: none; /* 设置文本为一个标准的文本 */ } 总结 图片的插入\n1 \u0026lt;img src=\u0026#34;\u0026#34; /\u0026gt; 标题\n1 2 3 4 5 6 7 \u0026lt;h1\u0026gt; \u0026lt;/h1\u0026gt; \u0026lt;h2\u0026gt; \u0026lt;/h2\u0026gt; 一直到h6\ncss样式\n这里样式比较多，需要了解\n直接使用标签\n类装饰器，前面要加“.\u0026quot;\nid装饰器，前面加\u0026quot;#\u0026quot;,每个标签使用的id是唯一的\n可以连接外部css样式\n1 \u0026lt;link rel=\u0026#34;xxx\u0026#34; href=\u0026#34;xxx\u0026#34;/\u0026gt; 超链接\n超链接的标签是\u0026lt;a href= \u0026quot;\u0026quot;\u0026gt; \u0026lt;/a\u0026gt;，凡是这样的标签都可以在style里面进行修饰，超链接也不例外\n","date":"2025-02-27T17:27:54+08:00","permalink":"https://XiaoPeng0x3.github.io/p/html-day01/","title":"Html-day01"},{"content":"多线程 创建启动一个线程 创建一个线程大概分为下面几步\n自定义线程类继承Thread类 重写run方法，编写线程执行体 创建线程对象，调用start方法启动线程 在继承Thread类时，首先要重写run方法，即把需要在多线程里面实现的逻辑写到run方法中\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 public class MyThread extends Thread{ @Override public void run() { // run方法 for (int i = 0; i \u0026lt; 20; i++) { System.out.println(\u0026#34;我在看代码！\u0026#34;); } } public static void main(String[] args) { // main线程 // 创建一个线程 MyThread mt = new MyThread(); // 调用start来开启线程 mt.start(); // 程序还是串行执行的 mt.run(); for (int i = 0; i \u0026lt; 20; i++) { System.out.println(\u0026#34;主线程\u0026#34;); } } } 与run方法不同的是，start方法会额外开启一个新的线程来执行run方法里面的逻辑，而调用run缺不会额外开启一个新的线程。\n多线程下载器 把一些业务逻辑放在run方法里面，我们就可以使用多线程来进行一些业务逻辑处理。\n首先构建好一个下载器\n1 2 3 4 5 6 7 8 9 class DownLoad{ public void downloader(String url, String name) { // 使用一个FileUtils包来进行书写 try{ FileUtils.copyURLToFile(new URL(url), new File(name)); }catch(IOExpection e) throw new RuntimeException(e); } } 构建好下载器后就可以启动线程这些资源\n还是三要素\n继承Thread 重写run方法 在main线程中new Thread对象，调用start方法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 public class TestThread extends Thread { // 定义好name String name; String url; public TestThread(String name, String url) { // xxxx } public void run() { // 重写run函数 // 使用下载器下载 DownLoader d = new DownLoader(); d.downloader(url, name); } public static void main(String[] args) { // 创建线程 TestThread t1 = new TestThread(xxx, xxx); TestThread t2 = new TestThread(xxx, xxx); // 启动start t1.start(); t2.start(); } } 这样就可以下载好图片。\n","date":"2025-02-26T20:08:05+08:00","permalink":"https://XiaoPeng0x3.github.io/p/thread%E7%B1%BB/","title":"Thread类"},{"content":"Quick Start 在安装后Pytorch之后，官方提供了一个基于FashionMNIST的分类任务，通过此任务可以让我们快速入门怎么样使用数据集去训练一个神经网络模型\n数据集加载 torchvision中集成了datasets的API以便于快速的下载一些常用的数据集\n1 2 from torchvision import datasets from torchvision.transforms import ToTensor 使用datasets\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Download training data from open datasets. train_data = datasets.FashionMNIST( root=\u0026#34;data\u0026#34;, train=True, download=True, transform=ToTensor(), ) # Download test data from open datasets. test_data = datasets.FashionMNIST( root=\u0026#34;data\u0026#34;, train=False, download=True, transform=ToTensor(), ) root是存储下载数据集的路径。ToTensor是把原始数据集转换为Pytorch的张量，一般来说，对于一张原始图片，其shape就是[H, W, C]，在Pytorch中，为了方便计算，通常在ToTensor后会把其shape转换为[C, H, W]的形状。\n加载数据集 在Pytorch中，加载数据集都是使用DataLoader类来进行加载，返回的是一个类似于迭代器的数据类型，值得注意的是，我们也可以自行构建一个自己的数据集来进行数据加载，而且这个数据集必须是Dataset类的子类。\n1 from torch.utils.data import DataLoader DataLoader的第一个参数类型就是Dataset类\n1 train_dataloader = DataLoader(train_data, shuffle= True, batch_size= 64) 后面就是一些必选参数，像shuffle, batch_size这些需要自己选择，一般来说，在训练集需要打开shuffle来进行训练，而测试集不需要随机打乱。batch_size就是每一次选择几张图片作为一个batch来进行训练，batch_size选择的越大，占用的内存也就越高。\n1 test_dataloader = DataLoader(test_data, shuffle=True, batch_size= 64) 加载之后，输入的数据应该有四个维度，即[B, C, H, W]，对于FashionMNIST来说，其数据维度就是[64, 1, 28, 28]，我们可以来验证一下\n1 2 3 4 for X, y in test_dataloader: print(f\u0026#34;Shape of X [N, C, H, W]: {X.shape}\u0026#34;) print(f\u0026#34;Shape of y: {y.shape} {y.dtype}\u0026#34;) break 选择损失函数和优化器 对于多分类问题，可以选择交叉熵损失函数即\n1 loss_func = nn.CrossEntropyLoss() 同样的，我们也可以选择不同的优化方法，例如SGD，SGD+Momentum，Adam等等这些损失器\n1 optim = torch.optim.Adam(model.parameters(), lr=0.001) 模型的训练 开始训练模型，可以把模型的训练封装为一个函数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def train(dataloader, model, loss_func, optimizer): # 获取总的数据个数 size = len(dataloader.dataset) # 开启训练模式 model.train() for batch, (x, y) in enumerate(dataloader): X, y = X.to(device), y.to(device) # 通过model来获取pred_y pred = model(X) # 损失函数 loss = loss_func(pred, y) # 反向传播 loss.backward() # 更新参数 optimizer.step() # 梯度归零 optimizer.zero_grad() # 每100个batch得到一个输出 if batch % 100 == 0: loss, current = loss.item(), (batch + 1) * len(X) print(f\u0026#39;loss: {loss:\u0026gt;7f} [{current:\u0026gt;5d} / {size:\u0026gt;5d}]\u0026#39;) 测试阶段 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def test(dataloader, model, loss_fn): # 获取总的样本个数 size = len(dataloader.dataset) # 获取批次数 num_batches = len(dataloader) # 开启评估模式 model.eval() test_loss, correct = 0, 0 with torch.no_grad(): for X, y in dataloader: X, y = X.to(device), y.to(device) pred = model(X) # 获取验证集上的损失 test_loss += loss_fn(pred, y).item() # 正确的样本率 # argmax(1) 与 真值是否相等 correct += (pred.argmax(dim=1) == y).sum().item() # 获取测试集上的损失值 test_loss /= num_batches ","date":"2025-01-03T16:29:50+08:00","permalink":"https://XiaoPeng0x3.github.io/p/quick_start/","title":"Quick_start"},{"content":"学习率 学习率是一个很重要的参数，而且学习率决定了网络能否快速的收敛并趋于稳定。目前为止，我们接触到的网络实际上都是一个优化问题，即如何找到损失函数的极小值。对于多元函数使用的就是梯度下降法去找到极小值，其中又有很多梯度下降的优化版本，例如sgd + momentum、adam等方法，这些方法里面都要用到学习率这个参数\n老师给出了许多学习率的选择方法\nstep 学习率 step学习率的思路就是每经过给定的几个epochs，就重新设置学习率。可以这样做是因为在学习初期，即使学习率很大也没有关系，大的学习率反而可以减少训练时间，当经过几轮epoch后，把学习率降低可以减少模型的震荡，从而提高精度。\ncos 学习率 在训练过程中，学习率的参数变化是cos型的\n线性下降学习率 学习率的选择是线性的\n平方根学习率 这个图片是反平方根的函数曲线\u0026hellip;.\n超参数选择 选择超参数（hyperparameter）是深度学习和机器学习中关键的步骤，影响模型的性能和训练效率。\n1. 检查初始损失 (Check initial loss)\n目的：确认模型和数据管道是否正确配置。 细节 使用默认或初始超参数（如随机初始化权重，标准学习率）。 检查初始损失是否异常高或为NaN。 如果损失值异常，可能是数据预处理或模型设置的问题。 2. 在小样本上过拟合 (Overfit a small sample)\n目的：验证模型是否有能力拟合数据（模型复杂度是否足够）。 细节 使用数据集中的一小部分样本（例如5-10个）。 调整模型直到它能够完全拟合这组数据，损失降到接近零。 如果无法过拟合，检查模型架构或超参数（如学习率、网络深度等）。 3. 找到能使损失下降的学习率 (Find LR that makes loss go down)\n目的：找到一个合适的学习率，使损失能稳步下降。 细节 采用 learning rate finder 技术。 逐步增加学习率，绘制损失随学习率变化的曲线。 选择损失开始明显下降但未发生震荡的学习率（通常在曲线的下降初期）。 4. 粗略网格搜索，训练1-5个epoch (Coarse grid, train for ~1-5 epochs)\n目的：快速筛选出表现较好的超参数范围。 细节 在关键超参数（如学习率、权重衰减、batch size）上进行粗粒度的网格搜索。 每次试验只训练1到5个epoch，足够观察趋势但不过多浪费计算资源。 排除性能较差的参数组合。 5. 精细网格搜索，延长训练时间 (Refine grid, train longer)\n目的：进一步优化超参数，找到最佳的参数组合。\n细节\n缩小关键超参数的搜索范围，进行更精细的网格搜索。 增加训练epoch（例如10到50个）以评估长期性能。 检查模型在验证集上的表现以避免过拟合。 6. 检查学习曲线 (Look at learning curves)\n目的：通过学习曲线分析模型的训练动态。\n细节\n学习曲线显示训练和验证损失或精度随时间变化的趋势。\n常见问题及解决办法\n如果验证损失明显高于训练损失：模型可能过拟合，需要正则化或增加数据。 如果两者都较高：可能是学习率太低或模型复杂度不足。 如果训练损失震荡：可能是学习率过高或模型过于复杂。 这也是老师在课上提到的这几点，为了方便查看训练的进展，一个很好的习惯是把模型在测试集上的精确度和验证集上的精确度给可视化，老师这里提到了一些有用的建议。\nloss一开始不降低\n你的初始化很bad!\nloss很高而且随着训练的进行不发生变化\n学习率太大了，试试学习率衰减的办法吧！\n衰减的太早了，导致不能快速下降\n准确率依旧在上升，应该训练更长的时间\n这种情况也是很常见的，即数据在训练集上精度很高，但是在测试集上精度却不增高，这是因为发生了过拟合，导致模型只能很好的识别出“见到”过的数据\n训练集与验证集之间精度在训练过程中相差很小，假设数据来源可靠的话，那么说明模型还不够复杂，或者模型还是处于欠拟合的状态\n总结 在这一部分，我们只介绍了学习率以及超参数选择的一些技巧，后面的内容与迁移学习以后再做总结！\n","date":"2024-12-07T12:30:58+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Cpart2/","title":"训练神经网络part2"},{"content":"前言 神经网络一个非常关键的地方就在于如何能够更快、更精确的求解出各种参数，这些参数一般是在学习的过程中可以得到，而有一些参数却需要人为的根据经验来进行初始化，例如学习率的大小、每次训练时batch size的大小、损失函数的选择以及激活函数的选择。下面来记录一下如何选择这些参数\n激活函数 激活函数在神经网络起到的是引入非线性的作用，当我们不选择激活函数的时候，实际上并没有增加有效层的层数，而激活函数又有很多种选择，早期的激活函数有sigmoid、tanh函数，而我们用的较多的有ReLu以及ReLu的变体\n类sigmoid函数 sigmoid函数在早期十分受欢迎，函数的值域在[0,1]中，这个函数在早期受欢迎的原因是它很好的模拟了神经元接受刺激产生冲动的一个过程，但是在实践中它有着很多的缺点\n饱和区 当x的取值变得很大或者很小时，其值趋近于1或者0，从图中可以近似估计一下，可以看到输出$\\sigma(x)$对于x的梯度近似为0，那么在使用反向传播链式求导时，很容易将上游梯度的结果变为一个非常小的值，也就是所谓的kill gradients，导致梯度不能通过反向传播而传递至前一层。\nNot zero centered 从图中看出，sigmoid函数不是关于原点分布的，这就会导致在计算参数W的梯度依赖于上一层的梯度，例如，给定两个参数W1和W2，对于输出来说 $$ f = X_1W_1 + X_2W_2 $$ 可以使用链式法则求得这两个参数的梯度 $$ \\frac{dL}{dW_i} = \\frac{dL}{df} \\frac{df}{dW_i} $$ 也就是 $$ \\frac{dL}{dW_i} = \\frac{dL}{df} X_i $$ 而因为$X_i$恒为正(来自sigmoid的输出)，所以$\\frac{dL}{dW_i}$的符号只取决于上游梯度，那么对于$W_1和W_2$这两个梯度来说，其符号要么同时为正，要么同时为负，所以从图上看就是这样的\n这就使得网络很难训练\n计算代价高 显然，指数级别的计算代价要明显高于一般运算\ntanh 这里也介绍了tanh函数，从图像上来说，与sigmoid相比，tanh只是少了not-zero-centered这个缺点\n类ReLu ReLu的激活函数是 $$ ReLu(x) = max(0, x) $$ 选择这个激活函数也是AlexNet一个创新点之一，与之前的类sigmoid函数相比，ReLu简单、收敛快、不存在饱和梯度，但是这个函数也是not zero centered，而且也有着其它的缺点\ndead ReLu 当输入X小于0时，ReLu只是简单的去把X设置为0，导致小于0的部分的梯度永远也不会去更新，当输入 $x≤0$，输出总是 0。因此，如果一个神经元的输入权重和偏置的组合导致它始终进入负区,该神经元在整个训练过程中都不会被激活，也不会对学习产生贡献。也就是该神经元是dead的，并不会增强模型的能力。\n为解决这个问题，研究人员又提出了ReLu的变体\nLeaky ReLu Leaky ReLu的思路是小于0的部分不是简单的设置为0，而是设置为一个很小很小的数\n把设置的这个很小的数叫做$\\alpha$，在反向传播中可以学习这个参数\n还有其它的各种变种\n总结 规则怪谈：\n当你不知道使用什么激活函数，或者真的不在乎那0.1%的精度提升，直接选择ReLu 当你需要极值的优化，那么可以尝试一下ReLu的变体 不要使用tanh和sigmoid 数据处理 对数据进行预处理可以更好的训练网络。\n下图是一个对原始数据进行0-1正态分布的处理过程\n这里有一个值得注意的代码细节\n1 2 X -= np.mean(X, axis=0) X /= np.std(X, axis=0) 这里对数据进行处理都是在同一类的数据进行处理，而在数据集里面按照一般约定，每一列是一个类的所有数据分布，所以这里是在axis=0(torch里面使用的是dim)上进行数据处理。\n对数据进行预处理可以使得在反向传播时更容易求解梯度和传播梯度\n对于图像来说，可以减去图片的均值、减去每个通道上的均值以及在每个通道上做正态分布初始化\n权重参数初始化 到目前为止，权重参数W一直是一个非常重要的参数，而且权重的初始化也是训练网络很重要的一部分，一个想法是，假设我们有着一个比较简单的网络，如果我们把权重参数全部初始化为0\n1 W = torch.zeros(N, D) 那么在前向传播的过程中，每一层的输入就都是0，那么这个网络实际上什么都做不了，一个比较常见的做法是把W按照高斯分布进行初始化\n1 W = torch.randn(N, D) * weight_scale 例如，weight_scale可以初始化为0.01\n1 W = torch.randn(N, D) * 0.01 这样初始化在网络层数比较小的时候没什么问题，但当网络层数非常多时，后面层获得的输入就会非常非常小，以至于无法表示\nXavier 初始化 对于激活函数是tanh时，xavier激活函数可以很好的结果这个问题，这个方法的核心思想在于把输入和输出的分布尽可能相似，也就是输入的方差与输出的方差一致。\n对于输出来说 $$ y_i = \\sum_{i = 0} ^ D X_iW_i $$ 为了使两者方差相等，即 $$ Var(y_i) = \\sum_{i=0} ^ D Var(X_iW_i) $$ 因为输入X的方差是1，即 $$ Var(y_i) = D*Var(W_i) $$ 所以$W_i$的方差就是原来的$\\frac{1}{D}$,只需要在原来初始化时除以输入维度数即可\n1 W = torch.randn(Din, Dout) / torch.sqrt(D) # 注意这里是方差 kaiming初始化 也叫做He初始化，这个初始化方法是专门为ReLu实现的，回想一下ReLu函数，在ReLu函数作用下，对于0-1分布来说，每次产生非0的概率就是0.5,所以对于这组数据来说，每次方差都要缩小一半。\n同样的，为了使输入和输出的分布近似相等，所以可以推导出在ReLu函数作用下的初始为\n1 W = torch.randn(Din, Dout) / torch.sqrt(2 / Din) 同样的，对于ResNet来说，整个初始化就是这样的\n这样可以保证在两个卷积层输出后方差不变。\n正则化 正则化技术是防止模型过拟合的一个关键技术，正则化可以从某种程度上减少模型的复杂度。\n在一开始，对于损失函数，我们讨论了L1正则化和L2正则化这两种简单有效的正则化方法\n此外老师还介绍了一种正则化方法DropOut,DropOut一般用于全连接层的优化，对于一些神经元的输出，DropOut会按照P的概率把这些神经元的输出置为0，其结果就像是在复杂的网络中选择一些简单的子网络\n一样，从而降低模型的复杂度。\n为了保证这两个模型依旧是等价的，我们把未丢弃的那些值都除以p，这样可以保证在DropOut前后两者均值相同。\n数据增广 数据增广的想法可能是更好的去模拟人类的思维，对于一张图片来说，我们可以对这张图片进行裁剪、旋转、增强亮度等操作，对于人类来说，即使经过这些操作，也还是很容易就可以辨别出这是同一张图片，这恰恰也是我们对机器也可以实现的能力。\n除此之外，在数据集较小、数据集图片质量不佳时，我们就可以人为的对数据进行一些操作，从而达到训练要求。\n总结 这次老师分享了一些训练网络时的一些技巧，包括激活函数的选择、数据预处理的重要性、权重参数初始化的方法、正则化以及数据增广的办法，这些都会在作业中用到！\n","date":"2024-12-07T12:30:50+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Cpart1/","title":"训练神经网络part1"},{"content":"前言 全连接神经网络面临的问题 在此之前，我们一直是在全连接层神经网络进行讨论，全连接神经网络其实也有许多不便之处\n无法理解图像模板\n对于之前的所有的有关图像分类实现的任务，对于给定的图片(size:3x32x32)，我们并不考虑图片整体或者图片的一些局部特征是什么样的，而是直接把3x32x32的图片展平为一个一维向量(1x3072)，然后经过一些矩阵乘法，我们就可以得到这个图片的scores\n内存问题\n3072维的向量似乎还是可以接受的，但是实际上这个图片是非常小的，假设我们使用了一些比较大的图片，那么输入的维度肯定会显著的提升。在进行神经网络全连接后，显然，每一层上都有着巨量的计算，而且为了有更好更复杂的模型，神经网络的层数也会增加，那么在此产生的计算和内存消耗将是巨大的\n卷积神经网络 卷积层 卷积神经网络的思路与全连接不同，卷积神经网络更加“尊重”图像的样子，它不会把输入图像进行压缩，而是在乎图像的整体特征，而保留整体特征的一个关键部分就是卷积核\n卷积核 卷积核可以保留提取一些图片中相似的特征，只要你了解卷积核是怎么提取图片的特征的，你就会明白为什么\n卷积核的计算\n卷积核会在原来输入图片的维度上进行计算，假设我们使用的卷积核是3x5x5的\n对于图像中每个5x5的部分，我们使用点积去进行计算\n多个卷积核的使用\n在经过卷积核的计算后，我们得到了一个1x28x28的输出(计算公式后文给出)，更一般的，我们会使用多个卷积核进行卷积，此时卷积核的维度就是Nx3x5x5，那么输出就会是Nx28x28的\n更一般的，对于一批数据，我们会对着一批数据进行卷积操作，此时就变为\n下面是其公式表达形式\n卷积层的堆叠\n为了搭建更加复杂的网络，通常会堆积多个卷积层(像之前两层网络一样，在一定范围内层数越多越复杂，能力越高)，此时注意，为了确保引入多个卷积层而不是只是一个卷积层，这里也引入了非线性激活函数(ReLu)\n输出大小 卷积的过程就是在原始图片上进行滑动相乘的结果，当使用5x5的卷积核时，输入图片大小为32x32，那么在输入的长上的最后一次运算就是第28, 29, 30, 31 ,32的格子上；同样对于宽也是一样的，这样就可以得到输出图像的特征就是28x28\n更一般的，对于输入特征W来说，我们使用大小为K的卷积核进行运算，那么输出特征就是**W-K+1**的，利用这个公式，我们就可以很方便的计算输入特征为32x32时，卷积核大小为5x5时的输出特征。\n带来的新问题 经过5x5的卷积核运算后，一个肉眼可见的差别就是输出图像与原始输入图像维数是不同的，当我们使用很多个卷积核后，输出的图像看起来像是降维一样。\n为了保持输出图像与输入图像的维度(可能是减少损失的信息)，我们可以使用填充输入图像的方式来保证维度。\n需要填充0的层数p就是(k-1)/2，经过填充，输出的图像大小与输入就有着相同的维度。\n不同的步长 在上面卷积核进行计算的时候，我们都是默认每次卷积的时候都是挨个滑动，事实上滑动的时候也可以跳跃着滑动，也就是说，在原先的基础上，如果每次都是N步去滑动卷积，那么输出的维度就会减小N倍，这个时候我们可以更新我们的计算公式\n池化层 池化分为平均池化层和最大池化层，例如，当使用大小为2x2的卷积核进行最大池化的时候，实际上就是对卷积后的每层输出进行特征筛选\n上图是一个使用最大层池化，步长为2的池化层，因为步长的原因，池化后的图片大小会发生变化，使用平均池化也是一样的原理，只需要计算2x2内的均值即可\n前向传播 对于给定的数据，怎么实现前线传播的计算呢？假设下面是给定的数据\nx, 大小为NxCxHxW，N代表这一批的数据，C表示图片的通道数，HxW是图片的大小 w，大小为FxCxHHxWW，有F个卷积核，每个卷积核大小都是CxHHxWW b，大小为哦F，表示偏执 现在思考怎么计算经过卷积后的数据out\n卷积的过程 首先，我们可以拿出一张图片来进行举例，那么这张图片就是x[i, :, :, :],同时，我们也拿出一个卷积核w[i, :, :, :]。回想一下老师在课堂上举过的例子\n这个输出是怎么得到的呢？因为图片是三通道的，所以在每一个通道上都要进行卷积操作，实际上得到的最后输出就是三通道上的总和，那么，对于图片来说，其计算过程就是这样的\n1 2 3 4 5 6 7 Hout, Wout = out.shape # 得到输出的size # 假设步长是stride for i in range(Hout): for j in range(Wout): # 这里假设输出和x有着同样size # 三个通道的总和 output[i, j] = (x[:, i:i+k, j:j+k] * w).sum() + b 更一般的，当我们可以进行推广\n1 2 3 4 5 6 7 8 9 10 11 # 输出图片的维度与步长stride和输入的填充有关 N,C,H,W = x.shape F,C,HH,WW = w.shape H_out = 1 + (H + 2 * pad - HH) // stride W_out = 1 + (W + 2 * pad - WW) // stride for n in range(N): for f in range(F): for i in range(H_out): for j in range(W_out): # 计算第n个数据在第f个卷积核上的输出 out[n, f, i, j] = (x[n, :, i, j] * w[f]).sum() + b[f] # 三通道相乘的和 反向传播求梯度 反向传播求梯度这里其实有点绕，假设我们已经知道损失函数对于输出的梯度dout，那么我们就可以使用链式法则进行求导\nout[0, 0, :, :]代表着第1个图片在第一个卷积核上的输出，我们来分析一下它是由哪些部分计算得到的 $$ out[0, 0, :, :] = x[0, 0, :, :] * w[0, 0, :, :] + x[0, 1, :, :] * w[0, 1, :, :] + x[0, 2, :, :] * w[0, 2, :, :] + bias[0] $$ 现在把卷积核的个数拓展到F个，那么求第f个卷积核的输出就是 $$ out[0, f, :, :] = x[0, 0, :, :] * w[f, 0, :, :] + x[0, 1, :, :] * w[f, 1, :, :] + x[0, 2, :, :] * w[f, 2, :, :] + bias[f] $$ 转换为代码就是\n1 out[0, f, :, :] = np.sum(x[0, :, :, :] * w[f, :, :, :], axis=0) + bias[f] 需要注意的是，要考虑的是，我们需要学习的参数在哪里参与运算，分层求解其梯度即可。\n正则化技术 当网络变得很深的时候，网络通常会变得难以训练，这是因为在我们依赖的方法上的缺点，对于一个很深的网络来说，当前层的梯度来自与损失函数对参数的梯度，而梯度在流动的时候又是依赖于链式法则和反向传播，因此，假设上游梯度：\n所有的梯度都是小于1的数\n那么在反向传播相乘的时候，很有可能出现梯度消失，即数值过小(nan)\n当梯度全部都是大于1的数\n那么在反向传播的过程中，多个大于1的数也会导致数值过大而溢出\n正则化的作用 why it works?，正则化就是调整数据之间的分布，例如，假设 $$ Y = X_1W_1 + X_2W_2 $$ 而不幸的是，X1可能是一些房屋面积的数据，例如100平米，150平米，而X2有可能是附近的医院个数，例如10,15，显然，两者数据差别有点大，我们可以假设损失函数对于各个数据之间的梯度图，例如\n其中X1是数值比较大的数，X2是数值比较小的数，中间的星号就是损失函数最小时X1和X2的取值，可以看到，整个图像的分布就像是一个椭圆形数据，对于X1来说，每次的变化值都要大于X2的变化值，这就导致了每次在梯度下降的时候，很有可能导致下降时候的振荡和无法收敛。\n使用正则化之后，可以把数据进行归一化操作，这样对于整个数据分布来说，其形状会更加接近于圆型，这样在梯度下降的时候，对于一些参数learning_rate就可以调整的比较大，同时也可以加快模型的收敛，减少训练时间。\n同时，这里的正则化技术有很多，例如batch normalization, layer normalization, xxx normalization，感兴趣的可以自行查阅\n批正则化处理的技术一般放在全连接层之后或者卷积层之后，而且是放在非线性激活函数之前\n总结 卷积神经网络的组成一般包括：卷积层、池化层和全连接层，其中每个层里面又有各种各样的细节，下次来看一看怎么更好的训练一个网络\n","date":"2024-11-25T17:12:53+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","title":"卷积神经网络"},{"content":"前言 这个问题似乎是之前没有接触到过的问题，题目的大概意思就是，给定一个数组，数组中可能会有重复的元素，现在我们的任务是\n调整元素，使得区间内没有重复的元素 调整后的区间和最大 贪心算法 例如， 一个数组是\narr = [1,2,3,4,5,5,6]\n怎样调整使得数组和最大且元素不重复呢？\n贪心的思路 从贪心思路出发， 只要每个元素都是最大的， 那么整体和就是最大的，所以，我们不去调整最大值，而是保留下来数组里面的最大值\n解题 先将数组进行排序\n1 arr.sort(reverse= True) 然后开始考虑挨个调整\n将当前元素与该元素前面一个元素进行比较\n因为有重复的元素，所以要考虑最大值有重复的情况\n例如\n1 arr = [6,6,6,5] arr[i]来说， 为了调整最大， 那么我们就需要将arr[i]和arr[i-1]-1做比较， 取这两个值之间的最小值\n如果与arr[i] = arr[i-1], 那么我们就可以把arr[i]调整为在贪心策略下的最大值 如果不相等，因为我们是经过排序的， 所以这个值的本身就是可以取到的最大值 例题 LeetcCode945.使数组唯一的最小增量\n给你一个整数数组 nums 。每次 move 操作将会选择任意一个满足 0 \u0026lt;= i \u0026lt; nums.length 的下标 i，并将 nums[i] 递增 1。\n返回使 nums 中的每个值都变成唯一的所需要的最少操作次数。\nLeetcode 1647. 字符频次唯一的最小删除次数\n如果字符串 s 中 不存在 两个不同字符 频次 相同的情况，就称 s 是 优质字符串 。\n给你一个字符串 s，返回使 s 成为 优质字符串 需要删除的 最小 字符数。\n字符串中字符的 频次 是该字符在字符串中的出现次数。例如，在字符串 \u0026quot;aab\u0026quot; 中，'a' 的频次是 2，而 'b' 的频次是 1 。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class Solution: def minDeletions(self, s: str) -\u0026gt; int: # 从大到小排序 # arr[i-1] = min(arr[i-1], arr[i]-1) temp = [0] * 26 for c in s: temp[ord(c)-ord(\u0026#39;a\u0026#39;)] += 1 # 排序 temp.sort(reverse=True) ans = 0 for i in range(1, len(temp)): a = temp[i] temp[i] = min(temp[i-1]-1, temp[i]) if temp[i] \u0026lt;= 0: temp[i] = 0 ans += a - temp[i] return ans ","date":"2024-11-24T17:24:18+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E5%8C%BA%E9%97%B4%E5%92%8C%E6%9C%80%E5%A4%A7%E4%B8%94%E5%85%83%E7%B4%A0%E4%B8%8D%E9%87%8D%E5%A4%8D/","title":"区间和最大且元素不重复"},{"content":"前言 前面讲解了一些优化算法，尤其是各种梯度下降算法，这次来看一看神经网络\n神经网络 神经网络的特点 前面我们在学习线性分类器的时候了解到，线性分类器对于异或、圆形、半圆形数据不能很好的划分出一条边界，这也导致了线性分类器不是那么的有效，而神经网络可以解决这个问题。\n一般的，神经网络可以划分为输入层、隐藏层、输出层这三种结构，而正是隐藏层的一些非线性特征使得神经网络可以拟合出各种决策边界，所以在线性分类器上解决不了的问题便可以使用神经网络很好的解决。\n为了简单起见，作业里面实现的是一个两层的神经网络，使用的激活函数是Relu激活函数\n得分方式的改变 在之前的线性分类中，我们把 $ X^T W $看作是一个得分的输出。在神经网络里面这里的计算方式也与其计算方式相同，不同的是，在多层神经网络之间传递上一层的分数时，总是要经过非线性激活函数输出后把分数传递到下一层，这是因为如果不加激活函数，那么实际上我们在做乘法的时候还是取得是一个线性计算的过程，所以要加上激活函数，从而引入非线性。\n全连接神经网络也叫做多层感知机\n因此，计算得分的方式可能会是\n1 2 3 4 5 6 7 import numpy as np h1 = X.dot(W1) # 得到第一层的分数 # 执行Relu, 即 h1 = max(0, h1), 只保留大于0的部分 h1[h1 \u0026lt; 0] = 0 # 经过激活函数后输出到下一层 scores = h1.dot(W2) # 得到分数 上文已提到，不加激活函数实际上做的还是线性变换\n可以看到，经过合并后，不加激活函数的结果等价于一个线性分类\n激活函数 激活函数的存在就是为了引入非线性，从而可以划分出非线性的决策边界，下面是一些激活函数\n简单的实现 ppt中给出了一个简单的使用MSE作为损失函数的两层神经网络\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 import numpy as np # N是训练的batch size; D_in 是input输入数据的维度; # H是隐藏层的节点数; D_out 输出的维度，即输出节点数. N, D_in, H, D_out = 64, 1000, 100, 10 # 创建输入、输出数据 x = np.random.randn(N, D_in) #（64，1000） y = np.random.randn(N, D_out) #（64，10）可以看成是一个10分类问题 # 权值初始化 w1 = np.random.randn(D_in, H) #(1000,100),即输入层到隐藏层的权重 w2 = np.random.randn(H, D_out) #(100,10),即隐藏层到输出层的权重 learning_rate = 1e-6 #学习率 for t in range(500): # 第一步：数据的前向传播，计算预测值p_pred h = x.dot(w1) h_relu = np.maximum(h, 0) y_pred = h_relu.dot(w2) # 第二步：计算计算预测值p_pred与真实值的误差 loss = np.square(y_pred - y).sum() print(t, loss) # 第三步：反向传播误差，更新两个权值矩阵 grad_y_pred = 2.0 * (y_pred - y) grad_w2 = h_relu.T.dot(grad_y_pred) grad_h_relu = grad_y_pred.dot(w2.T) grad_h = grad_h_relu.copy() grad_h[h \u0026lt; 0] = 0 grad_w1 = x.T.dot(grad_h) # 梯度下降法 w1 -= learning_rate * grad_w1 w2 -= learning_rate * grad_w2 这里比较有意思的地方是如何去更新我们的权重矩阵W1,W2\n反向传播 求得W1和W2得梯度便可以使用梯度下降法去进行跟新，那么怎么求这两个函数得梯度呢，答案就是去使用反向传播算法。\n反向传播算法的核心就是去利用链式求导法则，对于两层或者更多层的神经网络来说，直接求得损失函数对于权重的梯度是一件不太好实现的事情，实际上ppt里面讲解的就是链式求导法则，为了更好的理解链式求导，这里以损失函数为交叉熵函数实现的多分类问题来进行记录。\n链式求导 上面构建了一个简单的二层网络，这个网络的工作流程是这样的\n计算得分\n与之前的线性网络一致，对于输入$X$来说，输出的得分就是 $$ scores = XW_1 + b_1 $$ 不同的是，为了拟合出更多的非线性边界，这里的得分还需要向第二层输出\n激活函数引入非线性\n假设我们的激活函数为$ReLu$函数，那么 $$ Z(x) = \\left{ \\begin{aligned} x , x \u0026gt;= 0\\ 0, else \\end{aligned} \\right. $$ 也就是隐藏层h1的输出就是$Z(scores)$\n经过隐藏层输输入后，我们可以把计算第二层的结果看作之前的线性分类器 即 $$ output = Z(scores)W_2 + b_2 $$ 得到这个output后，可以把结果转为softmax，也就是 $$ y_{pred} = argmax[softmax(output)] $$ 这样就可以使用交叉熵损失函数计算损失\n梯度求解 需要额外注意的是，W的梯度dW是在损失函数中学习到的，我们更新W的意义就是去最小化损失函数，最小化损失函数也就是意味着我们的预测越准确，模型所产生的误差越小。\n对于一个单层或者多层网络来说，其输入输出、求导方式都是很相似的，下面是一般求解步骤\n求得损失函数对输出的梯度dout\n在常见的一些损失函数如MSE均值、softmax交叉熵等，可以求得其关于输出的导数，即求得$\\frac{dL}{dout}$\n求得输出关于输入的梯度\n对于输出来说，一层网络的输出就是 $$ output = XW + b $$ 所以，对于，根据链式求导法则，我们就可以很容易的求出损失函数关于输入的梯度\n在使用激活函数后，即output其实并不是原始的输出，而是经过激活函数处理后的输出，这也就意味着中间又多了一层关于激活函数的导数，我们以ReLu激活函数为例\n一般的，如果不加激活函数，那么我们的求导过程可能是这样的\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import torch # 假设我们已经知道损失函数关于输出的梯度 def backward(dout): \u0026#39;\u0026#39;\u0026#39; Inputs: - dout: Upstream derivative, of shape (N, M) - x: Input data, of shape (N, D) - w: Weights, of shape (D, M) - b: Biases, of shape (M,) \u0026#39;\u0026#39;\u0026#39; # 根据求导公式 dX = dout.mm(W.T) dW = x.T.mm(dout) db = dout.sum(dim = 0) return dX, dW, db 如果在输出层多加了激活函数，那么只需要再多计算一次乘积即可\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import torch def backward(dout): \u0026#39;\u0026#39;\u0026#39; Inputs: - dout: Upstream derivative, of shape (N, M) - x: Input data, of shape (N, D) - w: Weights, of shape (D, M) - b: Biases, of shape (M,) \u0026#39;\u0026#39;\u0026#39; # 计算dW的梯度 dW = x.T.mm(dout) # 注意，是由输出大于0的部分才有梯度，所以需要进行保留 dW[out \u0026lt; 0] = 0 更一般的，我们会直接对ReLu(x)做求导，从而当输入x发生变化时，我们的ReLu依旧会更加模块化\n作业 two_layer_net 讲解一下这个作业中较难的部分\n实现forward_pass 可以从函数的参数里面得到需要的参数， 例如W1, b1, W2, b2\n1 2 3 4 # Unpack variables from the params dictionary W1, b1 = params[\u0026#39;W1\u0026#39;], params[\u0026#39;b1\u0026#39;] W2, b2 = params[\u0026#39;W2\u0026#39;], params[\u0026#39;b2\u0026#39;] N, D = X.shape 需要额外注意的是这些参数的形状， 我们的训练数据X是NxD的，也就是说，这个训练集中有N个样本，每个样本都是简单的1xD向量，作业为了防止我们出错，还贴心的在注释里面给出了这些参数的形状\n1 2 3 4 5 6 7 8 \u0026#39;\u0026#39;\u0026#39; It should have following keys with shape W1: First layer weights; has shape (D, H) b1: First layer biases; has shape (H,) W2: Second layer weights; has shape (H, C) b2: Second layer biases; has shape (C,) - X: Input data of shape (N, D). Each X[i] is a training sample. \u0026#39;\u0026#39;\u0026#39; 根据这个注释，我们在做矩阵乘法的时候就特别方便\n1 2 3 4 # 第一层的输出 hidden = X.mm(W1) + b1 # 经过非线性激活函数 hidden[hidden \u0026lt; 0] = 0 此时，我们就得到了这个二层网络的隐藏层分数\n因此，计算输出的总分也是很简单\n1 2 # 未经softmax函数处理 scores = hidden.mm(W2) + b2 # raw_scores 到现在，我们就得到了网络的输出分数，现在让我们来梳理一下从图片到预测之间的流程\n3x32x32数据集\n我们把原始数据集展平为一个一维向量，把若干个这样的向量堆叠在一起，这样就得到了训练集X\n计算隐藏层输出\n与线性分类器计算分数一样，做乘法运算即可\n激活函数\n引入非线性，如ReLu, Sigmoid函数\n输出层\n得到隐藏层分数后计算输出层分数即可\nsoftmax得到概率\n我们把输出的scores经过softmax后得到近似概率分布，然后概率最高的就是我们网络将图片分类的结果\n交叉熵损失函数优化\n使用交叉熵函数优化，从而得到之前的W1, b1, W2, b2的梯度，并使用梯度下降法进行学习\n也就是说， 在forward_pass中，我们还剩最后两个步骤没有计算出来，下面我们将在nn_forward_backward中计算得出\nforward_backward 要想得到损失函数关于W1, b1, W2, b2的梯度， 我们得先求的损失函数，这里使用的是交叉熵损失函数，也就是说，我们需要求得softmax后的分数\nsoftmax过程\n这部分在A1中已经计算过，在这里在此计算一次。首先根据定义，其实就是每部分exp后除以总的exp和即可。我们的输出scores是一个NxC的矩阵，每一行(dim=1)的含义就是第i个样本(1\u0026lt;=i \u0026lt;=N)在10个类上的总分。例如，假如第i个样本在10个类中cat的分数最大，那么经过softmax后可以近似认为第i个样本是cat的概率最大\n1 2 3 4 5 6 7 8 9 10 11 12 # 从前向传播中得到分数,注意，这个分数其实是raw_scores scores, h1 = nn_forward_pass(params, X) # 得到分数后softmax化 # 得到每个类别的最大值 max_val, _ = torch.max(scores, dim=1) # 函数返回最大值和最大值的索引 # 除去最大值是防止exp值过大，同时不影响结果 scores_remove_max = scores - max_val.view(-1, 1) # 使用广播机制，不使用也可以 # scores_remove_max = scores - torch.max(scores, dim=1, keepdim=True).values # exp化 scores_exp = torch.exp(scores_remove_max) # 概率化 scores_prob = scores_exp / torch.sum(scores_exp, dim=1).view(-1, 1) # 不使用广播机制同上 链式法则\ndW2和db2\n在求得softmax化后的结果后，我们需要以损失函数的形式表达出来整个解，这里的损失函数是交叉熵损失函数，为了求得损失函数对W2的梯度，使用链式法则会更加简单清晰\n交叉熵损失 $$ Loss= -\\frac{1}{N}∑log(p_i)+reg⋅(∥W1∥^2+∥W2∥^2) $$ 这里的pi是预测值，也就是我们上面的softmax值，现在，我们可以把求解过程转换一下,即\n$$ \\frac{dL}{dW_2} = \\frac{dL}{dP} \\frac{dP}{dS} \\frac{dS}{dW_2} $$\n我们可以来挖掘一下Scores与W2的关系，显然有\n$$ Scores = h_1^T * W_2 + b_2 $$\n怎么求第一项的梯度呢？\n$ \\frac{dL}{dP} $的计算公式其实就是对数函数求导，而$\\frac{dP}{dS}$的结果就要从softmax公式出发\n$$ softmax(i) = \\frac{e^{scores_i}}{e^{scores}} $$\n这个时候就要分当前预测类的类别的情况了，因为对于$p_i$来说，每次都要计算两部分梯度，当计算类别正确时，也就是softmax公式的分子上是含有$e^y_i$，那么此时分子分母都是含有要求导部分；当求其它梯度时，分子上其实就是个常数，求导法则发生了变化。这里推荐一个视频,可能会帮助更好的理解。\n也就是说，对于这部分梯度来说，正确的类别结果-1(正确类别分子上还有求导到部分)，错误类别不需要-1，，而且对这部分求导是因为分母上有需要求导部分。\n而且，每个标签都是One-Hot格式，这样我们就可以求得$\\frac{dP}{dS}$\n所以求得$ \\frac{dL}{dS}$\n1 2 3 ds = scores_prob.clone() # NxC ds = ds[range(N), y] -= 1 ds /= N # 注意不要遗漏 $\\frac{dS}{dW}$ = h1(NxH)\n1 dW2 = h1.T.mm(ds) # HxC 同理也可以求得db2就是 ds\ndW1和db1\n这里同样使用的是链式法则\n$$ h1 = ReLu(XW_1+b_1) \\ Scores = h_1W_2 + b_2 $$\n所以要求 $$ \\frac{dL}{dW_1} = \\frac{dL}{dS} \\frac{dS}{dh1} \\frac{dh1}{dW_1} $$ 现在未知参数就是dh1,需要注意的是，因为是ReLu所以小于0的部分会置0\n1 2 dh1 = d_scores.mm(W2.T) dh1[h1 \u0026lt;= 0] = 0 # 小于等于0的不贡献梯度 这里不清晰的化还可以再加一部分即\n$$ \\frac{dh1}{dW_1} = X , h1 \u0026gt;= 0 $$\n现在，链式求导的部分我们就求解完了，也是这次作业最难的一部分。\n总结 多层感知机成功解决了线性分类不能完成的任务，但是多层感知机也有自身上的缺点，下节来看看大名鼎鼎鼎鼎大名的卷积神经网络！\n","date":"2024-11-23T16:48:55+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/","title":"神经网络——多层感知机"},{"content":"前言 继续来看看优化这部分\n梯度下降 优化部分主要讲解了与梯度下降以及梯度下降的各种优化版本\n虾几霸优化 对于评估一个W参数矩阵来说，需要计算出在这个W下的分类准确率即可。这里的”虾几把“的意思就是随机生成一个参数矩阵W，只要这个矩阵的准确率高于上一次计算的准确率，那么就把当前最优的W更新，然后一直模拟下去,一个可能的算法是这样的\n经过这种方法去求得的W在准确率大约在15%，不算太坏，但算不上好！\n梯度下降法 在一元函数中，导数可以理解为在这一点上的斜率，在多元函数中，我们使用梯度这个概念来进行导数的推广，实际上，梯度在每一维上的分量就是我们熟悉的导数\n沿着负梯度的方向就是目标函数下降最快的方向\n因此，对于损失函数来说，我们可以找到W的梯度矩阵dW，然后再对W进行优化,这种方法就是大名鼎鼎的梯度下降\n可以看到，这里我们就有了三个未决的超参数\n怎样初始化W 要迭代寻找多少次(num_steps) 学习率learning_rate 其中非常关键的一个参数就是learning_rate，因为最小化损失函数实际上就是去找到目标函数的极小值，在刚开始进行梯度下降时，初始位置在极小值的左边或者右边。下面用$ f(x) = sin(x) $来模拟一下整个过程\n当学习率很小时\n我们总能找到极值， 但是却要寻找很长时间，这是因为每一步都走的特别小，所以寻找要很长的时间，这里假设学习率是0.1，迭代100次\n当学习率很大时\n学习率很大，这就意味着每一步都走的很大，所以很容易错过最小值，从而造成振荡，下面是学习率为2的情形\n所以，这些参数的选取实际上是在训练神经网络的一些困难之处，而且我们的训练集通常很大，所以每次更换学习率后再训练的代价很大，来说一下这些优化方法！\n小批次计算 Mini Batch 在寻找学习率的时候，我们没必要在整个测试集上进行，而是去选择一批样本进行训练，其实这样做也可以减少内存的压力，一个可行的代码是\n1 2 3 4 5 6 7 import torch # 假设 X_train, y_train num_train = X_train.shape[0] # 得到样本总数 # batch size batch = 32 # 生成随机样本 idx = torch.randint(num_train, size=(batch, )) 这样，在每次训练的时候，我们就可以在小样本上进行迭代训练\n1 2 3 4 5 X_train_batch = X_train[idx] y_train_batch = y_train[idx] ############################ ... ############################ SGD+Momentum 在随机梯度中引入动量的概念，给我们的点增加一个“惯性”的特点\n可以看到， 我们的小球确实像物理中的小球那样，在不断的运动着！一个可能的代码是\n即先计算速度v，再根据速度v梯度下降\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 def f(x): return np.sin(x) def df(x): return np.cos(x) x0 = 1 # 初始化随机x v = x0 # 初始化 beta = 0.9 # 动量值 learning_rate = 0.1 # 学习率 for _ in range(50): # 迭代50次 v = beta * v + (1 - beta) * df(x0) x0 = x0 - learning_rate * v 这个动量的计算公式其实很有意思，它的前身或者本质就是指数加权平均。在使用随机梯度下降时，前面一时刻的梯度似乎不会对后面的梯度造成影响，这就导致随机梯度下降的过程是一个不断震荡的过程，而且很容易陷入局部最小值，而引入指数加权平均时，可以看到，每次梯度的更新都是取决于前面几次的平均值\n$$ v_{t+1} = \\beta * v_t + (1-\\beta)df(x) $$\n当$\\beta$取0.9时，也就是我们会取梯度的一个样本平均(假设样本为10)，这样就把之前计算过的梯度与现在联系在一起，从而避免震荡！\nNesterov Momentum 在ppt中的形式是这样的\nNesterov Momentum的改进思想在于，它在计算梯度之前，先对参数进行一个“预更新”，即朝动量方向提前迈出一步，这样梯度会变得更加准确。\nNesterov Momentum的更新公式为：\n预估下一步的位置：\n$$ \\tilde{\\theta} = \\theta_t + \\gamma v_t $$\n在预估位置上计算梯度：\n$$ v_{t+1}=γv_t−η∇f(θ~) $$\n更新参数：\n$$ θ_{t+1}=θ_t+v_{t+1} $$\n同样的，我们还可以使用这种方法去求sin(x)的极小值\n使用预估的x求梯度\n1 2 3 4 5 6 7 8 9 10 # 计算梯度下降路径 for _ in range(100): # 限定100步 # 预估位置 x_pred = x0 + beta * v # 在预估位置计算梯度 grad = df(x_pred) # 更新动量 v = beta * v - alpha * grad # 更新参数 x0 = x0 + v AdaGrad Adagrad 是一种自适应学习率的优化算法，它根据每个参数在训练过程中的历史梯度大小来调整学习率。对于稀疏特征或特征具有不同重要性的任务（如自然语言处理问题），Adagrad 具有较好的效果。\nAdagrad 的公式如下：\n更新梯度累积历史： $$ G_t=G_{t−1}+∇f(x_t)^2 $$\n这里 $G_t$是梯度平方的累计和（逐元素累加）。\n更新参数： $$ x_{t+1}=x_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\nabla f(x_t) $$\n$\\eta$ 是初始学习率。 $\\epsilon$ 是一个小值（如 $10^{-8}$），用于避免分母为零。 同样，我们来用sin(x)来模拟一下\n可以看到，在这种方法下，“小球”似乎没有它的“物理属性”！\nRMSProp RMSProp是AdaGrad引入指数衰减平均后的优化版本\n接着使用这种方法来求sin(x)的极小值\n有趣的一点是，与AdaGrad采取相同的学习率时，该方法产生了震荡，可能该方法在对学习率的初始值要求较高\nAdam Adma是RMSProp结合了Momentum的版本\n继续来寻找sin(x)的极小值点\n这是老师的经验，超参数的选择是个难点！\n总结 在了解这些优化技巧后，一个不错的建议是：优先使用Mini-batch和Adam优化。\n","date":"2024-11-15T20:20:39+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E4%BC%98%E5%8C%96%E4%B8%8E%E8%AE%AD%E7%BB%83/","title":"优化与训练"},{"content":"前言 好难好难\n线性分类器 线性模型 线性分类器在神经网络中相当于积木的低位\n你可以用很多层的线性分类器来实现一个神经网络，当然，为了提高模型拟合数据的能力，一般不会只去使用线性模型，而是会选择性的加入一些非线性模型\n从线性观点 让我们继续回到上几节课提到的CIFAR10数据集，这个数据集有10个不同的类别。而对于图像的表示，我们可以把图像(input)看作一个数字矩阵，我们想要实现的内容就是，对于一个数字矩阵，能否找到一个权重参数W,使得数据的结果发生一些变化，从而根据这个输出的结果来进行分类判断。\nWx + b是一个非常经典的线性模型\n例如，对于下面这个分类问题\n这里，我们假设输入的图像是一个简单的2x2的矩阵，也就是说，该图像仅仅只由4个特征点决定，那么，对于一个参数矩阵W来说，它的每一行可以看成是一个类别的权重，把这些元素对应相乘就可以得到该类别的\u0026quot;总分数\u0026quot;(例如，cat类在经过参数矩阵运算和bias之后的总分就是-96.8)。\n为了使表示结果更加的整洁易懂，我们可以把这个bias添加到参数矩阵W里面。\n从图像观点 这里的意思就是不把输入的数据拆分为一行，而是直接调整对应元素在图像中的数值\n给人在视觉上的观点就是给整个图像蒙上了一层模板，有着相同背景的图片更容易被划分到同一个类别中，也就意味着，每一类别好像是有了一张模板图片一样\n从几何上看 从几何上看，这些图片会被一个一个的超平面给划分切开，彼此之间没有交集\n线性模型不能解决的问题 线性模型显然不太适合取解决非线性模型。这里列出来一些线性分类器不能解决的问题。\n包括\n一三象限问题\n其实我觉得就是异或问题，对于这类问题，你无法找到一条直线来把两种颜色划分开来\n非线性问题\n对于这类数据一部分是呈现非线性的，除了非线性之外的数据是无法仅通过一条直线划分\n下面的图片很好的表明了这些例子。\n感知机不能学习异或问题！\n当然，如果一层感知机实现不了，那么就再来一层！天无绝人之路！\n损失函数 虽然线性模型很简单，但还是可以给我们许多启发。到目前为止，我们还没有给出一种可以更新权重W的一个方法，于是损失函数便登场了！\n损失函数可以理解为一个定量来分析真实值与我们的预测值之间的偏差的一个方式，按照这种方式，当我们的预测值越接近于真实值，那么我们可以认为，在这种条件下的W是loss友好的。\n另外说一点，在后续提到损失函数时，我们都更加倾向于这个损失函数是凸函数(Convex)，这样我们就可以通过导数解析的方式来求得损失函数最小时的权重W\nSVM损失函数 SVM的损失函数又叫做合页损失函数(hinge loss)，在上文中，我们提到可以使用一个线性分类器来对图像进行分类\n我们可以把一个这个结果S看成是线性变换后的结果，因此，对于每个图片，我们都可以通过这种方式来进行计算，从而得到它的一个分数\n其中，我们很有必要来展开阐述一下这个公式\n这个公式以得分输出的形式其实弱化了X W之间的关系，我们可以使用代码来进行描述\n假设这里有训练集X（X: A PyTorch tensor of shape (N, D) containing a minibatch of data.),并且给出了权重参数矩阵W（W: A PyTorch tensor of shape (D, C) containing weights.）\n也就是说，X的每张图片有D个特征，总共有N个样本；权重W有C个类别，每个类别有D个特征(按照列向量来说，每一个列对应一个图片)，这样，对于每一个样本X[i] (1xD)，我们都可以给这个图片计算出在不同类下面的分数\n1 2 3 4 5 6 7 8 import torch \u0026#39;\u0026#39;\u0026#39; X 是一个 NxD矩阵 W 是一个 DxC矩阵 \u0026#39;\u0026#39;\u0026#39; nums_train = X.shape[0] for i in range(nums_train): socres = W.t().mv(X[i]) # 得到这个图片在所有类上的分数 就像下面这个图片一样\n其中，假设X[0]是cat，那么我们就可以得到这张图片在10个类别上的对于分数;假设X[1]是car，那么我们同样可以得到car在10个类别上的分数，然后就可以得到所有的分数。\n注意一些技巧，我们想要统计的是在C个类上面的分数，所以输出的张量应该是Cx1或者1xC的;torch的mv函数很好的帮助我们将一个矩阵与一个向量相乘，在数学上的感受就是一个CxD的矩阵与一个Dx1的向量相乘，从而得到一个Cx1的输出分数\n再看SVM Loss 再看Loss函数，实际上做的就是一个衡量差值之间间隔的函数，方便起见，我们还是使用只有三个类和三个得分的输出\n怎么计算第一个cat的损失值呢？其实就是对于元素相加减，然后再与0作比较，所以cat的loss就是\n1 2 3 4 max(0, 5.1-3.2+1) + max(0, -1.7-3.2+1) Loss:2.9 同样我们也可以得到其它类的损失函数\n因此，这组数据的平均loss就是 Sum(loss) / x.shape[0]\n同样，我们还是可以使用代码来进行描述\n计算出所有类的分数\n这一步我们已经计算出\n找到对应正确的类，然后做加减法\n得到scores后，我们可以得到这个正确的类的分数。比如，第一张图片是cat，那么我们可以找到scores中是cat的分数是3.2\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import torch \u0026#39;\u0026#39;\u0026#39; X 是一个 NxD矩阵 W 是一个 DxC矩阵 y: A PyTorch tensor of shape (N,) containing training labels; y[i] = c means that X[i] has label c, where 0 \u0026lt;= c \u0026lt; C. y[i] 就是X[i]的标签类 \u0026#39;\u0026#39;\u0026#39; num_train = X.shape[0] num_classes = W.shape[1] for i in range(num_train): socres = W.t().mv(X[i]) # 得到这个图片在所有类上的分数 correct_class_score = scores[y[i]] # 得到所有分数中, X[i]的分数 for j in range(num_classes): if j == y[i]: # 不与自已比较 continue margin = scores[j] - correct_class_score + 1 # note delta = 1 if margin \u0026gt; 0: # 负分数不必相加 loss += margin 这样我们就得到这批样本的总的loss\n平均loss也可以求得\n1 loss /= num_train # 样本的总loss 除以 样本的总数量 矩阵形式 经过上面的铺垫，我们就可以为后面的矩阵求导做铺垫，现在让我们把这个hinge loss的公式展开\n因为我们不与自身作比较，所以，类的输出总分数就是 $$ W_j^{T} * X_i $$ 这个形式的意思就是$ W ^ T$的第j行实际上就是图像X[i]的参数，对应的，减去correct时的分数，而correct的表达可以是 $$ W_{y_i}^T * X_i $$ 然后累加和 $$ L_i = \\sum_{j \\neq y_i} \\max(0, w_j^T x_i - w_{y_i}^T x_i + \\Delta) $$ 我觉得这样写可以更加适合理解后面的梯度求导的形式！\n正则化 为了防止过拟合问题，我们可以给模型的参数W来加入一些惩罚项。\n首先我们来看看什么是过拟合。\n过拟合 以线性回归来举例，对于训练样本中的所有数据，如果我们的模型足够大、足够复杂，那么我们的模型就可以\u0026quot;记住\u0026quot;所有的点，于是，对于一个简单的样本来说，在训练后得到的模型大概是这样的：\n其中，f2是我们预期出现的模型性能的样子，正则化可以防止我们的模型拟合的过好，从而加强模型的预测能力。\nL1 L2正则化 正则化也有着不同的类别，常见的就是L1正则化和L2正则化\n其实正则化的目的就是去把W约束在一定的解的空间内，对于矩阵W来说，越简单的模型就意味着W值的某些取值取得越小，从而拟合出来得出现呈现出一些低阶多项式的形状，当我们把W的解约定在一定的取值内，我们假设这个取值是m，对于L1正则化的那个小尾巴 $ \\lambda R(W) $来说，其W的解 $$ 0 \u0026lt;= W_1 + W_2 + \u0026hellip;. + W_n \u0026lt;= m $$ 在二维空间内，我们可以得到这个解的区域是一个菱形区域(具体的数学证明设计凸优化的知识)\n同样的，对于L2正则化，我们同样要把参数W约束在一个范围内 $$ 0 \u0026lt;= W_1 ^ 2 + W_2 ^ 2 + \u0026hellip;.+W_n ^ 2 \u0026lt;= m $$\nL1 L2正则化在空间上的解释可以用下面这张图解释\nsoftmax与交叉熵损失函数 softmax函数常用于多分类问题，对于一组输出，比如说上面cat的输出，我们可以利用这个函数把各个输出的分数转换为概率来进行研究，其数学形式长这样\n分母是各个分数转化后的总和，分子是对于该类转化后的值\n交叉熵（Cross Entropy）是Shannon信息论中一个重要概念，主要用于度量两个概率分布间的差异性信息。在信息论中，交叉熵是表示两个概率分布 $p$, $q$ 的差异，其中 $p$ 表示真实分布，$q$ 表示预测分布，那么 $H(p,q)$ 就称为交叉熵 $$ H(p,q)=\\sum_i p_i \\cdot \\ln {1 \\over q_i} = - \\sum_i p_i \\ln q_i \\tag{1} $$ 在这个问题中，对于第i类来说，其真实分布$p$的概率就是1，所以总的表达式又可以进行化简\n然后slices中有一个有趣的问题，当一个图片在一个有着C类的数据集上进行分类时，假设我们预测得到的这个图片是每某个类的概率差不多，那么交叉熵损失是多少？\n带入公式，实际上就是 $ -log \\frac{1}{C} $, 也就是 $ log C $\n对于这个数据集来说，C的数量是10\n总结 这次我们了解了很多损失函数和避免模型过拟合的方法，但是还没有了解怎么求得我们的最优的参数矩阵W，下次来了解一下求最优参数的方法！\n","date":"2024-11-13T08:49:49+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/","title":"线性分类器"},{"content":"前言 Pytorch实现KNN近邻算法的一些思路\n图像分类 图像分类是计算机视觉的核心任务，当给定我们的模型一张图片，我们的模型应该可以正确的给出图片上的物体的类别\n例如，图片上有一只cat,那么模型应该正确输出cat。\n图像分类所面临的挑战 semantic gap 语义差异 在人类看来，几乎不用思考就可以辨别出图像上的物体，但是机器却是无情的执行命令的机器，怎么把图像上所蕴含的信息传递给机器呢？于是便有了像素的表示，我们可以把一张图片细分为很多个小格子，显然，格子越多，这样图片就更加清晰，图片是以数字形式存储的，这种形式通常被称为数字图像。数字图像是由像素（picture elements）组成的矩阵，每个像素代表图像中的一个小点。所以，现在来看，这样图片就是一个数字矩阵\n图像旋转 虽然可以使用矩阵和数字的方法来表示一张图片，可是，按照这种方法，当把原始图片旋转之后，每个格子内的数字就会发生变化，对于人类来说，即便是把图片旋转后，也可以很简单的辨认出图片上的物体；而对机器来说，如果不加以处理，当模型接受到一个这样的图片后，很有可能会发生错误的预测。\n更多的挑战 不但要识别出图片中是什么，还要准确的指出它的类别。例如，照片中是一只猫，但是，是橘猫、布偶猫还是狸花猫呢 更难的，一些野生动物为了更好的适应所生存的环境，其颜色会和环境发生重叠，也就是所谓的保护色 这些都给我们带来了更多的挑战\n图像数据集 这里，老师给出了常见的数据集，并且也对比了其中的图片数量级\n其中，我只使用过MNIST数据集(手写体数字识别)，在本次作业中实现的KNN近邻算法使用的是CIFAR10，与CIFAR100主要的区别就是总共只有10个类别，每个类别里面有很多张图片。选择这个数据库的原因是，MNIST中的数据太少，而ImageNet和Places365数据太多，折中选择了这个数据集。\nKNN近邻算法 对于这类算法，一般有两个通用的API\n训练\n1 2 3 def train(images, labels): # To do return modle 预测\n1 2 3 def predict(modle, test_images): # To do return test_labels 这个算法的核心思想是如果一个样本在特征空间中的k个最相邻的样本中的大多数都属于某一个类别，则该样本也属于这个类别。意思就是，对于一个预测的样本，如果离这个样本最近的数据都是属于A类别，那么这个要预测的样本很有可能就是A类别，毕竟它们很相似！\nL1距离 这里的距离指的就是曼哈顿距离,计算方法就是对应坐标作差后取绝对值。\nL2距离 L2距离指的是欧式距离，也就是在空间中计算两个点之间对应坐标距离的方法\n代码实现 现在来实现一下上文中提到的两个API，简单起见，这里使用numpy\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import numpy as np class KNN: def __init__(): pass def train(self, X, y): \u0026#39;\u0026#39;\u0026#39; X指的是输入的训练数据 y指的是训练数据对应的类别 \u0026#39;\u0026#39;\u0026#39; self.X = X self.y = y def predict(self, X): # X是输入的，需要预测其类别的test数据 num_test = X.shape[0] # 样本类别数 Ypred = np.aeros(num_test, dtype=self.y.dtype) # 遍历所有test数据，给test数据中的每个数据找到其距离最近的那个图像 for i in range(num_test): # 计算L1距离 dis = np.sum(np.abs(X[i, :] - self.X), axis = 1) # 找到其中距离最近的那个值对应的index，进而获得它的类别 min_idx = np.argmin(dis) # 对应的标签就是self.y的标签 Ypred[i] = self.y[min_idx] return Ypred 这里的predict使用的是显示loop,这种情况会特别耗时！\nk的取值 上面的代码没有显示的指定k的取值范围，而是直接使用k=1这个取值，首先，这样做很简单，因为只需要查看离测试样本距离最近的那个类是什么类型即可。但是，这种操作会使得算法会对异常值特别敏感，同时，不同类别之间的边界也是突出状而不是趋于平滑\n因此，我们可以提高k的取值，提高k值也减少了异常值对test数据的影响，因为test数据这次有k个不同的参考意见，而不是只去参考一个值。\n使用Pytorch来实现KNN算法 首先是计算两个向量之间的距离\n双重循环 This implementation uses a naive set of nested loops over the training and test data. The input data may have any number of dimensions \u0026ndash; for example this function should be able to compute nearest neighbor between vectors, in which case the inputs will have shape (num_{train, test}, D); it should also be able to compute nearest neighbors between images, where the inputs will have shape (num_{train, test}, C, H, W). More generally, the inputs will have shape (num_{train, test}, D1, D2, \u0026hellip;, Dn); you should flatten each element of shape (D1, D2, \u0026hellip;, Dn) into a vector of shape (D1 * D2 * \u0026hellip; * Dn) before computing distances.\n这里是老师给的一些代码的预处理的关键提示\nMore generally, the inputs will have shape (num_{train, test}, D1, D2, \u0026hellip;, Dn); you should flatten each element of shape (D1, D2, \u0026hellip;, Dn) into a vector of shape (D1 * D2 * \u0026hellip; * Dn) before computing distances.\n对于所有的输入向量，不管是什么维度的样本，我们都可以转换为一个二维张量，这样做可以简化一些计算，同时使得结果更加清晰。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import torch def compute_distances_two_loops(x_train: torch.Tensor, x_test: torch.Tensor): \u0026#34;\u0026#34;\u0026#34; Args: x_train: Tensor of shape (num_train, D1, D2, ...) x_test: Tensor of shape (num_test, D1, D2, ...) Returns: dists: Tensor of shape (num_train, num_test) where dists[i, j] is the squared Euclidean distance between the i-th training point and the j-th test point. It should have the same dtype as x_train. \u0026#34;\u0026#34;\u0026#34; num_train = x_train.shape[0] # 得到行数 num_test = x_test.shape[0] dists = x_train.new_zeros(num_train, num_test) # dists[i, j]是第i个x_train与x_test的距离 x_train_flat = x_train.view(num_train, -1) x_test_flat = x_test.view(num_test, -1) for i in range(num_train): for j in range(num_test): diff = x_train_flat[i] - x_test_flat[j] dists[i, j] = torch.sum(diff ** 2) return dists 这样做十分清晰，而且不开根是因为我们不需要得到确切的欧氏距离，只是单纯的比较大小，所以直接返回距离的平方即可。\n但是，这样并没有利用pytorch,会导致计算速度减慢，看看怎么优化！\n一重循环 这里使用的其实是一个高级特征，通过下标索引来访问元素(熟悉numpy的肯定不陌生)\n1 2 3 4 5 6 7 8 9 10 11 12 13 def compute_distances_one_loop(x_train: torch.Tensor, x_test: torch.Tensor): num_train = x_train.shape[0] num_test = x_test.shape[0] dists = x_train.new_zeros(num_train, num_test) x_train_flat = x_train.view(num_train, -1) x_test_flat = x_test.view(num_test, -1) for i in range(num_train): diff = x_train_flat[i] - x_test_flat # 计算每个x_train与x_test dists[i, :] = torch.sum(diff ** 2, dim=1) # 按照行进行求和 return dists 其中，diff = x_train_flat[i] - x_test_flat 这行代码可以自动帮我们计算第i个x_train与所有的x_test的差值，然后按照行的顺序进行求和(dim=1，numpy的是axis = 1)。\nNo loop 不使用循环 可以说不使用循环才是KNN里面一个十分精彩的部分，这里用到了高级机制BoardCast广播机制，下面我们来看一看这个部分的数学表达形式。\n图片来自bilibili视频.\n这里还需另外提醒的是矩阵的加减法，对于两个维数相同的矩阵$A 和B$，经过运算后得到$C$,其中，$$ C_{ij} = A_{ij} + B_{ij} $$\n$$ C_{ij} = A_{ij} - B_{ij} $$\n即对应元素相加减的操作\n1 2 3 4 5 6 7 8 9 10 11 12 13 def compute_distances_no_loops(x_train: torch.Tensor, x_test: torch.Tensor): num_train = x_train.shape[0] num_test = x_test.shape[0] dists = x_train.new_zeros(num_train, num_test) x_train_flat = x_train.view(num_train, -1) x_test_flat = x_test.view(num_test, -1) x_train_squre = torch.sum(x_train_flat ** 2, dim = 1) x_test_squre = torch.sum(x_test_flat ** 2, dim = 1) # 计算点积 temp = x_train_flat @ x_test_flat.t() dists = x_train_squre.view(-1, 1) + x_test_squre.view(1, -1) - 2 * temp return dists 根据上面的计算结果，x_train是一列的张量，x_test是一行的张量，从而两者进行广播后运算，值得注意的是，两者的内积可以转换为矩阵相乘的形式。\n预测predict 在计算出每个测试样本与(x_test)每个训练样本(x_train)的距离之后，对于每个测试样本，我们就可以找到其前k个最近值，然后确定其类别后返回。\n一个可能的函数看起来可能是这样的：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def predict_labels(dists: torch.Tensor, y_train: torch.Tensor, k: int = 1): num_train, num_test = dists.shape y_pred = torch.zeros(num_test, dtype=torch.int64) for j in range(num_test): # 遍历x_test数据 # 找到每个测试样本最近的k个训练样本的距离和索引 dists_k, indices = torch.topk(dists[:, j], k, largest=False) # largest=False 是升序 # 获取这些最近的k个训练样本的标签 nearest_labels = y_train[indices] # 计算每个标签的出现次数并选择出现次数最多的标签 labels, counts = torch.unique(nearest_labels, return_counts=True) # 记录最大出现次数 max_count = torch.max(counts) # 找到出现次数是最大出现次数的那个标签 most_common_labels = labels[counts == max_count] # 在计数最多的标签中选择数值最小的那个标签 y_pred[j] = torch.min(most_common_labels) return y_pred 这个代码的思路就是，寻找距离test数据的最近的k个不同类别，返回其出现次数最多的那个类别，如果有两个类别出现次数相同，那么就返回距离最近的那个。\n交叉验证 k折交叉验证（英语：k-fold cross-validation），将训练集分割成k个子样本，一个单独的子样本被保留作为验证模型的数据，其他k − 1个样本用来训练。交叉验证重复k次，每个子样本验证一次，平均k次的结果或者使用其它结合方式，最终得到一个单一估测。这个方法的优势在于，同时重复运用随机产生的子样本进行训练和验证，每次的结果验证一次，10次交叉验证是最常用的。\n对于原始数据 (raw 数据)，如果全部用于训练，则无法评估模型在未见过的数据上的表现，从而无法验证模型的有效性和准确性。因此，通常会将数据划分为两部分，一部分用于训练 (train)，另一部分用于测试 (test)，从而在测试集上评估模型的表现。\n在实际应用中，为了更好地选择超参数，我们会引入一个额外的数据集，称为验证集 (validation)。这样，数据集可以划分为三部分：训练集 (train)、验证集 (validation) 和测试集 (test)。\n具体流程如下：\n训练集：用于训练模型，使模型能够学习数据的特征和模式。 验证集：用于选择超参数。我们在验证集上测试不同的超参数组合，并选择在验证集上表现最好的参数设置。 测试集：在完成超参数选择后，我们在测试集上评估最终模型的性能。测试集完全不参与训练和参数选择，因此可以真实反映模型在新数据上的表现。 这种三分法的优势在于，验证集用于超参数选择，而测试集则用来评估模型在未见过的数据上的泛化能力，从而避免在超参数选择过程中过拟合测试集的风险。\n有关交叉验证的部分解释来自ChatGPT\n交叉验证 (Cross-Validation) 是一种用于更稳健地评估模型性能和选择超参数的方法。它通过多次数据划分和训练测试来减少模型对数据划分的偶然影响。在交叉验证中，我们通常将数据划分为多个等大小的部分（称为“折”或“folds”），并在每次训练时使用不同的折组合来训练和测试模型。\n交叉验证的数据划分步骤 以常用的 K 折交叉验证 (K-Fold Cross-Validation) 为例，具体步骤如下：\n将数据分成 K 个等大小的折：将数据集均匀分成 K 个折，记为 fold_1, fold_2, ..., fold_K。通常，K 的值是 5 或 10。\n多次训练和验证：对于每次迭代（共 K 次），使用 K-1 个折作为训练集，剩下的一个折作为验证集。具体来说：\n第一次迭代：使用 fold_2 到 fold_K 作为训练集，fold_1 作为验证集。 第二次迭代：使用 fold_1 和 fold_3 到 fold_K 作为训练集，fold_2 作为验证集。 以此类推，直到每个折都被用作一次验证集。 计算平均性能：在每次迭代中记录模型在验证集上的性能（例如准确率、损失等），然后将 K 次的验证结果平均，作为该模型在该超参数下的总体验证性能。\n选择最佳参数：对于每个超参数组合，都进行上述 K 次交叉验证，并根据平均性能选择表现最好的参数组合。\n最终测试：完成超参数选择后，可以在独立的测试集上评估模型的最终性能。\n交叉验证的优势 交叉验证避免了简单的训练/验证分割可能带来的偶然性，使模型能更稳健地评估数据上的表现。此外，交叉验证可以最大化地利用数据，因为每个数据点都能在验证集中出现一次，同时也在训练集中使用 K-1 次。这种方法尤其适合数据量较小的场景。\n总结 为了避免数据划分的偶然，我们保持三部分总体不变，对于train validation部分做出变化。\n我们把上面train数据分为nums个，每一块叫做一个fold, 假设分为5个fold，对于我们的待选参数集K来说，对于第i个参数K[i]，每次都选择1个fold作为验证集(validation)，剩下的4个作为训练集，然后再计算其准确率，对于K[i]来说，我们就得到了5个不同的准确率，然后可以取平均(mean)，作为选择K[i]为参数是的准确率，最后通过准确率就可以选出最优的K。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def knn_cross_validate( x_train: torch.Tensor, y_train: torch.Tensor, num_folds: int = 5, k_choices: List[int] = [1, 3, 5, 8, 10, 12, 15, 20, 50, 100], # 待选K ): x_train_folds = list(torch.chunk(x_train, num_folds)) # 把数据拆分为 num_folds块 y_train_folds = list(torch.chunk(y_train, num_folds)) k_to_accuracies = {k: [] for k in k_choices} # 使用键值对进行存储 for k in k_choices: for j in range(num_folds): x_val = x_train_folds[j] # 获取第j个验证集 y_val = y_train_folds[j] x_train_fold = torch.cat([x_train_folds[i] for i in range(num_folds) if i != j], dim=0) # 剩下的num_folds作为训练集 y_train_fold = torch.cat([y_train_folds[i] for i in range(num_folds) if i != j], dim=0) classifier = KnnClassifier(x_train_fold, y_train_fold) acc = classifier.check_accuracy(x_val, y_val, k) # 计算准确率 k_to_accuracies[k].append(acc) return k_to_accuracies 总结 KNN近邻算法还是非常适合初学者入门的，其中主要难点或者说比较新鲜的点就是广播机制BoardCast以及交叉验证的代码实现！\n","date":"2024-11-08T17:54:16+08:00","permalink":"https://XiaoPeng0x3.github.io/p/image-classifier%E7%AC%94%E8%AE%B0/","title":"Image Classifier笔记"},{"content":"数据泛化 一些常见的统计方法 mean：均值，对于一组数据来说，计算其均值可以直接使用np.mean来进行计算，对于多维数据，numpy引入了轴axis的概念，其中，轴的起点从0开始一直到n，例如，在二维数据中，其中每一行代表一个样本，每一列代表一个特征(类似于csv文件)\n​\t长,宽,高\nx1\t1,2,3\nx2\t1,4,5\nx3\t1,6,7\n对于这样一组数据，用列表来表示就是\n1 2 3 4 data = [x1, x2, x3] data = [[1,2,3], [1,4,5], [1,6,7]] 其中，按照axis=0的方式来计算均值，这里计算的就是长、宽、高 每个特征的均长、均宽、均高，按照axis = 1来进行计算，也就是计算每一行的均值，也就是每一个样本的均值(看起来没有什么意义)\n1 2 mean1 = np.mean(data, axis=0) # 按照列进行计算 mean2 = np.mean(data, axis=1) # 按照行进行计算 下面所有的方法var(方差)，std(标准差)等均可以按照不同的轴进行计算\nvar:variance，方差，不在叙述计算公式，可以直接使用np.var()\nstd：Standard deviation,可以根据方差得到,可以直接使用`np.std()\nnp.round:保留小数操作，例如，要对data保留三位小数，可以表示为\n1 ans = np.round(data, 3) min-max均值规化 公式为\n$$ x\u0026rsquo; = \\frac{x-min}{max-min}$$\n对于一组数据data,可以这样计算\n1 2 3 4 5 def MinMax(data:np.ndarray) -\u0026gt; np.ndarray: data_max = np.max(data, axis=0) data_min = np.min(data, axis=0) ans = (data - data_min) / (data_max - data_min) return ans 标准化 标准化可以把各个特征标准化为标准差为1，均值为0的正态分布\n公式为\n$$ x = \\frac{x-\\mu}{\\sigma} $$\n其中， $\\mu$是均值，$\\sigma$是标准差\n1 2 3 4 5 def Standardization(data: np.ndarray) -\u0026gt; np.ndarray: data_mean = np.mean(data, axis=0) data_std = np.std(data, axis=0) ans = (data - data_mean) / data_std return ans 总结 是否必须使用标准化方法？\n算法需求： 某些算法（如距离-based的算法, K-means, K邻近）对特征尺度非常敏感，标准化几乎是必需的。 某些算法（如决策树、随机森林等）对特征尺度不敏感，标准化不是必需的。 数据特性： 如果特征的数值范围已经很接近，标准化的效果可能不明显。 如果特征的数值范围差异很大，标准化可以显著提升模型性能。 模型性能： 通过实验比较标准化前后的模型性能，可以决定是否需要标准化。 ","date":"2024-11-04T10:43:36+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E6%95%B0%E6%8D%AE%E6%B3%9B%E5%8C%96/","title":"深度学习基础系列：数据泛化"},{"content":"reshape reshape可以改变矩阵的维度，例如，有一个一维矩阵\n1 2 3 4 import numpy as np a = np.arange(20) # 转变为四行五列 ans = np.reshape(a, (4,5)) # 传入一个元组 转化前后数据量是一致的，20个元素不可以转换为3行4列\n","date":"2024-11-03T23:19:33+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%9F%A9%E9%98%B5%E5%8F%98%E5%9E%8B/","title":"深度学习基础系列：矩阵变型"},{"content":"矩阵的转置 矩阵的转置可以说是一个很常见的矩阵操作了，对于简单的矩阵(二维及以下)的矩阵来说，只需要调用numpy的T属性即可，例如\n1 2 3 4 5 6 7 import numpy as np a = [[1,2,3], [3,4,5], [5,6,7]] a_nparray = np.array(a) # 转换为ndarray a_T = a_nparray.T # 是一个属性 或者直接使用transpose\n1 2 3 4 5 6 import numpy as np a = [[1,2,3], [3,4,5], [5,6,7]] a_nparray = np.transpose(a) ","date":"2024-11-03T22:56:32+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%9F%A9%E9%98%B5%E8%BD%AC%E7%BD%AE/","title":"深度学习基础系列：矩阵转置"},{"content":"Softmax函数 softmax函数常用于多分类问题，我们希望模型的输出可以作为预测的概率，即输出值越大的那个参数在预测的时候很有可能就是正确答案。\n但是回想一下概率的计算公式(扔骰子)，对于总的概率空间样本来说，其概率的总和一定是1，而我们的预测输出基本上总和不可能是1，这里softmax函数的作用就是压缩这些输出值，从而使用概率的方式进行表示\nsoftmax函数长这样\n给定一个K维向量$ z=[z_1,z_2,\u0026hellip;,z_K]$，Softmax函数的定义为：\n这里，$\\mathbf{z}$ 是一个K维向量，$z_j$ 是向量中的第 $j$ 个元素，$ 1 \\leq j \\leq K$。\n$$ \\sigma(\\mathbf{z})j = \\frac{e^{z_j}}{\\sum{k=1}^{K} e^{z_k}} $$\n计算过程 对于给定的一个list输入，先计算得到softmax函数的分母值\n1 2 3 4 5 6 7 8 9 10 11 12 13 import math input = [1, 2, 3] # 分母是math.exp形式 ans = list() sumVal = 0.0 for val in scores: sumVal += math.exp(val) # 分母 for val in scores: ans.append(math.exp(val) / sumVal) print(ans) 我们也可以使用numpy进行计算\n1 2 3 4 5 6 7 8 import numpy as np def softmax(scores: list[float]) -\u0026gt; list[float]: temp = np.array(scores) # 转换为ndarray exp_temp = np.exp(temp) # 计算所有值的exp值 exp_sum = np.sum(exp_temp) # 和为分母 # 计算分子 exp_temp ans = np.round(exp_temp / exp_sum, 4) # 每个exp值除以分母，并保留4位小数 return ans numpy的方便之处在于不用编写循环和计算快！\n","date":"2024-11-02T10:46:57+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97softmax/","title":"深度学习基础系列：softmax"},{"content":"上次我们了解了怎么用Go语言来创建和连接一个socket，这里来看一看怎么封装用户行为以及怎么实现用户广播上线功能\nServer的封装 server 这是在上一节中提到的server结构\n1 2 3 4 type Server struct { Ip string Port int } 可以看到，我们只有两个简单的成员属性，为了实现用户上线后全部广播的操作，我们需要在server中记录下来每次连接到server的client，这里可以使用map来进行记录，同时，为了实现全局广播的效果，我们可以在server中使用一个chan来进行管理。\n为什么要实现server要实现一个chan通道呢，当有用户上线建立连接后，我们就可以把上线的这个消息发送给chan来进行管理，然后遍历map就可以实现广播的操作。\nserver的实现 需要在原来的基础上多增加一些属性\n1 2 3 4 5 6 7 8 9 10 type Server struct { Ip string Port int // 创建用户表 OnlineMap map[string]*User // 同步的锁 mapLock sync.RWMutex // 负责全局广播的chan Message chan string } 在创建好一个server后，与上一篇文章一样，使用协程去处理连接之后的状态\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 func (this *Server) Start() { // 创建好一个监听对象 // 这个函数会有两个返回值 // 一个是创建的 listen对象， 一个是是否创建成功 listener, err := net.Listen(\u0026#34;tcp\u0026#34;, fmt.Sprintf(\u0026#34;%s:%d\u0026#34;, this.Ip, this.Port)) // 创建失败的话， err就会有一个失败code // err != nil 就是说明创建失败 if err != nil { fmt.Println(\u0026#34;创建监听对象失败！, err\u0026#34;, err.Error()) return } // 启动之后记得关闭，避免浪费资源 defer listener.Close() // 然后就是使用accept方法 // 在一个循环里面不停的接受数据 // 监听 // 全局管道 go this.ListenMessage() for { // 这里的 meaage 是net.Coon类型 conn, err := listener.Accept() // 说明接收到了数据 if err != nil { fmt.Println(\u0026#34;监听失败！\u0026#34;) continue } // 打开一个协程去处理 go this.Handle(conn) // 下面的代码不会阻塞 } } 这里的Handle方法可以去处理连接请求，有哪些请求呢？\n当一个用户上线后，应该把这个用户添加到Online表中 广播这个用户上线的消息 看到这里，起始我们缺少封装的user类，我们可以再封装一个user类\nUser的封装 user类的实现 在user里面，基本的属性有Name, Address这些操作，为了更加方便User把消息转发给client(转发操作指的是conn.Write操作)，在消息接收的上我们可以初始化一个chan来进行连接转发\n1 2 3 4 5 6 type User struct { Name string Addr string C chan string conn net.Conn } C是为了接受来自server的消息，conn是为了把消息转发给client，那么在初始化的时候，就得去监听，看是否有消息写回来\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 func NewUser(conn net.Conn) *User { userAddr := conn.RemoteAddr().String() // 可以得到客户端的地址 user := \u0026amp;User{ Name: userAddr, Addr: userAddr, C: make(chan string), conn: conn, } go user.ListenMessage() return user } // ListenMessage 监听User的chan func (this *User) ListenMessage() { for { mes := \u0026lt;-this.C this.conn.Write([]byte(mes + \u0026#34;\\n\u0026#34;)) } } 这样，在实现连接之后，我们就可以添加用户到在线表里面去\n1 2 3 4 5 6 7 8 9 10 11 12 func (this *Server) Handle(conn net.Conn) { // fmt.Println(\u0026#34;连接成功！\u0026#34;) // 执行到这里，说明已经有一个用户上线 newUser := NewUser(conn) this.mapLock.Lock() this.OnlineMap[newUser.Name] = newUser this.mapLock.Unlock() // 广播该用户已上线 this.Boardcast(newUser, \u0026#34;I am in!\u0026#34;) } user用户上线的广播 怎么实现Boardcast方法呢？可以直接利用server里面的chan来实现\n1 2 3 4 5 func (this *Server) Boardcast(u User, mes string) { // 把上线的消息发送给message chan sendMes := \u0026#34;[\u0026#34; + user.Name + user.Addr + mes + \u0026#34;]\u0026#34; this.Message \u0026lt;- sendMes // 发送给管道 } 在发送给管道后，server中的管道就就得到了数据，因此，就可以直接通过server中的chan进行消息的传输，遍历Onlinemap即可\n1 2 3 4 5 6 7 8 9 10 11 12 13 func (this *Server) ShareMessage() { // 只要message有消息 // 那么就发送给在线的所有用户 for { mes := \u0026lt;-this.Message for _, user := range this.OnlineMap { this.mapLock.Lock() user.C \u0026lt;- mes this.mapLock.Unlock() } } } 最后在服务启动的时候去监听转发信息功能即可\n总结 整个流程看起来是这样的\n创建tcp套接字并开启连接 连接后会创建User对象 User对象会向Server的message发送信息 server的message接收到信息之后会遍历整个Online表，把User上线的消息发送给在Online表中的每一个User 其中，消息的转发依赖于conn.Write，User上线后把上线消息写入Serevr的chan，Server再把该User上线的消息通过Online表写入User的chan。\n","date":"2024-11-02T09:09:40+08:00","permalink":"https://XiaoPeng0x3.github.io/p/tcp%E8%81%8A%E5%A4%A9%E5%AE%A4%E4%BA%8C%E7%94%A8%E6%88%B7%E4%B8%8A%E7%BA%BF%E5%92%8C%E5%B9%BF%E6%92%AD/","title":"TCP聊天室(二)：用户上线和广播"},{"content":"前言 在学习完Go语言之后，总是感觉没有合适的上手项目进行练习，最近正好看到一个TCP网络聊天室的小项目，这个项目只使用基础的包而不使用任何框架，非常适合练手。\n需要的工具有\nGo开发环境 nc工具，方便模拟client进行测试 建立连接 在Go中，我们可以使用net包来进行基本的server的socket的创建，也就是net.Listen方法\n1 net.Listen() 下面是这个函数的原型\n1 2 3 4 5 // The network must be \u0026#34;tcp\u0026#34;, \u0026#34;tcp4\u0026#34;, \u0026#34;tcp6\u0026#34;, \u0026#34;unix\u0026#34; or \u0026#34;unixpacket\u0026#34;. func Listen(network, address string) (Listener, error) { var lc ListenConfig return lc.Listen(context.Background(), network, address) } 可以看到，函数的两个参数都是string类型的，第一个参数指定的是通信的网络(可以直接指定tcp), 第二个是server的地址。返回值就是监听对象和err\n例如，我们想要启动一个监听,就可以这样写\n1 2 3 4 listener, err := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;127.0.0.1:8080\u0026#34;) if err != nil { // To do } 就创建好了一个scoket\n在得到listener后，要开启接受功能，可以调用\n1 listener.Accept() 同样，这个函数有两个返回值，正常使用是这样的\n1 2 3 4 conn, err := listener.Accept() if err != nil { // To do } 其中，返回的是一个net.Conn类型的参数，可以通过conn在socket之间传递数据\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 type Conn interface { // Read reads data from the connection. // Read can be made to time out and return an error after a fixed // time limit; see SetDeadline and SetReadDeadline. Read(b []byte) (n int, err error) // Write writes data to the connection. // Write can be made to time out and return an error after a fixed // time limit; see SetDeadline and SetWriteDeadline. Write(b []byte) (n int, err error) // Close closes the connection. // Any blocked Read or Write operations will be unblocked and return errors. Close() error // LocalAddr returns the local network address, if known. LocalAddr() Addr // RemoteAddr returns the remote network address, if known. RemoteAddr() Addr // ........ // ........ } conn.Read可以从连接中读取数据(server可以read来自client的数据)， 同时conn.Write可以从连接中发送数据(server向client发送数据)\n这里就实现了通信的基石，即发送和接受数据，其整个过程就是\n创建socket：net.Listen,返回一个net.Listener对象 开始接收请求：listener.Accept,返回一个net.Conn对象 使用net.Conn实现接收数据和发送数据 simple demo 这里创建一个简单的demo程序，在server接收到来自client的数据后，把接受到的数据全部转换为大写后发送给client\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 package main import ( \u0026#34;bytes\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;net\u0026#34; ) func main() { ip := \u0026#34;127.0.0.1\u0026#34; port := 8080 //CreateServer(ip, port) listener, err := net.Listen(\u0026#34;tcp\u0026#34;, fmt.Sprintf(\u0026#34;%s:%d\u0026#34;, ip, port)) defer listener.Close() if err != nil { fmt.Println(\u0026#34;Eror!\u0026#34;, err) return } buf := make([]byte, 1024) conn, err := listener.Accept() defer conn.Close() for { if err != nil { fmt.Println(\u0026#34;connect fail\u0026#34;, err) return } // 读取数据 conn.Read(buf) // 写回数据 conn.Write(bytes.ToUpper(buf)) } } 然后使用nc工具\n1 nc 127.0.0.1 8080 当我们发送hello的时候，server正确的返回了HELLO\n思考：当有多个client的时候怎么办？\n协程处理 只有一个用户创建连接的时候可以正常返回，但此时有多个用户创建了连接请求，由于我们只accept了一次连接请求，所以当多个用户尝试连接的时候，第二个及之后的那些用户无法与服务器建立连接。\n解决办法\n每次在循环的过程中不断的进行监听，而不是只监听一次。 原始代码是\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // ..... conn, err := listener.Accept() // 把这里添加到循环中 defer conn.Close() for { if err != nil { fmt.Println(\u0026#34;connect fail\u0026#34;, err) return } // 读取数据 conn.Read(buf) // 写回数据 conn.Write(bytes.ToUpper(buf)) } } 添加到循环后就可以不断的建立连接\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // ...... for { conn, err := listener.Accept() if err != nil { fmt.Println(\u0026#34;connect fail\u0026#34;, err) return } // 读取数据 conn.Read(buf) // 写回数据 conn.Write(bytes.ToUpper(buf)) } } 然后再新建client的时候就可以处理多用户连接。\n这样写有什么问题？\n可以发现，当在一个client发送第二组数据后，server什么都没有返回，这是因为在循环执行到\n1 conn.Write(bytes.ToUpper(buf)) server一直在期待新的链接，而不是去处理之前的client的数据\n使用go协程\n在每次conn成功后，为了保持后续的链接，可以把后续的read和write封装为go协程\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 package main import ( \u0026#34;bytes\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;net\u0026#34; ) func Handler(conn net.Conn) { defer conn.Close() buf := make([]byte, 1024) for { cnt, _ := conn.Read(buf) conn.Write(bytes.ToUpper(buf[:cnt])) } } func main() { ip := \u0026#34;127.0.0.1\u0026#34; port := 8080 //CreateServer(ip, port) listener, err := net.Listen(\u0026#34;tcp\u0026#34;, fmt.Sprintf(\u0026#34;%s:%d\u0026#34;, ip, port)) defer listener.Close() if err != nil { fmt.Println(\u0026#34;Eror!\u0026#34;, err) return } //defer conn.Close() for { conn, err := listener.Accept() if err != nil { fmt.Println(\u0026#34;connect fail\u0026#34;, err) return } go Handler(conn) } } 也就是说在主函数内，只负责去监听是否有用户链接，而链接后的读写就去创建一个新的协程，在这个协程内根据这个链接不断的去实现client-server之间的读写。\n总结 net.Listen：创建tcp socket, 返回listener对象 listener.Accept：监听客户端的连接, 返回net.Conn连接对象 net.Conn：实现read和write，读取和发送数据 go：开启一个协程 ","date":"2024-11-01T17:43:28+08:00","permalink":"https://XiaoPeng0x3.github.io/p/tcp%E8%81%8A%E5%A4%A9%E5%AE%A4%E4%B8%80tcp%E9%80%9A%E8%AE%AF/","title":"TCP聊天室(一)：tcp通讯"},{"content":"协方差矩阵 协方差(covariance)可以用来观测变量之间是否存在线性相关性。然而，协方差本身有一些局限性，因此在实际应用中，我们通常还会使用相关系数来进一步评估变量之间的相关性。\n协方差的局限性 尺度依赖性：\n协方差的值受变量尺度的影响。如果一个变量的值范围很大，而另一个变量的值范围很小，即使它们之间有很强的线性关系，协方差的绝对值也可能很大或很小，这使得直接比较不同变量之间的协方差变得困难。 单位依赖性：\n协方差的单位是两个变量单位的乘积。例如，如果一个变量的单位是米，另一个变量的单位是秒，那么协方差的单位将是米·秒。这使得协方差的解释更加复杂。 相关系数 为了克服协方差的这些局限性，我们通常使用 皮尔逊相关系数（Pearson correlation coefficient），它是一个标准化的协方差，范围在 -1 到 1 之间。\n皮尔逊相关系数的定义 皮尔逊相关系数 ( r ) 定义为： $$ r_{XY} = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y} $$\n其中：\n$\\sigma_X $ 是 X 的标准差。 $\\sigma_Y$ 是 Y 的标准差。 解释 ( r = 1 )：完全正相关，即两个变量完全同向变化。 ( r = -1 )：完全负相关，即两个变量完全反向变化。 ( r = 0 )：没有线性相关性。 ( |r| ) 接近 1：表示强相关性。 ( |r| ) 接近 0：表示弱相关性或没有相关性。 Python 示例 可以使用 numpy 或 pandas 库来计算皮尔逊相关系数。\n使用 numpy 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import numpy as np # 示例数据 data = np.array([ [1, 4], # 观测值1 [2, 5], # 观测值2 [3, 6] # 观测值3 ]) # 计算协方差矩阵 cov_matrix = np.cov(data, rowvar=False) print(\u0026#34;协方差矩阵:\\n\u0026#34;, cov_matrix) # 计算相关系数矩阵 corr_matrix = np.corrcoef(data, rowvar=False) # correlation coefficient print(\u0026#34;相关系数矩阵:\\n\u0026#34;, corr_matrix) 这里的np.cov()和np.corrcoef()，如果不指定第二个参数，那么第二个参数默认rowvar = True，意思就是这组数据是按照横向放置的，意思就是每一行是一个属性\n而在有时候需要从文件里面读取一些属性，例如\n1 2 3 X\tY\tZ x1\ty1\tz1 x2\ty2\tz2 那么这个时候，就可以把默认值设置为False,代表每一列是一个属性\n使用 pandas 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import pandas as pd # 示例数据 data = pd.DataFrame({ \u0026#39;X\u0026#39;: [1, 2, 3], \u0026#39;Y\u0026#39;: [4, 5, 6] }) # 计算协方差矩阵 cov_matrix = data.cov() print(\u0026#34;协方差矩阵:\\n\u0026#34;, cov_matrix) # 计算相关系数矩阵 corr_matrix = data.corr() print(\u0026#34;相关系数矩阵:\\n\u0026#34;, corr_matrix) 结论 协方差提供关于变量之间线性关系的一些信息，但建议使用皮尔逊相关系数。\n相关系数不仅标准化了协方差，还提供了一个易于解释的度量，范围在 -1 到 1 之间。1代表存在正相关关系，-1代表负相关关系。\n","date":"2024-10-26T16:01:06+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5/","title":"深度学习基础系列：协方差矩阵"},{"content":"矩阵乘法 矩阵乘法是矩阵运算的基础，简单来说，对于两个A和B矩阵来说，只要A的列数等于B的行数，那么这两个矩阵就可以发生运算\n即A是一个m x n的矩阵， B是一个n x k的矩阵，那么运算后的结果就是m x k的矩阵\n首先把矩阵转换为np.array类型，然后判断它们的类型，查看是否可以相乘\n1 2 3 4 5 6 7 8 import numpy as np def matrix_dot_vector(a:list[list[int|float]],b:list[int|float])-\u0026gt; list[int|float]: a = np.array(a) b = np.array(b) if a.shape[1] != b.shape[0]: return -1 c = np.dot(a, b) return c np.dot()接受两个数组(矩阵)并返回它们的结果\n","date":"2024-10-25T23:41:00+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95/","title":"深度学习基础系列：矩阵乘法"},{"content":"在上一篇中，我们学习了如何在server上创建一个TCP的套接字，这里来看一看对于服务端，server是怎么获取客户端的连接请求的。\n获取连接请求 在开启监听(listen)后，如果有客服端尝试连接服务器，那么内核将于客户端进行连接，因为请求连接可能会有多个，所以内核会维护一个队列来存放这些请求。当客户端连接到内核之后，那么内核可以使用accept函数来返回并接受来自这个连接。\n下面来用代码演示一下\n1 2 3 4 5 6 7 8 9 10 11 # 把上次的代码复制一下 from socket import * # 创建一个套接字 # 使用socket进行初始化 serverSocket = socket(AF_INET, SOCK_STREAM) # 使用IPV4地址簇，使用的是流式socket # 接下来开始进行绑定 serverSocket.bind((\u0026#34;127.0.0.1\u0026#34;, 8080)) # bind 需要的是一个tuple类型 # 绑定后可以开始listen, 即查看是否有客户端连接到服务器 serverSocket.listen(1) # 最多监听一个 在创建好server socket之后，我们就可以使用accept函数来进行连接。这里先不考虑TCP协议在连接时的一些细节\n1 2 3 4 5 6 \u0026#39;\u0026#39;\u0026#39; 返回值： connectionSocket 客户端连接套接字 addr 连接的客户端地址 \u0026#39;\u0026#39;\u0026#39; connectionSocket，addr = serverSocket.accept() 也就是说，这个操作会返回一个新的socket,不同的是，通过这个socket就可以实现server与client之间的通讯。\n可以接受或者发送数据，下面是一些API\n1 2 3 recv()/send() recvmsg()/sendmsg() recvfrom()/sendto() 需要注意的是，传递的这些字符全部都是流式数据，与原始字符串不同\n简单的demo 在这里，创建一个简单的小demo，我们可以创建一个简单的服务程序，这个server什么也不做，只是简单的把接受到的数据原封不动的发送给client\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from socket import * # 初始化好一个socket with socket(AF_INET, SOCK_STREAM) as serverScoket: # 绑定IP和port serverScoket.bind((\u0026#34;127.0.0.1\u0026#34;, 8080)) # 开启监听listen serverScoket.listen(1) # 这是一个监听队列，当处理多个请求的时候，会把未来得及处理的放入队列里面，其中的参数表示队列的大小 # 开启socket的accept,从而处理来自server的连接 connectionSocket, _ = serverScoket.accept() # 先忽略第二个返回值 print(\u0026#39;connect!\u0026#39;) with connectionSocket as c: while True: data = c.recv(1024) # 每次都尝试获得来自客户端的数据, 注意这是个字节流数据 if not data: break c.sendall(data) # 把接受到的数据返回 我们可以使用netcat这个工具来进行测试\n1 nc 1270.0.0.1 8080 可以看到，服务器端口已经连接上了\n1 connect! 通过这个有趣的连接，我们还可以使用eval函数来实现计算式求值\neval函数是危险的！这里只是做演示\neval() 可以执行任意的 Python 代码。如果传入的字符串包含恶意代码，这可能导致严重的安全漏洞。例如，攻击者可以通过构造恶意表达式来执行系统命令、访问敏感数据、修改文件等。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 from socket import * # 初始化好一个socket with socket(AF_INET, SOCK_STREAM) as serverScoket: # 绑定IP和port serverScoket.bind((\u0026#34;127.0.0.1\u0026#34;, 8080)) # 开启监听listen serverScoket.listen(1) # 这是一个监听队列，当处理多个请求的时候，会把未来得及处理的放入队列里面，其中的参数表示队列的大小 # 开启socket的accept,从而处理来自server的连接 connectionSocket, _ = serverScoket.accept() # 先忽略第二个返回值 print(\u0026#39;connect!\u0026#39;) with connectionSocket as c: while True: data = c.recv(1024) # 每次都尝试获得来自客户端的数据, 注意这是个字节流数据 if not data: break # 解码接收到的数据 expression = data.decode() # 计算表达式的结果 result = str(eval(expression))+\u0026#39;\\n\u0026#39; # 发送结果给客户端 c.sendall(result.encode()) 一些缺点 浪费性能 对于真实世界来说，这里的服务器实在是太弱了\n1 2 3 4 5 while True: data = c.recv(1024) if not data: break c.sendall(data) # 把接受到的数据返回 server每次只能处理一个请求，当client没有发送数据的时候，c.recv(1024)这行代码也就会永远阻塞在这里，很浪费性能。\n此时，很自然的想到以并发的方式去处理频繁的连接\n应对策略 多进程 可以使用fork来创建多个进程，其中，fork函数在子进程里面的返回值是0，所以，可以设想一下，当有服务来临时，父进程只去监听(accept)是否有连接，同时可以把读写操作放到子进程里面去运行，这样通过进程调度策略，就可以实现并发\n代码看起来是这样的：\n1 2 3 4 5 6 7 8 while (true) { pid_t pid; if ((pid = fork()) == 0) { // 子进程 // do read() or do write() } else { // accept } } 每当一个连接到来时，程序就会创建一个子进程来处理，可惜进程实在是不够“轻量”，而且进程调度器来进行调度的时候也需要有着内核态到用户态的转换、进程的变量读写保存，因此可以考虑使用多线程来进行尝试\n多线程 什么是线程？\nIn computer science, a thread of execution is the smallest sequence of programmed instructions that can be managed independently by a scheduler, which is typically a part of the operating system.[1] In many cases, a thread is a component of a process.\nThe multiple threads of a given process may be executed concurrently (via multithreading capabilities), sharing resources such as memory, while different processes do not share these resources. In particular, the threads of a process share its executable code and the values of its dynamically allocated variables and non-thread-local global variables at any given time.\nThe implementation of threads and processes differs between operating systems.\n线程是进程的一个子集(你可以这么认为)，一个进程里面会有多个线程，这些线程共享这个进程所有的资源(因为它们有着一样的页表)，所以在线程切换的时候，不需要有很大的开销，只需要维护每个线程内部不共享或者私有的数据即可。\n这样就可以使用多线程来处理\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import threading from socket import * # 把发送请求发送这里 def handle_client(c, addr): print(addr, \u0026#39;connect\u0026#39;) while True: data = c.recv(1024) if not data: break c.sendall(data) with socket(AF_INET, SOCK_STREAM) as serverSocket: serverSocket.bind((\u0026#34;127.0.0.1\u0026#34;, 8080)) serverSocket.listen() while True: connectionSocket, addr = serverSocket.accept() # 接受多个不同的请求 t = threading.Thread(target=handle_client, args=(connectionSocket, addr)) t.start() 多线程的方法可以使用线程池的方法去调度，不过线程也会占用系统的资源。\n结尾 之后记录一下select、poll、epoll这些方法\n","date":"2024-10-21T13:19:18+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%BA%8C%E6%9C%8D%E5%8A%A1%E7%AB%AF%E8%8E%B7%E5%8F%96%E8%BF%9E%E6%8E%A5%E8%AF%B7%E6%B1%82/","title":"网络编程(二)：服务端获取连接请求"},{"content":"前言 这个系列会记录所有的学习计算机网络时的笔记。\n首先计算机网络很重要(Web开发)，网络在日常生活和学习中无处不在，但是你真的了解底层的那些细节吗？比如TCP/IP协议、UDP协议，在面试的时候可能会问到相关的细节。\n参考书籍 本次参考的书籍是：\n计算机网络：自顶向下\n在此基础上，可以选择一些视频资料进行辅助，推荐一门好评较高的课程：中科大——计算机网络，同时，如果觉得学了这些知识而缺少lab来动手的话，可以参考这门lab,如果觉得过于简单，也可以挑战一下CS144.\nSocket Socket的中文翻译是套接字，这个翻译很难理解，你可以把它认为是一个接口，插座之类的物品，或者认为它是一个介质。在日常生活中，很多都是Client-Server的模式，即客户端请求服务器上的一些资源，服务器在收到客户端的请求之后把数据发送到给客户端，这些数据就是通过套接字来实现的传输的。\n套接字（Socket）是一个抽象层，应用程序可以通过它发送或接收数据，可对其进行像对文件一样的打开、读写和关闭等操作。套接字允许应用程序将 I/O 插入到网络中，并与网络中的其他应用程序进行通信。\n网络套接字是 IP 地址与端口 Port 的组合。\n为了满足不同的通信程序对通信质量和性能的要求，网络系统提供了三种不同类型的套接字，以供用户在设计网络应用程序时根据不同的要求来选择。分别是：\n流式套接字（SOCK-STREAM）。提供一种可靠的、面向连接的双向数据传输服务，**实现了数据无差错、无重复的发送。**流式套接字内设流量控制，被传输的数据看作是无记录边界的字节流。在 TCP/IP 协议簇中，使用 TCP 协议来实现字节流的传输，当用户想要发送大批量的数据或者对数据传输有较高的要求时，可以使用流式套接字。 数据报套接字（SOCK-DGRAM）。提供一种无连接、不可靠的双向数据传输服务。数据包以独立的形式被发送，并且保留了记录边界，不提供可靠性保证。数据在传输过程中可能会丢失或重复，并且不能保证在接收端按发送顺序接收数据。在 TCP/IP 协议簇中，使用 UDP 协议来实现数据报套接字。在出现差错的可能性较小或允许部分传输出错的应用场合，可以使用数据报套接字进行数据传输，这样通信的效率较高。 原始套接字（SOCK-RAW）。该套接字允许对较低层协议（如 IP 或 ICMP ）进行直接访问，常用于网络协议分析，检验新的网络协议实现，也可用于测试新配置或安装的网络设备。 这里需要记住两种协议：TCP和UDP，先不考虑它们底层是怎么实现的，TCP的特点就是可以保证在传送的过程中数据不丢失、不重复、不乱序，原原本本的把数据发送给接受者；而UDP协议考虑的很少，所以UDP协议可以用于传输速度快的场景。\n创建Socket 这里为了简单起见，使用Python来创建\n1 2 3 4 5 6 7 8 # 导入socket包 from socket import * # 创建一个TCP套接字 serverSocket = socket(AF_INET, SOCK_STREAM) # TCP是流式套接字 # 绑定Server的IP和Port serverSocket.bind((\u0026#34;127.0.0.1\u0026#34;, 8080)) # bind的参数是tuple类型，需要一个地址和端口 到这里，已经成功把serverSocket绑定到IP地址为127.0.0.1:8080的机器上了\n监听listen 在创建好serverSocket后，因为不知道什么时候server会收到连接请求，一个有效的方法是listen函数\n1 serverSocket.listen(1) 这里，函数的参数是监听队列的大小，当有多个连接请求时，这些请求会被放到监听队列里面。\n到目前为止，我们可以看一看serverSocket的一些信息\n1 print(ServerSocket) 下面是输出信息\n1 \u0026lt;socket.socket fd=340, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=(\u0026#39;127.0.0.1\u0026#39;, 8080)\u0026gt; 其中，第一个参数fd你可能不太了解，这个是一个文件描述符(file descriptor)，文件描述符是一个Obj的handle，或者你也可以理解为指向文件的指针，在Unix和类Unix系统中，所有的I/O都被抽象为文件描述符，包括网络套接字。\n流程 在创建一个socket时，可以遵循以下步骤\n根据协议初始化一个socket 给socket绑定IP 开启监听 关闭这个socket,防止内存泄漏 好，这就是创建socket的所有过程~~\n","date":"2024-10-20T17:31:13+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%B8%80%E5%88%9D%E8%AF%86%E5%A5%97%E6%8E%A5%E5%AD%97/","title":"网络编程(一)：初识套接字"},{"content":"堆结构 许多应用程序都需要处理有序的元素，但是不一定要求它们全部有序，或是不一定要一次就将它们完全排好序。例如，你要实现一个进程调度算法，每一个进程都有一个优先级(当有电话到来时，你的游戏很可能会被打断)，所以电话程序的优先级会高于游戏程序的优先级。\n所以当前面临的挑战是：一个电脑上每个时刻都会有许多不同的进程，怎么按照优先级去调度它们呢？也就是说，在每次调度的时候，我们都希望花费很少的时间去找到最重要的进程(或者最不重要的进程)，堆结构就很好的解决这个问题。\n完全二叉树 二叉树是一种常见的结构，对于一个结点来说，它通常可以有两个子节点(left, right)，看起来是这样的：\n1 2 3 3 1 2 4 5 6 7 用代码来表示：\n1 2 3 4 5 type Tree struct { Val int // 值域 Left *Tree Right *Tree } 不过这里我们不会使用这种形式，可以使用数组来进行父子结点之间关系。\n完全二叉树是一个特殊的二叉树，下面是维基百科对完全二叉树的介绍\n在一颗二叉树中，若除最后一层外的其余层都是满的，并且最后一层要么是满的，要么在右边缺少连续若干节点，则此二叉树为完全二叉树（Complete Binary Tree）。具有n个节点的完全二叉树的深度。深度为k的完全二叉树，至少有个节点，至多有个节点。\n通俗点来说，完全二叉树就是满二叉树从后向前去除子节点后剩下的二叉树。简单来说，堆按照根节点与子节点的大小关系可以分为大根堆和小根堆。大根堆的根节点比所有的子节点大，小根堆的根节点比子节点都小。所以按照这种规则，大根堆的根节点一定是数组里面的最大值，小根堆的根节点一定是数组里面的最小值。\n堆的实现 首先，对于一个不是堆结构的数组来说，第一步要做的就是heapify堆化，堆化需要用到两个辅助函数，\n就是 sink和swim。\n从名字上来看，sink就是向下交换，swim就是向上交换，对于一个数组，我们可以从第一个非叶子结点开始堆化，对于这些非叶子结点，我们每次都进行堆化，这样初始化到数组的第一个元素的时候，整个数组就是堆结构了。\nsink方法 首先是找到这个数组的第一个非叶子结点，对于一个数组arr来说，我们假定它的长度是n，那么它的第一个非叶子结点就是(n-1) / 2，这里从大根堆开始建堆.\n首先，一个堆结构看起来可能是这样的：\n1 2 3 type MaxHeap struct { item []int } 它的sink方法是：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 func (this *MaxHeap) sink(k int) { n := len(this.item) // 孩子结点 for 2*k+1 \u0026lt; n { child := 2*k + 1 if child+1 \u0026lt; n \u0026amp;\u0026amp; this.item[child] \u0026lt; this.item[child+1] { child = child + 1 } // 没必要交换 if this.item[k] \u0026gt;= this.item[child] { break } // 交换两个元素 this.item[k], this.item[child] = this.item[child], this.item[k] k = child } } 上面这段代码的意思是，给定一个下标，让这个下标的元素向下交换，从而满足堆结构。然后就可以开始进行堆化的操作了\nheapify方法 1 2 3 4 5 6 7 func (this *MaxHeap) heapify() { // 从非叶子结点开始 n := len(this.item) for i := (n - 1) / 2; i \u0026gt;= 0; i-- { this.sink(i) } } 经过此次操作，我们就已经有了堆结构，item切片的第一个元素就是数组中的最大值。\nswim方法 但是，到目前为止，我们还没有让这个堆实现动态数据的添加和实现，当新增一个数据的时候，我们就把它添加到切片的结尾末尾，此时，这个新增的结点很可能会破坏掉原来的堆结构，这个时候，我们应该给新添加进来的元素找到合适的位置，swim方法就可以很好的实现。\n1 2 3 4 5 6 7 func (this *MaxHeap) swim(k int) { for k \u0026gt; 0 \u0026amp;\u0026amp; this.item[(k-1)/2] \u0026lt; this.item[k] { // 父节点比自己小 this.item[(k-1)/2], this.item[k] = this.item[k], this.item[(k-1)/2] k = (k - 1) / 2 // 注意这里啊 } } 添加与删除 添加一个元素时，需要动态的调整位置\n1 2 3 4 func (this *MaxHeap) insert(val int) { this.item = append(this.item, val) this.swim(len(this.item) - 1) } 删除一个元素的时候(一般是根节点)，这里有一个trick，根节点与最后一个结点进行交换，然后再把新的结点sink使得找到合适的位置，这样做是因为，数组不适合频繁的添加删除元素，这样做可以更加高效！\n1 2 3 4 5 6 7 8 9 10 func (this *MaxHeap) delMax() int { ans := this.item[0] this.item[0], this.item[len(this.item)-1] = this.item[len(this.item)-1], this.item[0] this.item = this.item[:len(this.item)-1] // 删除 this.sink(0) return ans } func (this *MaxHeap) getMax() int { return this.item[0] } 到此，我们就成功的实现了一个大顶堆，小顶堆的实现与之很相似，这里直接贴出代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 type MinHeap struct { item []int } func (this *MinHeap) heapify(nums []int) { this.item = nums n := len(this.item) for i := (n - 1) / 2; i \u0026gt;= 0; i-- { this.sink(i) } } func (this *MinHeap) sink(k int) { n := len(this.item) for 2*k+1 \u0026lt; n { child := 2*k + 1 if child+1 \u0026lt; n \u0026amp;\u0026amp; this.item[child+1] \u0026lt; this.item[child] { child = child + 1 } if this.item[k] \u0026lt;= this.item[child] { break } this.item[k], this.item[child] = this.item[child], this.item[k] k = child } } func (this *MinHeap) swim(k int) { for k \u0026gt; 0 \u0026amp;\u0026amp; this.item[(k-1)/2] \u0026gt; this.item[k] { this.item[k], this.item[(k-1)/2] = this.item[(k-1)/2], this.item[k] k = (k - 1) / 2 } } func (this *MinHeap) genLength() int { return len(this.item) } func (this *MinHeap) insert(val int) { this.item = append(this.item, val) this.swim(len(this.item) - 1) } 拓展：堆排序 在我们刚才构建大顶堆的时候，注意到每次都可以以 $O(logK)$的时间复杂度调整并获取最大值，所以，对于n组数来说，我们就可以以$O(nlogK)$的复杂度来进行排序，其中$K$是树的高度\n这里是完整代码，需要注意交换的范围每次都是在缩小的\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 package main import ( \u0026#34;fmt\u0026#34; ) // ConstructMaxHeap 将输入数组转换为最大堆。 func ConstructMaxHeap(nums []int) { n := len(nums) for i := n/2 - 1; i \u0026gt;= 0; i-- { sink(nums, i, n) } } // sink 是辅助函数，用于下沉元素以保持最大堆性质。 func sink(nums []int, k int, n int) { for 2*k+1 \u0026lt; n { child := 2*k + 1 if child+1 \u0026lt; n \u0026amp;\u0026amp; nums[child] \u0026lt; nums[child+1] { child = child + 1 } if nums[k] \u0026gt;= nums[child] { break } nums[k], nums[child] = nums[child], nums[k] k = child } } // heapSort 使用堆排序算法对输入的切片进行排序。 func heapSort(nums []int) { ConstructMaxHeap(nums) n := len(nums) for i := n - 1; i \u0026gt; 0; i-- { nums[i], nums[0] = nums[0], nums[i] // 交换最大元素到正确位置 sink(nums, 0, i) // 重新构建堆 } } func main() { nums := []int{3, 2, 1, 5, 6, 4} fmt.Println(\u0026#34;Original array:\u0026#34;, nums) heapSort(nums) fmt.Println(\u0026#34;Sorted array:\u0026#34;, nums) } ","date":"2024-10-19T19:47:58+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E5%A0%86/","title":"数据结构之堆"}]