[{"content":"前言 神经网络一个非常关键的地方就在于如何能够更快、更精确的求解出各种参数，这些参数一般是在学习的过程中可以得到，而有一些参数却需要人为的根据经验来进行初始化，例如学习率的大小、每次训练时batch size的大小、损失函数的选择以及激活函数的选择。下面来记录一下如何选择这些参数\n激活函数 激活函数在神经网络起到的是引入非线性的作用，当我们不选择激活函数的时候，实际上并没有增加有效层的层数，而激活函数又有很多种选择，早期的激活函数有sigmoid、tanh函数，而我们用的较多的有ReLu以及ReLu的变体\n类sigmoid函数 sigmoid函数在早期十分受欢迎，函数的值域在[0,1]中，这个函数在早期受欢迎的原因是它很好的模拟了神经元接受刺激产生冲动的一个过程，但是在实践中它有着很多的缺点\n饱和区 当x的取值变得很大或者很小时，其值趋近于1或者0，从图中可以近似估计一下，可以看到输出$\\sigma(x)$对于x的梯度近似为0，那么在使用反向传播链式求导时，很容易将上游梯度的结果变为一个非常小的值，也就是所谓的kill gradients，导致梯度不能通过反向传播而传递至前一层。\nNot zero centered 从图中看出，sigmoid函数不是关于原点分布的，这就会导致在计算参数W的梯度依赖于上一层的梯度，例如，给定两个参数W1和W2，对于输出来说 $$ f = X_1W_1 + X_2W_2 $$ 可以使用链式法则求得这两个参数的梯度 $$ \\frac{dL}{dW_i} = \\frac{dL}{df} \\frac{df}{dW_i} $$ 也就是 $$ \\frac{dL}{dW_i} = \\frac{dL}{df} X_i $$ 而因为$X_i$恒为正(来自sigmoid的输出)，所以$\\frac{dL}{dW_i}$的符号只取决于上游梯度，那么对于$W_1和W_2$这两个梯度来说，其符号要么同时为正，要么同时为负，所以从图上看就是这样的\n这就使得网络很难训练\n计算代价高 显然，指数级别的计算代价要明显高于一般运算\ntanh 这里也介绍了tanh函数，从图像上来说，与sigmoid相比，tanh只是少了not-zero-centered这个缺点\n类ReLu ReLu的激活函数是 $$ ReLu(x) = max(0, x) $$ 选择这个激活函数也是AlexNet一个创新点之一，与之前的类sigmoid函数相比，ReLu简单、收敛快、不存在饱和梯度，但是这个函数也是not zero centered，而且也有着其它的缺点\ndead ReLu 当输入X小于0时，ReLu只是简单的去把X设置为0，导致小于0的部分的梯度永远也不会去更新，当输入 $x≤0$，输出总是 0。因此，如果一个神经元的输入权重和偏置的组合导致它始终进入负区,该神经元在整个训练过程中都不会被激活，也不会对学习产生贡献。也就是该神经元是dead的，并不会增强模型的能力。\n为解决这个问题，研究人员又提出了ReLu的变体\nLeaky ReLu Leaky ReLu的思路是小于0的部分不是简单的设置为0，而是设置为一个很小很小的数\n把设置的这个很小的数叫做$\\alpha$，在反向传播中可以学习这个参数\n还有其它的各种变种\n总结 规则怪谈：\n当你不知道使用什么激活函数，或者真的不在乎那0.1%的精度提升，直接选择ReLu 当你需要极值的优化，那么可以尝试一下ReLu的变体 不要使用tanh和sigmoid 数据处理 对数据进行预处理可以更好的训练网络。\n下图是一个对原始数据进行0-1正态分布的处理过程\n这里有一个值得注意的代码细节\n1 2 X -= np.mean(X, axis=0) X /= np.std(X, axis=0) 这里对数据进行处理都是在同一类的数据进行处理，而在数据集里面按照一般约定，每一列是一个类的所有数据分布，所以这里是在axis=0(torch里面使用的是dim)上进行数据处理。\n对数据进行预处理可以使得在反向传播时更容易求解梯度和传播梯度\n对于图像来说，可以减去图片的均值、减去每个通道上的均值以及在每个通道上做正态分布初始化\n权重参数初始化 到目前为止，权重参数W一直是一个非常重要的参数，而且权重的初始化也是训练网络很重要的一部分，一个想法是，假设我们有着一个比较简单的网络，如果我们把权重参数全部初始化为0\n1 W = torch.zeros(N, D) 那么在前向传播的过程中，每一层的输入就都是0，那么这个网络实际上什么都做不了，一个比较常见的做法是把W按照高斯分布进行初始化\n1 W = torch.randn(N, D) * weight_scale 例如，weight_scale可以初始化为0.01\n1 W = torch.randn(N, D) * 0.01 这样初始化在网络层数比较小的时候没什么问题，但当网络层数非常多时，后面层获得的输入就会非常非常小，以至于无法表示\nXavier 初始化 对于激活函数是tanh时，xavier激活函数可以很好的结果这个问题，这个方法的核心思想在于把输入和输出的分布尽可能相似，也就是输入的方差与输出的方差一致。\n对于输出来说 $$ y_i = \\sum_{i = 0} ^ D X_iW_i $$ 为了使两者方差相等，即 $$ Var(y_i) = \\sum_{i=0} ^ D Var(X_iW_i) $$ 因为输入X的方差是1，即 $$ Var(y_i) = D*Var(W_i) $$ 所以$W_i$的方差就是原来的$\\frac{1}{D}$,只需要在原来初始化时除以输入维度数即可\n1 W = torch.randn(Din, Dout) / torch.sqrt(D) # 注意这里是方差 kaiming初始化 也叫做He初始化，这个初始化方法是专门为ReLu实现的，回想一下ReLu函数，在ReLu函数作用下，对于0-1分布来说，每次产生非0的概率就是0.5,所以对于这组数据来说，每次方差都要缩小一半。\n同样的，为了使输入和输出的分布近似相等，所以可以推导出在ReLu函数作用下的初始为\n1 W = torch.randn(Din, Dout) / torch.sqrt(2 / Din) 同样的，对于ResNet来说，整个初始化就是这样的\n这样可以保证在两个卷积层输出后方差不变。\n正则化 正则化技术是防止模型过拟合的一个关键技术，正则化可以从某种程度上减少模型的复杂度。\n在一开始，对于损失函数，我们讨论了L1正则化和L2正则化这两种简单有效的正则化方法\n此外老师还介绍了一种正则化方法DropOut,DropOut一般用于全连接层的优化，对于一些神经元的输出，DropOut会按照P的概率把这些神经元的输出置为0，其结果就像是在复杂的网络中选择一些简单的子网络\n一样，从而降低模型的复杂度。\n为了保证这两个模型依旧是等价的，我们把未丢弃的那些值都除以p，这样可以保证在DropOut前后两者均值相同。\n数据增广 数据增广的想法可能是更好的去模拟人类的思维，对于一张图片来说，我们可以对这张图片进行裁剪、旋转、增强亮度等操作，对于人类来说，即使经过这些操作，也还是很容易就可以辨别出这是同一张图片，这恰恰也是我们对机器也可以实现的能力。\n除此之外，在数据集较小、数据集图片质量不佳时，我们就可以人为的对数据进行一些操作，从而达到训练要求。\n总结 这次老师分享了一些训练网络时的一些技巧，包括激活函数的选择、数据预处理的重要性、权重参数初始化的方法、正则化以及数据增广的办法，这些都会在作业中用到！\n","date":"2024-12-07T12:30:50+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Cpart1/","title":"训练神经网络part1"},{"content":"前言 全连接神经网络面临的问题 在此之前，我们一直是在全连接层神经网络进行讨论，全连接神经网络其实也有许多不便之处\n无法理解图像模板\n对于之前的所有的有关图像分类实现的任务，对于给定的图片(size:3x32x32)，我们并不考虑图片整体或者图片的一些局部特征是什么样的，而是直接把3x32x32的图片展平为一个一维向量(1x3072)，然后经过一些矩阵乘法，我们就可以得到这个图片的scores\n内存问题\n3072维的向量似乎还是可以接受的，但是实际上这个图片是非常小的，假设我们使用了一些比较大的图片，那么输入的维度肯定会显著的提升。在进行神经网络全连接后，显然，每一层上都有着巨量的计算，而且为了有更好更复杂的模型，神经网络的层数也会增加，那么在此产生的计算和内存消耗将是巨大的\n卷积神经网络 卷积层 卷积神经网络的思路与全连接不同，卷积神经网络更加“尊重”图像的样子，它不会把输入图像进行压缩，而是在乎图像的整体特征，而保留整体特征的一个关键部分就是卷积核\n卷积核 卷积核可以保留提取一些图片中相似的特征，只要你了解卷积核是怎么提取图片的特征的，你就会明白为什么\n卷积核的计算\n卷积核会在原来输入图片的维度上进行计算，假设我们使用的卷积核是3x5x5的\n对于图像中每个5x5的部分，我们使用点积去进行计算\n多个卷积核的使用\n在经过卷积核的计算后，我们得到了一个1x28x28的输出(计算公式后文给出)，更一般的，我们会使用多个卷积核进行卷积，此时卷积核的维度就是Nx3x5x5，那么输出就会是Nx28x28的\n更一般的，对于一批数据，我们会对着一批数据进行卷积操作，此时就变为\n下面是其公式表达形式\n卷积层的堆叠\n为了搭建更加复杂的网络，通常会堆积多个卷积层(像之前两层网络一样，在一定范围内层数越多越复杂，能力越高)，此时注意，为了确保引入多个卷积层而不是只是一个卷积层，这里也引入了非线性激活函数(ReLu)\n输出大小 卷积的过程就是在原始图片上进行滑动相乘的结果，当使用5x5的卷积核时，输入图片大小为32x32，那么在输入的长上的最后一次运算就是第28, 29, 30, 31 ,32的格子上；同样对于宽也是一样的，这样就可以得到输出图像的特征就是28x28\n更一般的，对于输入特征W来说，我们使用大小为K的卷积核进行运算，那么输出特征就是**W-K+1**的，利用这个公式，我们就可以很方便的计算输入特征为32x32时，卷积核大小为5x5时的输出特征。\n带来的新问题 经过5x5的卷积核运算后，一个肉眼可见的差别就是输出图像与原始输入图像维数是不同的，当我们使用很多个卷积核后，输出的图像看起来像是降维一样。\n为了保持输出图像与输入图像的维度(可能是减少损失的信息)，我们可以使用填充输入图像的方式来保证维度。\n需要填充0的层数p就是(k-1)/2，经过填充，输出的图像大小与输入就有着相同的维度。\n不同的步长 在上面卷积核进行计算的时候，我们都是默认每次卷积的时候都是挨个滑动，事实上滑动的时候也可以跳跃着滑动，也就是说，在原先的基础上，如果每次都是N步去滑动卷积，那么输出的维度就会减小N倍，这个时候我们可以更新我们的计算公式\n池化层 池化分为平均池化层和最大池化层，例如，当使用大小为2x2的卷积核进行最大池化的时候，实际上就是对卷积后的每层输出进行特征筛选\n上图是一个使用最大层池化，步长为2的池化层，因为步长的原因，池化后的图片大小会发生变化，使用平均池化也是一样的原理，只需要计算2x2内的均值即可\n前向传播 对于给定的数据，怎么实现前线传播的计算呢？假设下面是给定的数据\nx, 大小为NxCxHxW，N代表这一批的数据，C表示图片的通道数，HxW是图片的大小 w，大小为FxCxHHxWW，有F个卷积核，每个卷积核大小都是CxHHxWW b，大小为哦F，表示偏执 现在思考怎么计算经过卷积后的数据out\n卷积的过程 首先，我们可以拿出一张图片来进行举例，那么这张图片就是x[i, :, :, :],同时，我们也拿出一个卷积核w[i, :, :, :]。回想一下老师在课堂上举过的例子\n这个输出是怎么得到的呢？因为图片是三通道的，所以在每一个通道上都要进行卷积操作，实际上得到的最后输出就是三通道上的总和，那么，对于图片来说，其计算过程就是这样的\n1 2 3 4 5 6 7 Hout, Wout = out.shape # 得到输出的size # 假设步长是stride for i in range(Hout): for j in range(Wout): # 这里假设输出和x有着同样size # 三个通道的总和 output[i, j] = (x[:, i:i+k, j:j+k] * w).sum() + b 更一般的，当我们可以进行推广\n1 2 3 4 5 6 7 8 9 10 11 # 输出图片的维度与步长stride和输入的填充有关 N,C,H,W = x.shape F,C,HH,WW = w.shape H_out = 1 + (H + 2 * pad - HH) // stride W_out = 1 + (W + 2 * pad - WW) // stride for n in range(N): for f in range(F): for i in range(H_out): for j in range(W_out): # 计算第n个数据在第f个卷积核上的输出 out[n, f, i, j] = (x[n, :, i, j] * w[f]).sum() + b[f] # 三通道相乘的和 反向传播求梯度 反向传播求梯度这里其实有点绕，假设我们已经知道损失函数对于输出的梯度dout，那么我们就可以使用链式法则进行求导\nout[0, 0, :, :]代表着第1个图片在第一个卷积核上的输出，我们来分析一下它是由哪些部分计算得到的 $$ out[0, 0, :, :] = x[0, 0, :, :] * w[0, 0, :, :] + x[0, 1, :, :] * w[0, 1, :, :] + x[0, 2, :, :] * w[0, 2, :, :] + bias[0] $$ 现在把卷积核的个数拓展到F个，那么求第f个卷积核的输出就是 $$ out[0, f, :, :] = x[0, 0, :, :] * w[f, 0, :, :] + x[0, 1, :, :] * w[f, 1, :, :] + x[0, 2, :, :] * w[f, 2, :, :] + bias[f] $$ 转换为代码就是\n1 out[0, f, :, :] = np.sum(x[0, :, :, :] * w[f, :, :, :], axis=0) + bias[f] 需要注意的是，要考虑的是，我们需要学习的参数在哪里参与运算，分层求解其梯度即可。\n正则化技术 当网络变得很深的时候，网络通常会变得难以训练，这是因为在我们依赖的方法上的缺点，对于一个很深的网络来说，当前层的梯度来自与损失函数对参数的梯度，而梯度在流动的时候又是依赖于链式法则和反向传播，因此，假设上游梯度：\n所有的梯度都是小于1的数\n那么在反向传播相乘的时候，很有可能出现梯度消失，即数值过小(nan)\n当梯度全部都是大于1的数\n那么在反向传播的过程中，多个大于1的数也会导致数值过大而溢出\n正则化的作用 why it works?，正则化就是调整数据之间的分布，例如，假设 $$ Y = X_1W_1 + X_2W_2 $$ 而不幸的是，X1可能是一些房屋面积的数据，例如100平米，150平米，而X2有可能是附近的医院个数，例如10,15，显然，两者数据差别有点大，我们可以假设损失函数对于各个数据之间的梯度图，例如\n其中X1是数值比较大的数，X2是数值比较小的数，中间的星号就是损失函数最小时X1和X2的取值，可以看到，整个图像的分布就像是一个椭圆形数据，对于X1来说，每次的变化值都要大于X2的变化值，这就导致了每次在梯度下降的时候，很有可能导致下降时候的振荡和无法收敛。\n使用正则化之后，可以把数据进行归一化操作，这样对于整个数据分布来说，其形状会更加接近于圆型，这样在梯度下降的时候，对于一些参数learning_rate就可以调整的比较大，同时也可以加快模型的收敛，减少训练时间。\n同时，这里的正则化技术有很多，例如batch normalization, layer normalization, xxx normalization，感兴趣的可以自行查阅\n批正则化处理的技术一般放在全连接层之后或者卷积层之后，而且是放在非线性激活函数之前\n总结 卷积神经网络的组成一般包括：卷积层、池化层和全连接层，其中每个层里面又有各种各样的细节，下次来看一看怎么更好的训练一个网络\n","date":"2024-11-25T17:12:53+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","title":"卷积神经网络"},{"content":"前言 这个问题似乎是之前没有接触到过的问题，题目的大概意思就是，给定一个数组，数组中可能会有重复的元素，现在我们的任务是\n调整元素，使得区间内没有重复的元素 调整后的区间和最大 贪心算法 例如， 一个数组是\narr = [1,2,3,4,5,5,6]\n怎样调整使得数组和最大且元素不重复呢？\n贪心的思路 从贪心思路出发， 只要每个元素都是最大的， 那么整体和就是最大的，所以，我们不去调整最大值，而是保留下来数组里面的最大值\n解题 先将数组进行排序\n1 arr.sort(reverse= True) 然后开始考虑挨个调整\n将当前元素与该元素前面一个元素进行比较\n因为有重复的元素，所以要考虑最大值有重复的情况\n例如\n1 arr = [6,6,6,5] arr[i]来说， 为了调整最大， 那么我们就需要将arr[i]和arr[i-1]-1做比较， 取这两个值之间的最小值\n如果与arr[i] = arr[i-1], 那么我们就可以把arr[i]调整为在贪心策略下的最大值 如果不相等，因为我们是经过排序的， 所以这个值的本身就是可以取到的最大值 例题 LeetcCode945.使数组唯一的最小增量\n给你一个整数数组 nums 。每次 move 操作将会选择任意一个满足 0 \u0026lt;= i \u0026lt; nums.length 的下标 i，并将 nums[i] 递增 1。\n返回使 nums 中的每个值都变成唯一的所需要的最少操作次数。\nLeetcode 1647. 字符频次唯一的最小删除次数\n如果字符串 s 中 不存在 两个不同字符 频次 相同的情况，就称 s 是 优质字符串 。\n给你一个字符串 s，返回使 s 成为 优质字符串 需要删除的 最小 字符数。\n字符串中字符的 频次 是该字符在字符串中的出现次数。例如，在字符串 \u0026quot;aab\u0026quot; 中，'a' 的频次是 2，而 'b' 的频次是 1 。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class Solution: def minDeletions(self, s: str) -\u0026gt; int: # 从大到小排序 # arr[i-1] = min(arr[i-1], arr[i]-1) temp = [0] * 26 for c in s: temp[ord(c)-ord(\u0026#39;a\u0026#39;)] += 1 # 排序 temp.sort(reverse=True) ans = 0 for i in range(1, len(temp)): a = temp[i] temp[i] = min(temp[i-1]-1, temp[i]) if temp[i] \u0026lt;= 0: temp[i] = 0 ans += a - temp[i] return ans ","date":"2024-11-24T17:24:18+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E5%8C%BA%E9%97%B4%E5%92%8C%E6%9C%80%E5%A4%A7%E4%B8%94%E5%85%83%E7%B4%A0%E4%B8%8D%E9%87%8D%E5%A4%8D/","title":"区间和最大且元素不重复"},{"content":"前言 前面讲解了一些优化算法，尤其是各种梯度下降算法，这次来看一看神经网络\n神经网络 神经网络的特点 前面我们在学习线性分类器的时候了解到，线性分类器对于异或、圆形、半圆形数据不能很好的划分出一条边界，这也导致了线性分类器不是那么的有效，而神经网络可以解决这个问题。\n一般的，神经网络可以划分为输入层、隐藏层、输出层这三种结构，而正是隐藏层的一些非线性特征使得神经网络可以拟合出各种决策边界，所以在线性分类器上解决不了的问题便可以使用神经网络很好的解决。\n为了简单起见，作业里面实现的是一个两层的神经网络，使用的激活函数是Relu激活函数\n得分方式的改变 在之前的线性分类中，我们把 $ X^T W $看作是一个得分的输出。在神经网络里面这里的计算方式也与其计算方式相同，不同的是，在多层神经网络之间传递上一层的分数时，总是要经过非线性激活函数输出后把分数传递到下一层，这是因为如果不加激活函数，那么实际上我们在做乘法的时候还是取得是一个线性计算的过程，所以要加上激活函数，从而引入非线性。\n全连接神经网络也叫做多层感知机\n因此，计算得分的方式可能会是\n1 2 3 4 5 6 7 import numpy as np h1 = X.dot(W1) # 得到第一层的分数 # 执行Relu, 即 h1 = max(0, h1), 只保留大于0的部分 h1[h1 \u0026lt; 0] = 0 # 经过激活函数后输出到下一层 scores = h1.dot(W2) # 得到分数 上文已提到，不加激活函数实际上做的还是线性变换\n可以看到，经过合并后，不加激活函数的结果等价于一个线性分类\n激活函数 激活函数的存在就是为了引入非线性，从而可以划分出非线性的决策边界，下面是一些激活函数\n简单的实现 ppt中给出了一个简单的使用MSE作为损失函数的两层神经网络\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 import numpy as np # N是训练的batch size; D_in 是input输入数据的维度; # H是隐藏层的节点数; D_out 输出的维度，即输出节点数. N, D_in, H, D_out = 64, 1000, 100, 10 # 创建输入、输出数据 x = np.random.randn(N, D_in) #（64，1000） y = np.random.randn(N, D_out) #（64，10）可以看成是一个10分类问题 # 权值初始化 w1 = np.random.randn(D_in, H) #(1000,100),即输入层到隐藏层的权重 w2 = np.random.randn(H, D_out) #(100,10),即隐藏层到输出层的权重 learning_rate = 1e-6 #学习率 for t in range(500): # 第一步：数据的前向传播，计算预测值p_pred h = x.dot(w1) h_relu = np.maximum(h, 0) y_pred = h_relu.dot(w2) # 第二步：计算计算预测值p_pred与真实值的误差 loss = np.square(y_pred - y).sum() print(t, loss) # 第三步：反向传播误差，更新两个权值矩阵 grad_y_pred = 2.0 * (y_pred - y) grad_w2 = h_relu.T.dot(grad_y_pred) grad_h_relu = grad_y_pred.dot(w2.T) grad_h = grad_h_relu.copy() grad_h[h \u0026lt; 0] = 0 grad_w1 = x.T.dot(grad_h) # 梯度下降法 w1 -= learning_rate * grad_w1 w2 -= learning_rate * grad_w2 这里比较有意思的地方是如何去更新我们的权重矩阵W1,W2\n反向传播 求得W1和W2得梯度便可以使用梯度下降法去进行跟新，那么怎么求这两个函数得梯度呢，答案就是去使用反向传播算法。\n反向传播算法的核心就是去利用链式求导法则，对于两层或者更多层的神经网络来说，直接求得损失函数对于权重的梯度是一件不太好实现的事情，实际上ppt里面讲解的就是链式求导法则，为了更好的理解链式求导，这里以损失函数为交叉熵函数实现的多分类问题来进行记录。\n链式求导 上面构建了一个简单的二层网络，这个网络的工作流程是这样的\n计算得分\n与之前的线性网络一致，对于输入$X$来说，输出的得分就是 $$ scores = XW_1 + b_1 $$ 不同的是，为了拟合出更多的非线性边界，这里的得分还需要向第二层输出\n激活函数引入非线性\n假设我们的激活函数为$ReLu$函数，那么 $$ Z(x) = \\left{ \\begin{aligned} x , x \u0026gt;= 0\\ 0, else \\end{aligned} \\right. $$ 也就是隐藏层h1的输出就是$Z(scores)$\n经过隐藏层输输入后，我们可以把计算第二层的结果看作之前的线性分类器 即 $$ output = Z(scores)W_2 + b_2 $$ 得到这个output后，可以把结果转为softmax，也就是 $$ y_{pred} = argmax[softmax(output)] $$ 这样就可以使用交叉熵损失函数计算损失\n梯度求解 需要额外注意的是，W的梯度dW是在损失函数中学习到的，我们更新W的意义就是去最小化损失函数，最小化损失函数也就是意味着我们的预测越准确，模型所产生的误差越小。\n对于一个单层或者多层网络来说，其输入输出、求导方式都是很相似的，下面是一般求解步骤\n求得损失函数对输出的梯度dout\n在常见的一些损失函数如MSE均值、softmax交叉熵等，可以求得其关于输出的导数，即求得$\\frac{dL}{dout}$\n求得输出关于输入的梯度\n对于输出来说，一层网络的输出就是 $$ output = XW + b $$ 所以，对于，根据链式求导法则，我们就可以很容易的求出损失函数关于输入的梯度\n在使用激活函数后，即output其实并不是原始的输出，而是经过激活函数处理后的输出，这也就意味着中间又多了一层关于激活函数的导数，我们以ReLu激活函数为例\n一般的，如果不加激活函数，那么我们的求导过程可能是这样的\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import torch # 假设我们已经知道损失函数关于输出的梯度 def backward(dout): \u0026#39;\u0026#39;\u0026#39; Inputs: - dout: Upstream derivative, of shape (N, M) - x: Input data, of shape (N, D) - w: Weights, of shape (D, M) - b: Biases, of shape (M,) \u0026#39;\u0026#39;\u0026#39; # 根据求导公式 dX = dout.mm(W.T) dW = x.T.mm(dout) db = dout.sum(dim = 0) return dX, dW, db 如果在输出层多加了激活函数，那么只需要再多计算一次乘积即可\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import torch def backward(dout): \u0026#39;\u0026#39;\u0026#39; Inputs: - dout: Upstream derivative, of shape (N, M) - x: Input data, of shape (N, D) - w: Weights, of shape (D, M) - b: Biases, of shape (M,) \u0026#39;\u0026#39;\u0026#39; # 计算dW的梯度 dW = x.T.mm(dout) # 注意，是由输出大于0的部分才有梯度，所以需要进行保留 dW[out \u0026lt; 0] = 0 更一般的，我们会直接对ReLu(x)做求导，从而当输入x发生变化时，我们的ReLu依旧会更加模块化\n作业 two_layer_net 讲解一下这个作业中较难的部分\n实现forward_pass 可以从函数的参数里面得到需要的参数， 例如W1, b1, W2, b2\n1 2 3 4 # Unpack variables from the params dictionary W1, b1 = params[\u0026#39;W1\u0026#39;], params[\u0026#39;b1\u0026#39;] W2, b2 = params[\u0026#39;W2\u0026#39;], params[\u0026#39;b2\u0026#39;] N, D = X.shape 需要额外注意的是这些参数的形状， 我们的训练数据X是NxD的，也就是说，这个训练集中有N个样本，每个样本都是简单的1xD向量，作业为了防止我们出错，还贴心的在注释里面给出了这些参数的形状\n1 2 3 4 5 6 7 8 \u0026#39;\u0026#39;\u0026#39; It should have following keys with shape W1: First layer weights; has shape (D, H) b1: First layer biases; has shape (H,) W2: Second layer weights; has shape (H, C) b2: Second layer biases; has shape (C,) - X: Input data of shape (N, D). Each X[i] is a training sample. \u0026#39;\u0026#39;\u0026#39; 根据这个注释，我们在做矩阵乘法的时候就特别方便\n1 2 3 4 # 第一层的输出 hidden = X.mm(W1) + b1 # 经过非线性激活函数 hidden[hidden \u0026lt; 0] = 0 此时，我们就得到了这个二层网络的隐藏层分数\n因此，计算输出的总分也是很简单\n1 2 # 未经softmax函数处理 scores = hidden.mm(W2) + b2 # raw_scores 到现在，我们就得到了网络的输出分数，现在让我们来梳理一下从图片到预测之间的流程\n3x32x32数据集\n我们把原始数据集展平为一个一维向量，把若干个这样的向量堆叠在一起，这样就得到了训练集X\n计算隐藏层输出\n与线性分类器计算分数一样，做乘法运算即可\n激活函数\n引入非线性，如ReLu, Sigmoid函数\n输出层\n得到隐藏层分数后计算输出层分数即可\nsoftmax得到概率\n我们把输出的scores经过softmax后得到近似概率分布，然后概率最高的就是我们网络将图片分类的结果\n交叉熵损失函数优化\n使用交叉熵函数优化，从而得到之前的W1, b1, W2, b2的梯度，并使用梯度下降法进行学习\n也就是说， 在forward_pass中，我们还剩最后两个步骤没有计算出来，下面我们将在nn_forward_backward中计算得出\nforward_backward 要想得到损失函数关于W1, b1, W2, b2的梯度， 我们得先求的损失函数，这里使用的是交叉熵损失函数，也就是说，我们需要求得softmax后的分数\nsoftmax过程\n这部分在A1中已经计算过，在这里在此计算一次。首先根据定义，其实就是每部分exp后除以总的exp和即可。我们的输出scores是一个NxC的矩阵，每一行(dim=1)的含义就是第i个样本(1\u0026lt;=i \u0026lt;=N)在10个类上的总分。例如，假如第i个样本在10个类中cat的分数最大，那么经过softmax后可以近似认为第i个样本是cat的概率最大\n1 2 3 4 5 6 7 8 9 10 11 12 # 从前向传播中得到分数,注意，这个分数其实是raw_scores scores, h1 = nn_forward_pass(params, X) # 得到分数后softmax化 # 得到每个类别的最大值 max_val, _ = torch.max(scores, dim=1) # 函数返回最大值和最大值的索引 # 除去最大值是防止exp值过大，同时不影响结果 scores_remove_max = scores - max_val.view(-1, 1) # 使用广播机制，不使用也可以 # scores_remove_max = scores - torch.max(scores, dim=1, keepdim=True).values # exp化 scores_exp = torch.exp(scores_remove_max) # 概率化 scores_prob = scores_exp / torch.sum(scores_exp, dim=1).view(-1, 1) # 不使用广播机制同上 链式法则\ndW2和db2\n在求得softmax化后的结果后，我们需要以损失函数的形式表达出来整个解，这里的损失函数是交叉熵损失函数，为了求得损失函数对W2的梯度，使用链式法则会更加简单清晰\n交叉熵损失 $$ Loss= -\\frac{1}{N}∑log(p_i)+reg⋅(∥W1∥^2+∥W2∥^2) $$ 这里的pi是预测值，也就是我们上面的softmax值，现在，我们可以把求解过程转换一下,即\n$$ \\frac{dL}{dW_2} = \\frac{dL}{dP} \\frac{dP}{dS} \\frac{dS}{dW_2} $$\n我们可以来挖掘一下Scores与W2的关系，显然有\n$$ Scores = h_1^T * W_2 + b_2 $$\n怎么求第一项的梯度呢？\n$ \\frac{dL}{dP} $的计算公式其实就是对数函数求导，而$\\frac{dP}{dS}$的结果就要从softmax公式出发\n$$ softmax(i) = \\frac{e^{scores_i}}{e^{scores}} $$\n这个时候就要分当前预测类的类别的情况了，因为对于$p_i$来说，每次都要计算两部分梯度，当计算类别正确时，也就是softmax公式的分子上是含有$e^y_i$，那么此时分子分母都是含有要求导部分；当求其它梯度时，分子上其实就是个常数，求导法则发生了变化。这里推荐一个视频,可能会帮助更好的理解。\n也就是说，对于这部分梯度来说，正确的类别结果-1(正确类别分子上还有求导到部分)，错误类别不需要-1，，而且对这部分求导是因为分母上有需要求导部分。\n而且，每个标签都是One-Hot格式，这样我们就可以求得$\\frac{dP}{dS}$\n所以求得$ \\frac{dL}{dS}$\n1 2 3 ds = scores_prob.clone() # NxC ds = ds[range(N), y] -= 1 ds /= N # 注意不要遗漏 $\\frac{dS}{dW}$ = h1(NxH)\n1 dW2 = h1.T.mm(ds) # HxC 同理也可以求得db2就是 ds\ndW1和db1\n这里同样使用的是链式法则\n$$ h1 = ReLu(XW_1+b_1) \\ Scores = h_1W_2 + b_2 $$\n所以要求 $$ \\frac{dL}{dW_1} = \\frac{dL}{dS} \\frac{dS}{dh1} \\frac{dh1}{dW_1} $$ 现在未知参数就是dh1,需要注意的是，因为是ReLu所以小于0的部分会置0\n1 2 dh1 = d_scores.mm(W2.T) dh1[h1 \u0026lt;= 0] = 0 # 小于等于0的不贡献梯度 这里不清晰的化还可以再加一部分即\n$$ \\frac{dh1}{dW_1} = X , h1 \u0026gt;= 0 $$\n现在，链式求导的部分我们就求解完了，也是这次作业最难的一部分。\n总结 多层感知机成功解决了线性分类不能完成的任务，但是多层感知机也有自身上的缺点，下节来看看大名鼎鼎鼎鼎大名的卷积神经网络！\n","date":"2024-11-23T16:48:55+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/","title":"神经网络——多层感知机"},{"content":"前言 继续来看看优化这部分\n梯度下降 优化部分主要讲解了与梯度下降以及梯度下降的各种优化版本\n虾几霸优化 对于评估一个W参数矩阵来说，需要计算出在这个W下的分类准确率即可。这里的”虾几把“的意思就是随机生成一个参数矩阵W，只要这个矩阵的准确率高于上一次计算的准确率，那么就把当前最优的W更新，然后一直模拟下去,一个可能的算法是这样的\n经过这种方法去求得的W在准确率大约在15%，不算太坏，但算不上好！\n梯度下降法 在一元函数中，导数可以理解为在这一点上的斜率，在多元函数中，我们使用梯度这个概念来进行导数的推广，实际上，梯度在每一维上的分量就是我们熟悉的导数\n沿着负梯度的方向就是目标函数下降最快的方向\n因此，对于损失函数来说，我们可以找到W的梯度矩阵dW，然后再对W进行优化,这种方法就是大名鼎鼎的梯度下降\n可以看到，这里我们就有了三个未决的超参数\n怎样初始化W 要迭代寻找多少次(num_steps) 学习率learning_rate 其中非常关键的一个参数就是learning_rate，因为最小化损失函数实际上就是去找到目标函数的极小值，在刚开始进行梯度下降时，初始位置在极小值的左边或者右边。下面用$ f(x) = sin(x) $来模拟一下整个过程\n当学习率很小时\n我们总能找到极值， 但是却要寻找很长时间，这是因为每一步都走的特别小，所以寻找要很长的时间，这里假设学习率是0.1，迭代100次\n当学习率很大时\n学习率很大，这就意味着每一步都走的很大，所以很容易错过最小值，从而造成振荡，下面是学习率为2的情形\n所以，这些参数的选取实际上是在训练神经网络的一些困难之处，而且我们的训练集通常很大，所以每次更换学习率后再训练的代价很大，来说一下这些优化方法！\n小批次计算 Mini Batch 在寻找学习率的时候，我们没必要在整个测试集上进行，而是去选择一批样本进行训练，其实这样做也可以减少内存的压力，一个可行的代码是\n1 2 3 4 5 6 7 import torch # 假设 X_train, y_train num_train = X_train.shape[0] # 得到样本总数 # batch size batch = 32 # 生成随机样本 idx = torch.randint(num_train, size=(batch, )) 这样，在每次训练的时候，我们就可以在小样本上进行迭代训练\n1 2 3 4 5 X_train_batch = X_train[idx] y_train_batch = y_train[idx] ############################ ... ############################ SGD+Momentum 在随机梯度中引入动量的概念，给我们的点增加一个“惯性”的特点\n可以看到， 我们的小球确实像物理中的小球那样，在不断的运动着！一个可能的代码是\n即先计算速度v，再根据速度v梯度下降\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 def f(x): return np.sin(x) def df(x): return np.cos(x) x0 = 1 # 初始化随机x v = x0 # 初始化 beta = 0.9 # 动量值 learning_rate = 0.1 # 学习率 for _ in range(50): # 迭代50次 v = beta * v + (1 - beta) * df(x0) x0 = x0 - learning_rate * v 这个动量的计算公式其实很有意思，它的前身或者本质就是指数加权平均。在使用随机梯度下降时，前面一时刻的梯度似乎不会对后面的梯度造成影响，这就导致随机梯度下降的过程是一个不断震荡的过程，而且很容易陷入局部最小值，而引入指数加权平均时，可以看到，每次梯度的更新都是取决于前面几次的平均值\n$$ v_{t+1} = \\beta * v_t + (1-\\beta)df(x) $$\n当$\\beta$取0.9时，也就是我们会取梯度的一个样本平均(假设样本为10)，这样就把之前计算过的梯度与现在联系在一起，从而避免震荡！\nNesterov Momentum 在ppt中的形式是这样的\nNesterov Momentum的改进思想在于，它在计算梯度之前，先对参数进行一个“预更新”，即朝动量方向提前迈出一步，这样梯度会变得更加准确。\nNesterov Momentum的更新公式为：\n预估下一步的位置：\n$$ \\tilde{\\theta} = \\theta_t + \\gamma v_t $$\n在预估位置上计算梯度：\n$$ v_{t+1}=γv_t−η∇f(θ~) $$\n更新参数：\n$$ θ_{t+1}=θ_t+v_{t+1} $$\n同样的，我们还可以使用这种方法去求sin(x)的极小值\n使用预估的x求梯度\n1 2 3 4 5 6 7 8 9 10 # 计算梯度下降路径 for _ in range(100): # 限定100步 # 预估位置 x_pred = x0 + beta * v # 在预估位置计算梯度 grad = df(x_pred) # 更新动量 v = beta * v - alpha * grad # 更新参数 x0 = x0 + v AdaGrad Adagrad 是一种自适应学习率的优化算法，它根据每个参数在训练过程中的历史梯度大小来调整学习率。对于稀疏特征或特征具有不同重要性的任务（如自然语言处理问题），Adagrad 具有较好的效果。\nAdagrad 的公式如下：\n更新梯度累积历史： $$ G_t=G_{t−1}+∇f(x_t)^2 $$\n这里 $G_t$是梯度平方的累计和（逐元素累加）。\n更新参数： $$ x_{t+1}=x_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\nabla f(x_t) $$\n$\\eta$ 是初始学习率。 $\\epsilon$ 是一个小值（如 $10^{-8}$），用于避免分母为零。 同样，我们来用sin(x)来模拟一下\n可以看到，在这种方法下，“小球”似乎没有它的“物理属性”！\nRMSProp RMSProp是AdaGrad引入指数衰减平均后的优化版本\n接着使用这种方法来求sin(x)的极小值\n有趣的一点是，与AdaGrad采取相同的学习率时，该方法产生了震荡，可能该方法在对学习率的初始值要求较高\nAdam Adma是RMSProp结合了Momentum的版本\n继续来寻找sin(x)的极小值点\n这是老师的经验，超参数的选择是个难点！\n总结 在了解这些优化技巧后，一个不错的建议是：优先使用Mini-batch和Adam优化。\n","date":"2024-11-15T20:20:39+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E4%BC%98%E5%8C%96%E4%B8%8E%E8%AE%AD%E7%BB%83/","title":"优化与训练"},{"content":"前言 好难好难\n线性分类器 线性模型 线性分类器在神经网络中相当于积木的低位\n你可以用很多层的线性分类器来实现一个神经网络，当然，为了提高模型拟合数据的能力，一般不会只去使用线性模型，而是会选择性的加入一些非线性模型\n从线性观点 让我们继续回到上几节课提到的CIFAR10数据集，这个数据集有10个不同的类别。而对于图像的表示，我们可以把图像(input)看作一个数字矩阵，我们想要实现的内容就是，对于一个数字矩阵，能否找到一个权重参数W,使得数据的结果发生一些变化，从而根据这个输出的结果来进行分类判断。\nWx + b是一个非常经典的线性模型\n例如，对于下面这个分类问题\n这里，我们假设输入的图像是一个简单的2x2的矩阵，也就是说，该图像仅仅只由4个特征点决定，那么，对于一个参数矩阵W来说，它的每一行可以看成是一个类别的权重，把这些元素对应相乘就可以得到该类别的\u0026quot;总分数\u0026quot;(例如，cat类在经过参数矩阵运算和bias之后的总分就是-96.8)。\n为了使表示结果更加的整洁易懂，我们可以把这个bias添加到参数矩阵W里面。\n从图像观点 这里的意思就是不把输入的数据拆分为一行，而是直接调整对应元素在图像中的数值\n给人在视觉上的观点就是给整个图像蒙上了一层模板，有着相同背景的图片更容易被划分到同一个类别中，也就意味着，每一类别好像是有了一张模板图片一样\n从几何上看 从几何上看，这些图片会被一个一个的超平面给划分切开，彼此之间没有交集\n线性模型不能解决的问题 线性模型显然不太适合取解决非线性模型。这里列出来一些线性分类器不能解决的问题。\n包括\n一三象限问题\n其实我觉得就是异或问题，对于这类问题，你无法找到一条直线来把两种颜色划分开来\n非线性问题\n对于这类数据一部分是呈现非线性的，除了非线性之外的数据是无法仅通过一条直线划分\n下面的图片很好的表明了这些例子。\n感知机不能学习异或问题！\n当然，如果一层感知机实现不了，那么就再来一层！天无绝人之路！\n损失函数 虽然线性模型很简单，但还是可以给我们许多启发。到目前为止，我们还没有给出一种可以更新权重W的一个方法，于是损失函数便登场了！\n损失函数可以理解为一个定量来分析真实值与我们的预测值之间的偏差的一个方式，按照这种方式，当我们的预测值越接近于真实值，那么我们可以认为，在这种条件下的W是loss友好的。\n另外说一点，在后续提到损失函数时，我们都更加倾向于这个损失函数是凸函数(Convex)，这样我们就可以通过导数解析的方式来求得损失函数最小时的权重W\nSVM损失函数 SVM的损失函数又叫做合页损失函数(hinge loss)，在上文中，我们提到可以使用一个线性分类器来对图像进行分类\n我们可以把一个这个结果S看成是线性变换后的结果，因此，对于每个图片，我们都可以通过这种方式来进行计算，从而得到它的一个分数\n其中，我们很有必要来展开阐述一下这个公式\n这个公式以得分输出的形式其实弱化了X W之间的关系，我们可以使用代码来进行描述\n假设这里有训练集X（X: A PyTorch tensor of shape (N, D) containing a minibatch of data.),并且给出了权重参数矩阵W（W: A PyTorch tensor of shape (D, C) containing weights.）\n也就是说，X的每张图片有D个特征，总共有N个样本；权重W有C个类别，每个类别有D个特征(按照列向量来说，每一个列对应一个图片)，这样，对于每一个样本X[i] (1xD)，我们都可以给这个图片计算出在不同类下面的分数\n1 2 3 4 5 6 7 8 import torch \u0026#39;\u0026#39;\u0026#39; X 是一个 NxD矩阵 W 是一个 DxC矩阵 \u0026#39;\u0026#39;\u0026#39; nums_train = X.shape[0] for i in range(nums_train): socres = W.t().mv(X[i]) # 得到这个图片在所有类上的分数 就像下面这个图片一样\n其中，假设X[0]是cat，那么我们就可以得到这张图片在10个类别上的对于分数;假设X[1]是car，那么我们同样可以得到car在10个类别上的分数，然后就可以得到所有的分数。\n注意一些技巧，我们想要统计的是在C个类上面的分数，所以输出的张量应该是Cx1或者1xC的;torch的mv函数很好的帮助我们将一个矩阵与一个向量相乘，在数学上的感受就是一个CxD的矩阵与一个Dx1的向量相乘，从而得到一个Cx1的输出分数\n再看SVM Loss 再看Loss函数，实际上做的就是一个衡量差值之间间隔的函数，方便起见，我们还是使用只有三个类和三个得分的输出\n怎么计算第一个cat的损失值呢？其实就是对于元素相加减，然后再与0作比较，所以cat的loss就是\n1 2 3 4 max(0, 5.1-3.2+1) + max(0, -1.7-3.2+1) Loss:2.9 同样我们也可以得到其它类的损失函数\n因此，这组数据的平均loss就是 Sum(loss) / x.shape[0]\n同样，我们还是可以使用代码来进行描述\n计算出所有类的分数\n这一步我们已经计算出\n找到对应正确的类，然后做加减法\n得到scores后，我们可以得到这个正确的类的分数。比如，第一张图片是cat，那么我们可以找到scores中是cat的分数是3.2\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import torch \u0026#39;\u0026#39;\u0026#39; X 是一个 NxD矩阵 W 是一个 DxC矩阵 y: A PyTorch tensor of shape (N,) containing training labels; y[i] = c means that X[i] has label c, where 0 \u0026lt;= c \u0026lt; C. y[i] 就是X[i]的标签类 \u0026#39;\u0026#39;\u0026#39; num_train = X.shape[0] num_classes = W.shape[1] for i in range(num_train): socres = W.t().mv(X[i]) # 得到这个图片在所有类上的分数 correct_class_score = scores[y[i]] # 得到所有分数中, X[i]的分数 for j in range(num_classes): if j == y[i]: # 不与自已比较 continue margin = scores[j] - correct_class_score + 1 # note delta = 1 if margin \u0026gt; 0: # 负分数不必相加 loss += margin 这样我们就得到这批样本的总的loss\n平均loss也可以求得\n1 loss /= num_train # 样本的总loss 除以 样本的总数量 矩阵形式 经过上面的铺垫，我们就可以为后面的矩阵求导做铺垫，现在让我们把这个hinge loss的公式展开\n因为我们不与自身作比较，所以，类的输出总分数就是 $$ W_j^{T} * X_i $$ 这个形式的意思就是$ W ^ T$的第j行实际上就是图像X[i]的参数，对应的，减去correct时的分数，而correct的表达可以是 $$ W_{y_i}^T * X_i $$ 然后累加和 $$ L_i = \\sum_{j \\neq y_i} \\max(0, w_j^T x_i - w_{y_i}^T x_i + \\Delta) $$ 我觉得这样写可以更加适合理解后面的梯度求导的形式！\n正则化 为了防止过拟合问题，我们可以给模型的参数W来加入一些惩罚项。\n首先我们来看看什么是过拟合。\n过拟合 以线性回归来举例，对于训练样本中的所有数据，如果我们的模型足够大、足够复杂，那么我们的模型就可以\u0026quot;记住\u0026quot;所有的点，于是，对于一个简单的样本来说，在训练后得到的模型大概是这样的：\n其中，f2是我们预期出现的模型性能的样子，正则化可以防止我们的模型拟合的过好，从而加强模型的预测能力。\nL1 L2正则化 正则化也有着不同的类别，常见的就是L1正则化和L2正则化\n其实正则化的目的就是去把W约束在一定的解的空间内，对于矩阵W来说，越简单的模型就意味着W值的某些取值取得越小，从而拟合出来得出现呈现出一些低阶多项式的形状，当我们把W的解约定在一定的取值内，我们假设这个取值是m，对于L1正则化的那个小尾巴 $ \\lambda R(W) $来说，其W的解 $$ 0 \u0026lt;= W_1 + W_2 + \u0026hellip;. + W_n \u0026lt;= m $$ 在二维空间内，我们可以得到这个解的区域是一个菱形区域(具体的数学证明设计凸优化的知识)\n同样的，对于L2正则化，我们同样要把参数W约束在一个范围内 $$ 0 \u0026lt;= W_1 ^ 2 + W_2 ^ 2 + \u0026hellip;.+W_n ^ 2 \u0026lt;= m $$\nL1 L2正则化在空间上的解释可以用下面这张图解释\nsoftmax与交叉熵损失函数 softmax函数常用于多分类问题，对于一组输出，比如说上面cat的输出，我们可以利用这个函数把各个输出的分数转换为概率来进行研究，其数学形式长这样\n分母是各个分数转化后的总和，分子是对于该类转化后的值\n交叉熵（Cross Entropy）是Shannon信息论中一个重要概念，主要用于度量两个概率分布间的差异性信息。在信息论中，交叉熵是表示两个概率分布 $p$, $q$ 的差异，其中 $p$ 表示真实分布，$q$ 表示预测分布，那么 $H(p,q)$ 就称为交叉熵 $$ H(p,q)=\\sum_i p_i \\cdot \\ln {1 \\over q_i} = - \\sum_i p_i \\ln q_i \\tag{1} $$ 在这个问题中，对于第i类来说，其真实分布$p$的概率就是1，所以总的表达式又可以进行化简\n然后slices中有一个有趣的问题，当一个图片在一个有着C类的数据集上进行分类时，假设我们预测得到的这个图片是每某个类的概率差不多，那么交叉熵损失是多少？\n带入公式，实际上就是 $ -log \\frac{1}{C} $, 也就是 $ log C $\n对于这个数据集来说，C的数量是10\n总结 这次我们了解了很多损失函数和避免模型过拟合的方法，但是还没有了解怎么求得我们的最优的参数矩阵W，下次来了解一下求最优参数的方法！\n","date":"2024-11-13T08:49:49+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/","title":"线性分类器"},{"content":"前言 Pytorch实现KNN近邻算法的一些思路\n图像分类 图像分类是计算机视觉的核心任务，当给定我们的模型一张图片，我们的模型应该可以正确的给出图片上的物体的类别\n例如，图片上有一只cat,那么模型应该正确输出cat。\n图像分类所面临的挑战 semantic gap 语义差异 在人类看来，几乎不用思考就可以辨别出图像上的物体，但是机器却是无情的执行命令的机器，怎么把图像上所蕴含的信息传递给机器呢？于是便有了像素的表示，我们可以把一张图片细分为很多个小格子，显然，格子越多，这样图片就更加清晰，图片是以数字形式存储的，这种形式通常被称为数字图像。数字图像是由像素（picture elements）组成的矩阵，每个像素代表图像中的一个小点。所以，现在来看，这样图片就是一个数字矩阵\n图像旋转 虽然可以使用矩阵和数字的方法来表示一张图片，可是，按照这种方法，当把原始图片旋转之后，每个格子内的数字就会发生变化，对于人类来说，即便是把图片旋转后，也可以很简单的辨认出图片上的物体；而对机器来说，如果不加以处理，当模型接受到一个这样的图片后，很有可能会发生错误的预测。\n更多的挑战 不但要识别出图片中是什么，还要准确的指出它的类别。例如，照片中是一只猫，但是，是橘猫、布偶猫还是狸花猫呢 更难的，一些野生动物为了更好的适应所生存的环境，其颜色会和环境发生重叠，也就是所谓的保护色 这些都给我们带来了更多的挑战\n图像数据集 这里，老师给出了常见的数据集，并且也对比了其中的图片数量级\n其中，我只使用过MNIST数据集(手写体数字识别)，在本次作业中实现的KNN近邻算法使用的是CIFAR10，与CIFAR100主要的区别就是总共只有10个类别，每个类别里面有很多张图片。选择这个数据库的原因是，MNIST中的数据太少，而ImageNet和Places365数据太多，折中选择了这个数据集。\nKNN近邻算法 对于这类算法，一般有两个通用的API\n训练\n1 2 3 def train(images, labels): # To do return modle 预测\n1 2 3 def predict(modle, test_images): # To do return test_labels 这个算法的核心思想是如果一个样本在特征空间中的k个最相邻的样本中的大多数都属于某一个类别，则该样本也属于这个类别。意思就是，对于一个预测的样本，如果离这个样本最近的数据都是属于A类别，那么这个要预测的样本很有可能就是A类别，毕竟它们很相似！\nL1距离 这里的距离指的就是曼哈顿距离,计算方法就是对应坐标作差后取绝对值。\nL2距离 L2距离指的是欧式距离，也就是在空间中计算两个点之间对应坐标距离的方法\n代码实现 现在来实现一下上文中提到的两个API，简单起见，这里使用numpy\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import numpy as np class KNN: def __init__(): pass def train(self, X, y): \u0026#39;\u0026#39;\u0026#39; X指的是输入的训练数据 y指的是训练数据对应的类别 \u0026#39;\u0026#39;\u0026#39; self.X = X self.y = y def predict(self, X): # X是输入的，需要预测其类别的test数据 num_test = X.shape[0] # 样本类别数 Ypred = np.aeros(num_test, dtype=self.y.dtype) # 遍历所有test数据，给test数据中的每个数据找到其距离最近的那个图像 for i in range(num_test): # 计算L1距离 dis = np.sum(np.abs(X[i, :] - self.X), axis = 1) # 找到其中距离最近的那个值对应的index，进而获得它的类别 min_idx = np.argmin(dis) # 对应的标签就是self.y的标签 Ypred[i] = self.y[min_idx] return Ypred 这里的predict使用的是显示loop,这种情况会特别耗时！\nk的取值 上面的代码没有显示的指定k的取值范围，而是直接使用k=1这个取值，首先，这样做很简单，因为只需要查看离测试样本距离最近的那个类是什么类型即可。但是，这种操作会使得算法会对异常值特别敏感，同时，不同类别之间的边界也是突出状而不是趋于平滑\n因此，我们可以提高k的取值，提高k值也减少了异常值对test数据的影响，因为test数据这次有k个不同的参考意见，而不是只去参考一个值。\n使用Pytorch来实现KNN算法 首先是计算两个向量之间的距离\n双重循环 This implementation uses a naive set of nested loops over the training and test data. The input data may have any number of dimensions \u0026ndash; for example this function should be able to compute nearest neighbor between vectors, in which case the inputs will have shape (num_{train, test}, D); it should also be able to compute nearest neighbors between images, where the inputs will have shape (num_{train, test}, C, H, W). More generally, the inputs will have shape (num_{train, test}, D1, D2, \u0026hellip;, Dn); you should flatten each element of shape (D1, D2, \u0026hellip;, Dn) into a vector of shape (D1 * D2 * \u0026hellip; * Dn) before computing distances.\n这里是老师给的一些代码的预处理的关键提示\nMore generally, the inputs will have shape (num_{train, test}, D1, D2, \u0026hellip;, Dn); you should flatten each element of shape (D1, D2, \u0026hellip;, Dn) into a vector of shape (D1 * D2 * \u0026hellip; * Dn) before computing distances.\n对于所有的输入向量，不管是什么维度的样本，我们都可以转换为一个二维张量，这样做可以简化一些计算，同时使得结果更加清晰。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import torch def compute_distances_two_loops(x_train: torch.Tensor, x_test: torch.Tensor): \u0026#34;\u0026#34;\u0026#34; Args: x_train: Tensor of shape (num_train, D1, D2, ...) x_test: Tensor of shape (num_test, D1, D2, ...) Returns: dists: Tensor of shape (num_train, num_test) where dists[i, j] is the squared Euclidean distance between the i-th training point and the j-th test point. It should have the same dtype as x_train. \u0026#34;\u0026#34;\u0026#34; num_train = x_train.shape[0] # 得到行数 num_test = x_test.shape[0] dists = x_train.new_zeros(num_train, num_test) # dists[i, j]是第i个x_train与x_test的距离 x_train_flat = x_train.view(num_train, -1) x_test_flat = x_test.view(num_test, -1) for i in range(num_train): for j in range(num_test): diff = x_train_flat[i] - x_test_flat[j] dists[i, j] = torch.sum(diff ** 2) return dists 这样做十分清晰，而且不开根是因为我们不需要得到确切的欧氏距离，只是单纯的比较大小，所以直接返回距离的平方即可。\n但是，这样并没有利用pytorch,会导致计算速度减慢，看看怎么优化！\n一重循环 这里使用的其实是一个高级特征，通过下标索引来访问元素(熟悉numpy的肯定不陌生)\n1 2 3 4 5 6 7 8 9 10 11 12 13 def compute_distances_one_loop(x_train: torch.Tensor, x_test: torch.Tensor): num_train = x_train.shape[0] num_test = x_test.shape[0] dists = x_train.new_zeros(num_train, num_test) x_train_flat = x_train.view(num_train, -1) x_test_flat = x_test.view(num_test, -1) for i in range(num_train): diff = x_train_flat[i] - x_test_flat # 计算每个x_train与x_test dists[i, :] = torch.sum(diff ** 2, dim=1) # 按照行进行求和 return dists 其中，diff = x_train_flat[i] - x_test_flat 这行代码可以自动帮我们计算第i个x_train与所有的x_test的差值，然后按照行的顺序进行求和(dim=1，numpy的是axis = 1)。\nNo loop 不使用循环 可以说不使用循环才是KNN里面一个十分精彩的部分，这里用到了高级机制BoardCast广播机制，下面我们来看一看这个部分的数学表达形式。\n图片来自bilibili视频.\n这里还需另外提醒的是矩阵的加减法，对于两个维数相同的矩阵$A 和B$，经过运算后得到$C$,其中，$$ C_{ij} = A_{ij} + B_{ij} $$\n$$ C_{ij} = A_{ij} - B_{ij} $$\n即对应元素相加减的操作\n1 2 3 4 5 6 7 8 9 10 11 12 13 def compute_distances_no_loops(x_train: torch.Tensor, x_test: torch.Tensor): num_train = x_train.shape[0] num_test = x_test.shape[0] dists = x_train.new_zeros(num_train, num_test) x_train_flat = x_train.view(num_train, -1) x_test_flat = x_test.view(num_test, -1) x_train_squre = torch.sum(x_train_flat ** 2, dim = 1) x_test_squre = torch.sum(x_test_flat ** 2, dim = 1) # 计算点积 temp = x_train_flat @ x_test_flat.t() dists = x_train_squre.view(-1, 1) + x_test_squre.view(1, -1) - 2 * temp return dists 根据上面的计算结果，x_train是一列的张量，x_test是一行的张量，从而两者进行广播后运算，值得注意的是，两者的内积可以转换为矩阵相乘的形式。\n预测predict 在计算出每个测试样本与(x_test)每个训练样本(x_train)的距离之后，对于每个测试样本，我们就可以找到其前k个最近值，然后确定其类别后返回。\n一个可能的函数看起来可能是这样的：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def predict_labels(dists: torch.Tensor, y_train: torch.Tensor, k: int = 1): num_train, num_test = dists.shape y_pred = torch.zeros(num_test, dtype=torch.int64) for j in range(num_test): # 遍历x_test数据 # 找到每个测试样本最近的k个训练样本的距离和索引 dists_k, indices = torch.topk(dists[:, j], k, largest=False) # largest=False 是升序 # 获取这些最近的k个训练样本的标签 nearest_labels = y_train[indices] # 计算每个标签的出现次数并选择出现次数最多的标签 labels, counts = torch.unique(nearest_labels, return_counts=True) # 记录最大出现次数 max_count = torch.max(counts) # 找到出现次数是最大出现次数的那个标签 most_common_labels = labels[counts == max_count] # 在计数最多的标签中选择数值最小的那个标签 y_pred[j] = torch.min(most_common_labels) return y_pred 这个代码的思路就是，寻找距离test数据的最近的k个不同类别，返回其出现次数最多的那个类别，如果有两个类别出现次数相同，那么就返回距离最近的那个。\n交叉验证 k折交叉验证（英语：k-fold cross-validation），将训练集分割成k个子样本，一个单独的子样本被保留作为验证模型的数据，其他k − 1个样本用来训练。交叉验证重复k次，每个子样本验证一次，平均k次的结果或者使用其它结合方式，最终得到一个单一估测。这个方法的优势在于，同时重复运用随机产生的子样本进行训练和验证，每次的结果验证一次，10次交叉验证是最常用的。\n对于原始数据 (raw 数据)，如果全部用于训练，则无法评估模型在未见过的数据上的表现，从而无法验证模型的有效性和准确性。因此，通常会将数据划分为两部分，一部分用于训练 (train)，另一部分用于测试 (test)，从而在测试集上评估模型的表现。\n在实际应用中，为了更好地选择超参数，我们会引入一个额外的数据集，称为验证集 (validation)。这样，数据集可以划分为三部分：训练集 (train)、验证集 (validation) 和测试集 (test)。\n具体流程如下：\n训练集：用于训练模型，使模型能够学习数据的特征和模式。 验证集：用于选择超参数。我们在验证集上测试不同的超参数组合，并选择在验证集上表现最好的参数设置。 测试集：在完成超参数选择后，我们在测试集上评估最终模型的性能。测试集完全不参与训练和参数选择，因此可以真实反映模型在新数据上的表现。 这种三分法的优势在于，验证集用于超参数选择，而测试集则用来评估模型在未见过的数据上的泛化能力，从而避免在超参数选择过程中过拟合测试集的风险。\n有关交叉验证的部分解释来自ChatGPT\n交叉验证 (Cross-Validation) 是一种用于更稳健地评估模型性能和选择超参数的方法。它通过多次数据划分和训练测试来减少模型对数据划分的偶然影响。在交叉验证中，我们通常将数据划分为多个等大小的部分（称为“折”或“folds”），并在每次训练时使用不同的折组合来训练和测试模型。\n交叉验证的数据划分步骤 以常用的 K 折交叉验证 (K-Fold Cross-Validation) 为例，具体步骤如下：\n将数据分成 K 个等大小的折：将数据集均匀分成 K 个折，记为 fold_1, fold_2, ..., fold_K。通常，K 的值是 5 或 10。\n多次训练和验证：对于每次迭代（共 K 次），使用 K-1 个折作为训练集，剩下的一个折作为验证集。具体来说：\n第一次迭代：使用 fold_2 到 fold_K 作为训练集，fold_1 作为验证集。 第二次迭代：使用 fold_1 和 fold_3 到 fold_K 作为训练集，fold_2 作为验证集。 以此类推，直到每个折都被用作一次验证集。 计算平均性能：在每次迭代中记录模型在验证集上的性能（例如准确率、损失等），然后将 K 次的验证结果平均，作为该模型在该超参数下的总体验证性能。\n选择最佳参数：对于每个超参数组合，都进行上述 K 次交叉验证，并根据平均性能选择表现最好的参数组合。\n最终测试：完成超参数选择后，可以在独立的测试集上评估模型的最终性能。\n交叉验证的优势 交叉验证避免了简单的训练/验证分割可能带来的偶然性，使模型能更稳健地评估数据上的表现。此外，交叉验证可以最大化地利用数据，因为每个数据点都能在验证集中出现一次，同时也在训练集中使用 K-1 次。这种方法尤其适合数据量较小的场景。\n总结 为了避免数据划分的偶然，我们保持三部分总体不变，对于train validation部分做出变化。\n我们把上面train数据分为nums个，每一块叫做一个fold, 假设分为5个fold，对于我们的待选参数集K来说，对于第i个参数K[i]，每次都选择1个fold作为验证集(validation)，剩下的4个作为训练集，然后再计算其准确率，对于K[i]来说，我们就得到了5个不同的准确率，然后可以取平均(mean)，作为选择K[i]为参数是的准确率，最后通过准确率就可以选出最优的K。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def knn_cross_validate( x_train: torch.Tensor, y_train: torch.Tensor, num_folds: int = 5, k_choices: List[int] = [1, 3, 5, 8, 10, 12, 15, 20, 50, 100], # 待选K ): x_train_folds = list(torch.chunk(x_train, num_folds)) # 把数据拆分为 num_folds块 y_train_folds = list(torch.chunk(y_train, num_folds)) k_to_accuracies = {k: [] for k in k_choices} # 使用键值对进行存储 for k in k_choices: for j in range(num_folds): x_val = x_train_folds[j] # 获取第j个验证集 y_val = y_train_folds[j] x_train_fold = torch.cat([x_train_folds[i] for i in range(num_folds) if i != j], dim=0) # 剩下的num_folds作为训练集 y_train_fold = torch.cat([y_train_folds[i] for i in range(num_folds) if i != j], dim=0) classifier = KnnClassifier(x_train_fold, y_train_fold) acc = classifier.check_accuracy(x_val, y_val, k) # 计算准确率 k_to_accuracies[k].append(acc) return k_to_accuracies 总结 KNN近邻算法还是非常适合初学者入门的，其中主要难点或者说比较新鲜的点就是广播机制BoardCast以及交叉验证的代码实现！\n","date":"2024-11-08T17:54:16+08:00","permalink":"https://XiaoPeng0x3.github.io/p/image-classifier%E7%AC%94%E8%AE%B0/","title":"Image Classifier笔记"},{"content":"数据泛化 一些常见的统计方法 mean：均值，对于一组数据来说，计算其均值可以直接使用np.mean来进行计算，对于多维数据，numpy引入了轴axis的概念，其中，轴的起点从0开始一直到n，例如，在二维数据中，其中每一行代表一个样本，每一列代表一个特征(类似于csv文件)\n​\t长,宽,高\nx1\t1,2,3\nx2\t1,4,5\nx3\t1,6,7\n对于这样一组数据，用列表来表示就是\n1 2 3 4 data = [x1, x2, x3] data = [[1,2,3], [1,4,5], [1,6,7]] 其中，按照axis=0的方式来计算均值，这里计算的就是长、宽、高 每个特征的均长、均宽、均高，按照axis = 1来进行计算，也就是计算每一行的均值，也就是每一个样本的均值(看起来没有什么意义)\n1 2 mean1 = np.mean(data, axis=0) # 按照列进行计算 mean2 = np.mean(data, axis=1) # 按照行进行计算 下面所有的方法var(方差)，std(标准差)等均可以按照不同的轴进行计算\nvar:variance，方差，不在叙述计算公式，可以直接使用np.var()\nstd：Standard deviation,可以根据方差得到,可以直接使用`np.std()\nnp.round:保留小数操作，例如，要对data保留三位小数，可以表示为\n1 ans = np.round(data, 3) min-max均值规化 公式为\n$$ x\u0026rsquo; = \\frac{x-min}{max-min}$$\n对于一组数据data,可以这样计算\n1 2 3 4 5 def MinMax(data:np.ndarray) -\u0026gt; np.ndarray: data_max = np.max(data, axis=0) data_min = np.min(data, axis=0) ans = (data - data_min) / (data_max - data_min) return ans 标准化 标准化可以把各个特征标准化为标准差为1，均值为0的正态分布\n公式为\n$$ x = \\frac{x-\\mu}{\\sigma} $$\n其中， $\\mu$是均值，$\\sigma$是标准差\n1 2 3 4 5 def Standardization(data: np.ndarray) -\u0026gt; np.ndarray: data_mean = np.mean(data, axis=0) data_std = np.std(data, axis=0) ans = (data - data_mean) / data_std return ans 总结 是否必须使用标准化方法？\n算法需求： 某些算法（如距离-based的算法, K-means, K邻近）对特征尺度非常敏感，标准化几乎是必需的。 某些算法（如决策树、随机森林等）对特征尺度不敏感，标准化不是必需的。 数据特性： 如果特征的数值范围已经很接近，标准化的效果可能不明显。 如果特征的数值范围差异很大，标准化可以显著提升模型性能。 模型性能： 通过实验比较标准化前后的模型性能，可以决定是否需要标准化。 ","date":"2024-11-04T10:43:36+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E6%95%B0%E6%8D%AE%E6%B3%9B%E5%8C%96/","title":"深度学习基础系列：数据泛化"},{"content":"reshape reshape可以改变矩阵的维度，例如，有一个一维矩阵\n1 2 3 4 import numpy as np a = np.arange(20) # 转变为四行五列 ans = np.reshape(a, (4,5)) # 传入一个元组 转化前后数据量是一致的，20个元素不可以转换为3行4列\n","date":"2024-11-03T23:19:33+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%9F%A9%E9%98%B5%E5%8F%98%E5%9E%8B/","title":"深度学习基础系列：矩阵变型"},{"content":"矩阵的转置 矩阵的转置可以说是一个很常见的矩阵操作了，对于简单的矩阵(二维及以下)的矩阵来说，只需要调用numpy的T属性即可，例如\n1 2 3 4 5 6 7 import numpy as np a = [[1,2,3], [3,4,5], [5,6,7]] a_nparray = np.array(a) # 转换为ndarray a_T = a_nparray.T # 是一个属性 或者直接使用transpose\n1 2 3 4 5 6 import numpy as np a = [[1,2,3], [3,4,5], [5,6,7]] a_nparray = np.transpose(a) ","date":"2024-11-03T22:56:32+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%9F%A9%E9%98%B5%E8%BD%AC%E7%BD%AE/","title":"深度学习基础系列：矩阵转置"},{"content":"Softmax函数 softmax函数常用于多分类问题，我们希望模型的输出可以作为预测的概率，即输出值越大的那个参数在预测的时候很有可能就是正确答案。\n但是回想一下概率的计算公式(扔骰子)，对于总的概率空间样本来说，其概率的总和一定是1，而我们的预测输出基本上总和不可能是1，这里softmax函数的作用就是压缩这些输出值，从而使用概率的方式进行表示\nsoftmax函数长这样\n给定一个K维向量$ z=[z_1,z_2,\u0026hellip;,z_K]$，Softmax函数的定义为：\n这里，$\\mathbf{z}$ 是一个K维向量，$z_j$ 是向量中的第 $j$ 个元素，$ 1 \\leq j \\leq K$。\n$$ \\sigma(\\mathbf{z})j = \\frac{e^{z_j}}{\\sum{k=1}^{K} e^{z_k}} $$\n计算过程 对于给定的一个list输入，先计算得到softmax函数的分母值\n1 2 3 4 5 6 7 8 9 10 11 12 13 import math input = [1, 2, 3] # 分母是math.exp形式 ans = list() sumVal = 0.0 for val in scores: sumVal += math.exp(val) # 分母 for val in scores: ans.append(math.exp(val) / sumVal) print(ans) 我们也可以使用numpy进行计算\n1 2 3 4 5 6 7 8 import numpy as np def softmax(scores: list[float]) -\u0026gt; list[float]: temp = np.array(scores) # 转换为ndarray exp_temp = np.exp(temp) # 计算所有值的exp值 exp_sum = np.sum(exp_temp) # 和为分母 # 计算分子 exp_temp ans = np.round(exp_temp / exp_sum, 4) # 每个exp值除以分母，并保留4位小数 return ans numpy的方便之处在于不用编写循环和计算快！\n","date":"2024-11-02T10:46:57+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97softmax/","title":"深度学习基础系列：softmax"},{"content":"上次我们了解了怎么用Go语言来创建和连接一个socket，这里来看一看怎么封装用户行为以及怎么实现用户广播上线功能\nServer的封装 server 这是在上一节中提到的server结构\n1 2 3 4 type Server struct { Ip string Port int } 可以看到，我们只有两个简单的成员属性，为了实现用户上线后全部广播的操作，我们需要在server中记录下来每次连接到server的client，这里可以使用map来进行记录，同时，为了实现全局广播的效果，我们可以在server中使用一个chan来进行管理。\n为什么要实现server要实现一个chan通道呢，当有用户上线建立连接后，我们就可以把上线的这个消息发送给chan来进行管理，然后遍历map就可以实现广播的操作。\nserver的实现 需要在原来的基础上多增加一些属性\n1 2 3 4 5 6 7 8 9 10 type Server struct { Ip string Port int // 创建用户表 OnlineMap map[string]*User // 同步的锁 mapLock sync.RWMutex // 负责全局广播的chan Message chan string } 在创建好一个server后，与上一篇文章一样，使用协程去处理连接之后的状态\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 func (this *Server) Start() { // 创建好一个监听对象 // 这个函数会有两个返回值 // 一个是创建的 listen对象， 一个是是否创建成功 listener, err := net.Listen(\u0026#34;tcp\u0026#34;, fmt.Sprintf(\u0026#34;%s:%d\u0026#34;, this.Ip, this.Port)) // 创建失败的话， err就会有一个失败code // err != nil 就是说明创建失败 if err != nil { fmt.Println(\u0026#34;创建监听对象失败！, err\u0026#34;, err.Error()) return } // 启动之后记得关闭，避免浪费资源 defer listener.Close() // 然后就是使用accept方法 // 在一个循环里面不停的接受数据 // 监听 // 全局管道 go this.ListenMessage() for { // 这里的 meaage 是net.Coon类型 conn, err := listener.Accept() // 说明接收到了数据 if err != nil { fmt.Println(\u0026#34;监听失败！\u0026#34;) continue } // 打开一个协程去处理 go this.Handle(conn) // 下面的代码不会阻塞 } } 这里的Handle方法可以去处理连接请求，有哪些请求呢？\n当一个用户上线后，应该把这个用户添加到Online表中 广播这个用户上线的消息 看到这里，起始我们缺少封装的user类，我们可以再封装一个user类\nUser的封装 user类的实现 在user里面，基本的属性有Name, Address这些操作，为了更加方便User把消息转发给client(转发操作指的是conn.Write操作)，在消息接收的上我们可以初始化一个chan来进行连接转发\n1 2 3 4 5 6 type User struct { Name string Addr string C chan string conn net.Conn } C是为了接受来自server的消息，conn是为了把消息转发给client，那么在初始化的时候，就得去监听，看是否有消息写回来\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 func NewUser(conn net.Conn) *User { userAddr := conn.RemoteAddr().String() // 可以得到客户端的地址 user := \u0026amp;User{ Name: userAddr, Addr: userAddr, C: make(chan string), conn: conn, } go user.ListenMessage() return user } // ListenMessage 监听User的chan func (this *User) ListenMessage() { for { mes := \u0026lt;-this.C this.conn.Write([]byte(mes + \u0026#34;\\n\u0026#34;)) } } 这样，在实现连接之后，我们就可以添加用户到在线表里面去\n1 2 3 4 5 6 7 8 9 10 11 12 func (this *Server) Handle(conn net.Conn) { // fmt.Println(\u0026#34;连接成功！\u0026#34;) // 执行到这里，说明已经有一个用户上线 newUser := NewUser(conn) this.mapLock.Lock() this.OnlineMap[newUser.Name] = newUser this.mapLock.Unlock() // 广播该用户已上线 this.Boardcast(newUser, \u0026#34;I am in!\u0026#34;) } user用户上线的广播 怎么实现Boardcast方法呢？可以直接利用server里面的chan来实现\n1 2 3 4 5 func (this *Server) Boardcast(u User, mes string) { // 把上线的消息发送给message chan sendMes := \u0026#34;[\u0026#34; + user.Name + user.Addr + mes + \u0026#34;]\u0026#34; this.Message \u0026lt;- sendMes // 发送给管道 } 在发送给管道后，server中的管道就就得到了数据，因此，就可以直接通过server中的chan进行消息的传输，遍历Onlinemap即可\n1 2 3 4 5 6 7 8 9 10 11 12 13 func (this *Server) ShareMessage() { // 只要message有消息 // 那么就发送给在线的所有用户 for { mes := \u0026lt;-this.Message for _, user := range this.OnlineMap { this.mapLock.Lock() user.C \u0026lt;- mes this.mapLock.Unlock() } } } 最后在服务启动的时候去监听转发信息功能即可\n总结 整个流程看起来是这样的\n创建tcp套接字并开启连接 连接后会创建User对象 User对象会向Server的message发送信息 server的message接收到信息之后会遍历整个Online表，把User上线的消息发送给在Online表中的每一个User 其中，消息的转发依赖于conn.Write，User上线后把上线消息写入Serevr的chan，Server再把该User上线的消息通过Online表写入User的chan。\n","date":"2024-11-02T09:09:40+08:00","permalink":"https://XiaoPeng0x3.github.io/p/tcp%E8%81%8A%E5%A4%A9%E5%AE%A4%E4%BA%8C%E7%94%A8%E6%88%B7%E4%B8%8A%E7%BA%BF%E5%92%8C%E5%B9%BF%E6%92%AD/","title":"TCP聊天室(二)：用户上线和广播"},{"content":"前言 在学习完Go语言之后，总是感觉没有合适的上手项目进行练习，最近正好看到一个TCP网络聊天室的小项目，这个项目只使用基础的包而不使用任何框架，非常适合练手。\n需要的工具有\nGo开发环境 nc工具，方便模拟client进行测试 建立连接 在Go中，我们可以使用net包来进行基本的server的socket的创建，也就是net.Listen方法\n1 net.Listen() 下面是这个函数的原型\n1 2 3 4 5 // The network must be \u0026#34;tcp\u0026#34;, \u0026#34;tcp4\u0026#34;, \u0026#34;tcp6\u0026#34;, \u0026#34;unix\u0026#34; or \u0026#34;unixpacket\u0026#34;. func Listen(network, address string) (Listener, error) { var lc ListenConfig return lc.Listen(context.Background(), network, address) } 可以看到，函数的两个参数都是string类型的，第一个参数指定的是通信的网络(可以直接指定tcp), 第二个是server的地址。返回值就是监听对象和err\n例如，我们想要启动一个监听,就可以这样写\n1 2 3 4 listener, err := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;127.0.0.1:8080\u0026#34;) if err != nil { // To do } 就创建好了一个scoket\n在得到listener后，要开启接受功能，可以调用\n1 listener.Accept() 同样，这个函数有两个返回值，正常使用是这样的\n1 2 3 4 conn, err := listener.Accept() if err != nil { // To do } 其中，返回的是一个net.Conn类型的参数，可以通过conn在socket之间传递数据\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 type Conn interface { // Read reads data from the connection. // Read can be made to time out and return an error after a fixed // time limit; see SetDeadline and SetReadDeadline. Read(b []byte) (n int, err error) // Write writes data to the connection. // Write can be made to time out and return an error after a fixed // time limit; see SetDeadline and SetWriteDeadline. Write(b []byte) (n int, err error) // Close closes the connection. // Any blocked Read or Write operations will be unblocked and return errors. Close() error // LocalAddr returns the local network address, if known. LocalAddr() Addr // RemoteAddr returns the remote network address, if known. RemoteAddr() Addr // ........ // ........ } conn.Read可以从连接中读取数据(server可以read来自client的数据)， 同时conn.Write可以从连接中发送数据(server向client发送数据)\n这里就实现了通信的基石，即发送和接受数据，其整个过程就是\n创建socket：net.Listen,返回一个net.Listener对象 开始接收请求：listener.Accept,返回一个net.Conn对象 使用net.Conn实现接收数据和发送数据 simple demo 这里创建一个简单的demo程序，在server接收到来自client的数据后，把接受到的数据全部转换为大写后发送给client\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 package main import ( \u0026#34;bytes\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;net\u0026#34; ) func main() { ip := \u0026#34;127.0.0.1\u0026#34; port := 8080 //CreateServer(ip, port) listener, err := net.Listen(\u0026#34;tcp\u0026#34;, fmt.Sprintf(\u0026#34;%s:%d\u0026#34;, ip, port)) defer listener.Close() if err != nil { fmt.Println(\u0026#34;Eror!\u0026#34;, err) return } buf := make([]byte, 1024) conn, err := listener.Accept() defer conn.Close() for { if err != nil { fmt.Println(\u0026#34;connect fail\u0026#34;, err) return } // 读取数据 conn.Read(buf) // 写回数据 conn.Write(bytes.ToUpper(buf)) } } 然后使用nc工具\n1 nc 127.0.0.1 8080 当我们发送hello的时候，server正确的返回了HELLO\n思考：当有多个client的时候怎么办？\n协程处理 只有一个用户创建连接的时候可以正常返回，但此时有多个用户创建了连接请求，由于我们只accept了一次连接请求，所以当多个用户尝试连接的时候，第二个及之后的那些用户无法与服务器建立连接。\n解决办法\n每次在循环的过程中不断的进行监听，而不是只监听一次。 原始代码是\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // ..... conn, err := listener.Accept() // 把这里添加到循环中 defer conn.Close() for { if err != nil { fmt.Println(\u0026#34;connect fail\u0026#34;, err) return } // 读取数据 conn.Read(buf) // 写回数据 conn.Write(bytes.ToUpper(buf)) } } 添加到循环后就可以不断的建立连接\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // ...... for { conn, err := listener.Accept() if err != nil { fmt.Println(\u0026#34;connect fail\u0026#34;, err) return } // 读取数据 conn.Read(buf) // 写回数据 conn.Write(bytes.ToUpper(buf)) } } 然后再新建client的时候就可以处理多用户连接。\n这样写有什么问题？\n可以发现，当在一个client发送第二组数据后，server什么都没有返回，这是因为在循环执行到\n1 conn.Write(bytes.ToUpper(buf)) server一直在期待新的链接，而不是去处理之前的client的数据\n使用go协程\n在每次conn成功后，为了保持后续的链接，可以把后续的read和write封装为go协程\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 package main import ( \u0026#34;bytes\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;net\u0026#34; ) func Handler(conn net.Conn) { defer conn.Close() buf := make([]byte, 1024) for { cnt, _ := conn.Read(buf) conn.Write(bytes.ToUpper(buf[:cnt])) } } func main() { ip := \u0026#34;127.0.0.1\u0026#34; port := 8080 //CreateServer(ip, port) listener, err := net.Listen(\u0026#34;tcp\u0026#34;, fmt.Sprintf(\u0026#34;%s:%d\u0026#34;, ip, port)) defer listener.Close() if err != nil { fmt.Println(\u0026#34;Eror!\u0026#34;, err) return } //defer conn.Close() for { conn, err := listener.Accept() if err != nil { fmt.Println(\u0026#34;connect fail\u0026#34;, err) return } go Handler(conn) } } 也就是说在主函数内，只负责去监听是否有用户链接，而链接后的读写就去创建一个新的协程，在这个协程内根据这个链接不断的去实现client-server之间的读写。\n总结 net.Listen：创建tcp socket, 返回listener对象 listener.Accept：监听客户端的连接, 返回net.Conn连接对象 net.Conn：实现read和write，读取和发送数据 go：开启一个协程 ","date":"2024-11-01T17:43:28+08:00","permalink":"https://XiaoPeng0x3.github.io/p/tcp%E8%81%8A%E5%A4%A9%E5%AE%A4%E4%B8%80tcp%E9%80%9A%E8%AE%AF/","title":"TCP聊天室(一)：tcp通讯"},{"content":"协方差矩阵 协方差(covariance)可以用来观测变量之间是否存在线性相关性。然而，协方差本身有一些局限性，因此在实际应用中，我们通常还会使用相关系数来进一步评估变量之间的相关性。\n协方差的局限性 尺度依赖性：\n协方差的值受变量尺度的影响。如果一个变量的值范围很大，而另一个变量的值范围很小，即使它们之间有很强的线性关系，协方差的绝对值也可能很大或很小，这使得直接比较不同变量之间的协方差变得困难。 单位依赖性：\n协方差的单位是两个变量单位的乘积。例如，如果一个变量的单位是米，另一个变量的单位是秒，那么协方差的单位将是米·秒。这使得协方差的解释更加复杂。 相关系数 为了克服协方差的这些局限性，我们通常使用 皮尔逊相关系数（Pearson correlation coefficient），它是一个标准化的协方差，范围在 -1 到 1 之间。\n皮尔逊相关系数的定义 皮尔逊相关系数 ( r ) 定义为： $$ r_{XY} = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y} $$\n其中：\n$\\sigma_X $ 是 X 的标准差。 $\\sigma_Y$ 是 Y 的标准差。 解释 ( r = 1 )：完全正相关，即两个变量完全同向变化。 ( r = -1 )：完全负相关，即两个变量完全反向变化。 ( r = 0 )：没有线性相关性。 ( |r| ) 接近 1：表示强相关性。 ( |r| ) 接近 0：表示弱相关性或没有相关性。 Python 示例 可以使用 numpy 或 pandas 库来计算皮尔逊相关系数。\n使用 numpy 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import numpy as np # 示例数据 data = np.array([ [1, 4], # 观测值1 [2, 5], # 观测值2 [3, 6] # 观测值3 ]) # 计算协方差矩阵 cov_matrix = np.cov(data, rowvar=False) print(\u0026#34;协方差矩阵:\\n\u0026#34;, cov_matrix) # 计算相关系数矩阵 corr_matrix = np.corrcoef(data, rowvar=False) # correlation coefficient print(\u0026#34;相关系数矩阵:\\n\u0026#34;, corr_matrix) 这里的np.cov()和np.corrcoef()，如果不指定第二个参数，那么第二个参数默认rowvar = True，意思就是这组数据是按照横向放置的，意思就是每一行是一个属性\n而在有时候需要从文件里面读取一些属性，例如\n1 2 3 X\tY\tZ x1\ty1\tz1 x2\ty2\tz2 那么这个时候，就可以把默认值设置为False,代表每一列是一个属性\n使用 pandas 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import pandas as pd # 示例数据 data = pd.DataFrame({ \u0026#39;X\u0026#39;: [1, 2, 3], \u0026#39;Y\u0026#39;: [4, 5, 6] }) # 计算协方差矩阵 cov_matrix = data.cov() print(\u0026#34;协方差矩阵:\\n\u0026#34;, cov_matrix) # 计算相关系数矩阵 corr_matrix = data.corr() print(\u0026#34;相关系数矩阵:\\n\u0026#34;, corr_matrix) 结论 协方差提供关于变量之间线性关系的一些信息，但建议使用皮尔逊相关系数。\n相关系数不仅标准化了协方差，还提供了一个易于解释的度量，范围在 -1 到 1 之间。1代表存在正相关关系，-1代表负相关关系。\n","date":"2024-10-26T16:01:06+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5/","title":"深度学习基础系列：协方差矩阵"},{"content":"矩阵乘法 矩阵乘法是矩阵运算的基础，简单来说，对于两个A和B矩阵来说，只要A的列数等于B的行数，那么这两个矩阵就可以发生运算\n即A是一个m x n的矩阵， B是一个n x k的矩阵，那么运算后的结果就是m x k的矩阵\n首先把矩阵转换为np.array类型，然后判断它们的类型，查看是否可以相乘\n1 2 3 4 5 6 7 8 import numpy as np def matrix_dot_vector(a:list[list[int|float]],b:list[int|float])-\u0026gt; list[int|float]: a = np.array(a) b = np.array(b) if a.shape[1] != b.shape[0]: return -1 c = np.dot(a, b) return c np.dot()接受两个数组(矩阵)并返回它们的结果\n","date":"2024-10-25T23:41:00+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95/","title":"深度学习基础系列：矩阵乘法"},{"content":"在上一篇中，我们学习了如何在server上创建一个TCP的套接字，这里来看一看对于服务端，server是怎么获取客户端的连接请求的。\n获取连接请求 在开启监听(listen)后，如果有客服端尝试连接服务器，那么内核将于客户端进行连接，因为请求连接可能会有多个，所以内核会维护一个队列来存放这些请求。当客户端连接到内核之后，那么内核可以使用accept函数来返回并接受来自这个连接。\n下面来用代码演示一下\n1 2 3 4 5 6 7 8 9 10 11 # 把上次的代码复制一下 from socket import * # 创建一个套接字 # 使用socket进行初始化 serverSocket = socket(AF_INET, SOCK_STREAM) # 使用IPV4地址簇，使用的是流式socket # 接下来开始进行绑定 serverSocket.bind((\u0026#34;127.0.0.1\u0026#34;, 8080)) # bind 需要的是一个tuple类型 # 绑定后可以开始listen, 即查看是否有客户端连接到服务器 serverSocket.listen(1) # 最多监听一个 在创建好server socket之后，我们就可以使用accept函数来进行连接。这里先不考虑TCP协议在连接时的一些细节\n1 2 3 4 5 6 \u0026#39;\u0026#39;\u0026#39; 返回值： connectionSocket 客户端连接套接字 addr 连接的客户端地址 \u0026#39;\u0026#39;\u0026#39; connectionSocket，addr = serverSocket.accept() 也就是说，这个操作会返回一个新的socket,不同的是，通过这个socket就可以实现server与client之间的通讯。\n可以接受或者发送数据，下面是一些API\n1 2 3 recv()/send() recvmsg()/sendmsg() recvfrom()/sendto() 需要注意的是，传递的这些字符全部都是流式数据，与原始字符串不同\n简单的demo 在这里，创建一个简单的小demo，我们可以创建一个简单的服务程序，这个server什么也不做，只是简单的把接受到的数据原封不动的发送给client\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from socket import * # 初始化好一个socket with socket(AF_INET, SOCK_STREAM) as serverScoket: # 绑定IP和port serverScoket.bind((\u0026#34;127.0.0.1\u0026#34;, 8080)) # 开启监听listen serverScoket.listen(1) # 这是一个监听队列，当处理多个请求的时候，会把未来得及处理的放入队列里面，其中的参数表示队列的大小 # 开启socket的accept,从而处理来自server的连接 connectionSocket, _ = serverScoket.accept() # 先忽略第二个返回值 print(\u0026#39;connect!\u0026#39;) with connectionSocket as c: while True: data = c.recv(1024) # 每次都尝试获得来自客户端的数据, 注意这是个字节流数据 if not data: break c.sendall(data) # 把接受到的数据返回 我们可以使用netcat这个工具来进行测试\n1 nc 1270.0.0.1 8080 可以看到，服务器端口已经连接上了\n1 connect! 通过这个有趣的连接，我们还可以使用eval函数来实现计算式求值\neval函数是危险的！这里只是做演示\neval() 可以执行任意的 Python 代码。如果传入的字符串包含恶意代码，这可能导致严重的安全漏洞。例如，攻击者可以通过构造恶意表达式来执行系统命令、访问敏感数据、修改文件等。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 from socket import * # 初始化好一个socket with socket(AF_INET, SOCK_STREAM) as serverScoket: # 绑定IP和port serverScoket.bind((\u0026#34;127.0.0.1\u0026#34;, 8080)) # 开启监听listen serverScoket.listen(1) # 这是一个监听队列，当处理多个请求的时候，会把未来得及处理的放入队列里面，其中的参数表示队列的大小 # 开启socket的accept,从而处理来自server的连接 connectionSocket, _ = serverScoket.accept() # 先忽略第二个返回值 print(\u0026#39;connect!\u0026#39;) with connectionSocket as c: while True: data = c.recv(1024) # 每次都尝试获得来自客户端的数据, 注意这是个字节流数据 if not data: break # 解码接收到的数据 expression = data.decode() # 计算表达式的结果 result = str(eval(expression))+\u0026#39;\\n\u0026#39; # 发送结果给客户端 c.sendall(result.encode()) 一些缺点 浪费性能 对于真实世界来说，这里的服务器实在是太弱了\n1 2 3 4 5 while True: data = c.recv(1024) if not data: break c.sendall(data) # 把接受到的数据返回 server每次只能处理一个请求，当client没有发送数据的时候，c.recv(1024)这行代码也就会永远阻塞在这里，很浪费性能。\n此时，很自然的想到以并发的方式去处理频繁的连接\n应对策略 多进程 可以使用fork来创建多个进程，其中，fork函数在子进程里面的返回值是0，所以，可以设想一下，当有服务来临时，父进程只去监听(accept)是否有连接，同时可以把读写操作放到子进程里面去运行，这样通过进程调度策略，就可以实现并发\n代码看起来是这样的：\n1 2 3 4 5 6 7 8 while (true) { pid_t pid; if ((pid = fork()) == 0) { // 子进程 // do read() or do write() } else { // accept } } 每当一个连接到来时，程序就会创建一个子进程来处理，可惜进程实在是不够“轻量”，而且进程调度器来进行调度的时候也需要有着内核态到用户态的转换、进程的变量读写保存，因此可以考虑使用多线程来进行尝试\n多线程 什么是线程？\nIn computer science, a thread of execution is the smallest sequence of programmed instructions that can be managed independently by a scheduler, which is typically a part of the operating system.[1] In many cases, a thread is a component of a process.\nThe multiple threads of a given process may be executed concurrently (via multithreading capabilities), sharing resources such as memory, while different processes do not share these resources. In particular, the threads of a process share its executable code and the values of its dynamically allocated variables and non-thread-local global variables at any given time.\nThe implementation of threads and processes differs between operating systems.\n线程是进程的一个子集(你可以这么认为)，一个进程里面会有多个线程，这些线程共享这个进程所有的资源(因为它们有着一样的页表)，所以在线程切换的时候，不需要有很大的开销，只需要维护每个线程内部不共享或者私有的数据即可。\n这样就可以使用多线程来处理\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import threading from socket import * # 把发送请求发送这里 def handle_client(c, addr): print(addr, \u0026#39;connect\u0026#39;) while True: data = c.recv(1024) if not data: break c.sendall(data) with socket(AF_INET, SOCK_STREAM) as serverSocket: serverSocket.bind((\u0026#34;127.0.0.1\u0026#34;, 8080)) serverSocket.listen() while True: connectionSocket, addr = serverSocket.accept() # 接受多个不同的请求 t = threading.Thread(target=handle_client, args=(connectionSocket, addr)) t.start() 多线程的方法可以使用线程池的方法去调度，不过线程也会占用系统的资源。\n结尾 之后记录一下select、poll、epoll这些方法\n","date":"2024-10-21T13:19:18+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%BA%8C%E6%9C%8D%E5%8A%A1%E7%AB%AF%E8%8E%B7%E5%8F%96%E8%BF%9E%E6%8E%A5%E8%AF%B7%E6%B1%82/","title":"网络编程(二)：服务端获取连接请求"},{"content":"前言 这个系列会记录所有的学习计算机网络时的笔记。\n首先计算机网络很重要(Web开发)，网络在日常生活和学习中无处不在，但是你真的了解底层的那些细节吗？比如TCP/IP协议、UDP协议，在面试的时候可能会问到相关的细节。\n参考书籍 本次参考的书籍是：\n计算机网络：自顶向下\n在此基础上，可以选择一些视频资料进行辅助，推荐一门好评较高的课程：中科大——计算机网络，同时，如果觉得学了这些知识而缺少lab来动手的话，可以参考这门lab,如果觉得过于简单，也可以挑战一下CS144.\nSocket Socket的中文翻译是套接字，这个翻译很难理解，你可以把它认为是一个接口，插座之类的物品，或者认为它是一个介质。在日常生活中，很多都是Client-Server的模式，即客户端请求服务器上的一些资源，服务器在收到客户端的请求之后把数据发送到给客户端，这些数据就是通过套接字来实现的传输的。\n套接字（Socket）是一个抽象层，应用程序可以通过它发送或接收数据，可对其进行像对文件一样的打开、读写和关闭等操作。套接字允许应用程序将 I/O 插入到网络中，并与网络中的其他应用程序进行通信。\n网络套接字是 IP 地址与端口 Port 的组合。\n为了满足不同的通信程序对通信质量和性能的要求，网络系统提供了三种不同类型的套接字，以供用户在设计网络应用程序时根据不同的要求来选择。分别是：\n流式套接字（SOCK-STREAM）。提供一种可靠的、面向连接的双向数据传输服务，**实现了数据无差错、无重复的发送。**流式套接字内设流量控制，被传输的数据看作是无记录边界的字节流。在 TCP/IP 协议簇中，使用 TCP 协议来实现字节流的传输，当用户想要发送大批量的数据或者对数据传输有较高的要求时，可以使用流式套接字。 数据报套接字（SOCK-DGRAM）。提供一种无连接、不可靠的双向数据传输服务。数据包以独立的形式被发送，并且保留了记录边界，不提供可靠性保证。数据在传输过程中可能会丢失或重复，并且不能保证在接收端按发送顺序接收数据。在 TCP/IP 协议簇中，使用 UDP 协议来实现数据报套接字。在出现差错的可能性较小或允许部分传输出错的应用场合，可以使用数据报套接字进行数据传输，这样通信的效率较高。 原始套接字（SOCK-RAW）。该套接字允许对较低层协议（如 IP 或 ICMP ）进行直接访问，常用于网络协议分析，检验新的网络协议实现，也可用于测试新配置或安装的网络设备。 这里需要记住两种协议：TCP和UDP，先不考虑它们底层是怎么实现的，TCP的特点就是可以保证在传送的过程中数据不丢失、不重复、不乱序，原原本本的把数据发送给接受者；而UDP协议考虑的很少，所以UDP协议可以用于传输速度快的场景。\n创建Socket 这里为了简单起见，使用Python来创建\n1 2 3 4 5 6 7 8 # 导入socket包 from socket import * # 创建一个TCP套接字 serverSocket = socket(AF_INET, SOCK_STREAM) # TCP是流式套接字 # 绑定Server的IP和Port serverSocket.bind((\u0026#34;127.0.0.1\u0026#34;, 8080)) # bind的参数是tuple类型，需要一个地址和端口 到这里，已经成功把serverSocket绑定到IP地址为127.0.0.1:8080的机器上了\n监听listen 在创建好serverSocket后，因为不知道什么时候server会收到连接请求，一个有效的方法是listen函数\n1 serverSocket.listen(1) 这里，函数的参数是监听队列的大小，当有多个连接请求时，这些请求会被放到监听队列里面。\n到目前为止，我们可以看一看serverSocket的一些信息\n1 print(ServerSocket) 下面是输出信息\n1 \u0026lt;socket.socket fd=340, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=(\u0026#39;127.0.0.1\u0026#39;, 8080)\u0026gt; 其中，第一个参数fd你可能不太了解，这个是一个文件描述符(file descriptor)，文件描述符是一个Obj的handle，或者你也可以理解为指向文件的指针，在Unix和类Unix系统中，所有的I/O都被抽象为文件描述符，包括网络套接字。\n流程 在创建一个socket时，可以遵循以下步骤\n根据协议初始化一个socket 给socket绑定IP 开启监听 关闭这个socket,防止内存泄漏 好，这就是创建socket的所有过程~~\n","date":"2024-10-20T17:31:13+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%B8%80%E5%88%9D%E8%AF%86%E5%A5%97%E6%8E%A5%E5%AD%97/","title":"网络编程(一)：初识套接字"},{"content":"堆结构 许多应用程序都需要处理有序的元素，但是不一定要求它们全部有序，或是不一定要一次就将它们完全排好序。例如，你要实现一个进程调度算法，每一个进程都有一个优先级(当有电话到来时，你的游戏很可能会被打断)，所以电话程序的优先级会高于游戏程序的优先级。\n所以当前面临的挑战是：一个电脑上每个时刻都会有许多不同的进程，怎么按照优先级去调度它们呢？也就是说，在每次调度的时候，我们都希望花费很少的时间去找到最重要的进程(或者最不重要的进程)，堆结构就很好的解决这个问题。\n完全二叉树 二叉树是一种常见的结构，对于一个结点来说，它通常可以有两个子节点(left, right)，看起来是这样的：\n1 2 3 3 1 2 4 5 6 7 用代码来表示：\n1 2 3 4 5 type Tree struct { Val int // 值域 Left *Tree Right *Tree } 不过这里我们不会使用这种形式，可以使用数组来进行父子结点之间关系。\n完全二叉树是一个特殊的二叉树，下面是维基百科对完全二叉树的介绍\n在一颗二叉树中，若除最后一层外的其余层都是满的，并且最后一层要么是满的，要么在右边缺少连续若干节点，则此二叉树为完全二叉树（Complete Binary Tree）。具有n个节点的完全二叉树的深度。深度为k的完全二叉树，至少有个节点，至多有个节点。\n通俗点来说，完全二叉树就是满二叉树从后向前去除子节点后剩下的二叉树。简单来说，堆按照根节点与子节点的大小关系可以分为大根堆和小根堆。大根堆的根节点比所有的子节点大，小根堆的根节点比子节点都小。所以按照这种规则，大根堆的根节点一定是数组里面的最大值，小根堆的根节点一定是数组里面的最小值。\n堆的实现 首先，对于一个不是堆结构的数组来说，第一步要做的就是heapify堆化，堆化需要用到两个辅助函数，\n就是 sink和swim。\n从名字上来看，sink就是向下交换，swim就是向上交换，对于一个数组，我们可以从第一个非叶子结点开始堆化，对于这些非叶子结点，我们每次都进行堆化，这样初始化到数组的第一个元素的时候，整个数组就是堆结构了。\nsink方法 首先是找到这个数组的第一个非叶子结点，对于一个数组arr来说，我们假定它的长度是n，那么它的第一个非叶子结点就是(n-1) / 2，这里从大根堆开始建堆.\n首先，一个堆结构看起来可能是这样的：\n1 2 3 type MaxHeap struct { item []int } 它的sink方法是：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 func (this *MaxHeap) sink(k int) { n := len(this.item) // 孩子结点 for 2*k+1 \u0026lt; n { child := 2*k + 1 if child+1 \u0026lt; n \u0026amp;\u0026amp; this.item[child] \u0026lt; this.item[child+1] { child = child + 1 } // 没必要交换 if this.item[k] \u0026gt;= this.item[child] { break } // 交换两个元素 this.item[k], this.item[child] = this.item[child], this.item[k] k = child } } 上面这段代码的意思是，给定一个下标，让这个下标的元素向下交换，从而满足堆结构。然后就可以开始进行堆化的操作了\nheapify方法 1 2 3 4 5 6 7 func (this *MaxHeap) heapify() { // 从非叶子结点开始 n := len(this.item) for i := (n - 1) / 2; i \u0026gt;= 0; i-- { this.sink(i) } } 经过此次操作，我们就已经有了堆结构，item切片的第一个元素就是数组中的最大值。\nswim方法 但是，到目前为止，我们还没有让这个堆实现动态数据的添加和实现，当新增一个数据的时候，我们就把它添加到切片的结尾末尾，此时，这个新增的结点很可能会破坏掉原来的堆结构，这个时候，我们应该给新添加进来的元素找到合适的位置，swim方法就可以很好的实现。\n1 2 3 4 5 6 7 func (this *MaxHeap) swim(k int) { for k \u0026gt; 0 \u0026amp;\u0026amp; this.item[(k-1)/2] \u0026lt; this.item[k] { // 父节点比自己小 this.item[(k-1)/2], this.item[k] = this.item[k], this.item[(k-1)/2] k = (k - 1) / 2 // 注意这里啊 } } 添加与删除 添加一个元素时，需要动态的调整位置\n1 2 3 4 func (this *MaxHeap) insert(val int) { this.item = append(this.item, val) this.swim(len(this.item) - 1) } 删除一个元素的时候(一般是根节点)，这里有一个trick，根节点与最后一个结点进行交换，然后再把新的结点sink使得找到合适的位置，这样做是因为，数组不适合频繁的添加删除元素，这样做可以更加高效！\n1 2 3 4 5 6 7 8 9 10 func (this *MaxHeap) delMax() int { ans := this.item[0] this.item[0], this.item[len(this.item)-1] = this.item[len(this.item)-1], this.item[0] this.item = this.item[:len(this.item)-1] // 删除 this.sink(0) return ans } func (this *MaxHeap) getMax() int { return this.item[0] } 到此，我们就成功的实现了一个大顶堆，小顶堆的实现与之很相似，这里直接贴出代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 type MinHeap struct { item []int } func (this *MinHeap) heapify(nums []int) { this.item = nums n := len(this.item) for i := (n - 1) / 2; i \u0026gt;= 0; i-- { this.sink(i) } } func (this *MinHeap) sink(k int) { n := len(this.item) for 2*k+1 \u0026lt; n { child := 2*k + 1 if child+1 \u0026lt; n \u0026amp;\u0026amp; this.item[child+1] \u0026lt; this.item[child] { child = child + 1 } if this.item[k] \u0026lt;= this.item[child] { break } this.item[k], this.item[child] = this.item[child], this.item[k] k = child } } func (this *MinHeap) swim(k int) { for k \u0026gt; 0 \u0026amp;\u0026amp; this.item[(k-1)/2] \u0026gt; this.item[k] { this.item[k], this.item[(k-1)/2] = this.item[(k-1)/2], this.item[k] k = (k - 1) / 2 } } func (this *MinHeap) genLength() int { return len(this.item) } func (this *MinHeap) insert(val int) { this.item = append(this.item, val) this.swim(len(this.item) - 1) } 拓展：堆排序 在我们刚才构建大顶堆的时候，注意到每次都可以以 $O(logK)$的时间复杂度调整并获取最大值，所以，对于n组数来说，我们就可以以$O(nlogK)$的复杂度来进行排序，其中$K$是树的高度\n这里是完整代码，需要注意交换的范围每次都是在缩小的\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 package main import ( \u0026#34;fmt\u0026#34; ) // ConstructMaxHeap 将输入数组转换为最大堆。 func ConstructMaxHeap(nums []int) { n := len(nums) for i := n/2 - 1; i \u0026gt;= 0; i-- { sink(nums, i, n) } } // sink 是辅助函数，用于下沉元素以保持最大堆性质。 func sink(nums []int, k int, n int) { for 2*k+1 \u0026lt; n { child := 2*k + 1 if child+1 \u0026lt; n \u0026amp;\u0026amp; nums[child] \u0026lt; nums[child+1] { child = child + 1 } if nums[k] \u0026gt;= nums[child] { break } nums[k], nums[child] = nums[child], nums[k] k = child } } // heapSort 使用堆排序算法对输入的切片进行排序。 func heapSort(nums []int) { ConstructMaxHeap(nums) n := len(nums) for i := n - 1; i \u0026gt; 0; i-- { nums[i], nums[0] = nums[0], nums[i] // 交换最大元素到正确位置 sink(nums, 0, i) // 重新构建堆 } } func main() { nums := []int{3, 2, 1, 5, 6, 4} fmt.Println(\u0026#34;Original array:\u0026#34;, nums) heapSort(nums) fmt.Println(\u0026#34;Sorted array:\u0026#34;, nums) } ","date":"2024-10-19T19:47:58+08:00","permalink":"https://XiaoPeng0x3.github.io/p/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E5%A0%86/","title":"数据结构之堆"}]